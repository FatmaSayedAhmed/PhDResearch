{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DR1-GPU-2Classes-DenseNet201_Pre-trained Network.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "th5H5qITuXiJ",
        "colab_type": "code",
        "outputId": "6ef88143-b4b5-4c03-e92c-ecef1c5822ea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "V2ti6rWyvy9C",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Class 0 Vs Class 1"
      ]
    },
    {
      "metadata": {
        "id": "nWXHgttcv4OH",
        "colab_type": "code",
        "outputId": "99d6ae25-ef32-447d-bbba-4e0d848fe87b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2921
        }
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf \n",
        "import keras\n",
        "import os\n",
        "\n",
        "from subprocess import check_output\n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "from PIL import Image\n",
        "            \n",
        "from keras.applications.densenet import preprocess_input     \n",
        "\n",
        "\n",
        "from keras.preprocessing import image\n",
        "\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "from sklearn.cross_validation import train_test_split\n",
        "\n",
        "from keras.utils import np_utils\n",
        "\n",
        "from keras.applications.densenet import DenseNet201    \n",
        "\n",
        "from keras.models import Model, load_model\n",
        "\n",
        "from keras.layers import Dense, Dropout, Reshape\n",
        "\n",
        "from keras.optimizers import SGD\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.metrics import classification_report,confusion_matrix\n",
        "\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "from keras.layers import Activation, Flatten \n",
        "from keras import backend as K\n",
        "from keras import applications\n",
        "\n",
        "\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0' #use GPU with ID=0\n",
        "config = tf.ConfigProto()\n",
        "config.gpu_options.per_process_gpu_memory_fraction = 0.7 # maximun alloc gpu50% of MEM\n",
        "sess = tf.Session(config = config)\n",
        "keras.backend.set_session(sess)\n",
        "\n",
        "\n",
        "# dimensions of our images.\n",
        "img_width, img_height = 224, 224\n",
        " \n",
        "top_model_weights_path = '/content/gdrive/My Drive/Colab Notebooks/bottleneck_fc_model_DenseNet201_PTN_data1.h5'\n",
        "\n",
        "train_data_dir = '/content/gdrive/My Drive/Colab Notebooks/data1_Train1708_Test732/train'\n",
        "validation_data_dir = '/content/gdrive/My Drive/Colab Notebooks/data1_Train1708_Test732/validation'\n",
        "nb_train_samples = 3416\n",
        "nb_validation_samples = 1464\n",
        "\n",
        "epochs = 50\n",
        "batch_size = 8  # batch size in flow_images_from_directory needs to correspond to the image number of the test images.\n",
        "\n",
        "\n",
        "def save_bottleneck_features():\n",
        "    datagen = ImageDataGenerator(rescale=1. / 255)\n",
        "\n",
        "    model = applications.DenseNet201(weights='imagenet', include_top=False)\n",
        "\n",
        "    #model.summary()\n",
        "    \n",
        "    generator = datagen.flow_from_directory(\n",
        "        train_data_dir,\n",
        "        target_size=(img_width, img_height),\n",
        "        batch_size=batch_size,\n",
        "        class_mode=None,\n",
        "        shuffle=False)\n",
        "    \n",
        "    bottleneck_features_train = model.predict_generator(\n",
        "        generator, nb_train_samples // batch_size)\n",
        "    np.save(open('DR1_bottleneck_features_train_DenseNet201_PTN_data1.npy', 'wb'),\n",
        "            bottleneck_features_train)\n",
        "\n",
        "    generator = datagen.flow_from_directory(\n",
        "        validation_data_dir,\n",
        "        target_size=(img_width, img_height),\n",
        "        batch_size=batch_size,\n",
        "        class_mode=None,\n",
        "        shuffle=False)\n",
        "    bottleneck_features_validation = model.predict_generator(\n",
        "        generator, nb_validation_samples // batch_size)\n",
        "    np.save(open('DR1_bottleneck_features_validation_DenseNet201_PTN_data1.npy', 'wb'),\n",
        "            bottleneck_features_validation)\n",
        "\n",
        "\n",
        "def train_top_model():\n",
        "    train_data = np.load(open('DR1_bottleneck_features_train_DenseNet201_PTN_data1.npy','rb'))\n",
        "    train_labels = np.array(\n",
        "        [0] * (nb_train_samples // 2) + [1] * (nb_train_samples // 2))\n",
        "    \n",
        "    print(' train_labels shape',train_labels.shape)\n",
        "\n",
        "    #print(' train_labels : \\n ',train_labels)\n",
        "\n",
        "    validation_data = np.load(open('DR1_bottleneck_features_validation_DenseNet201_PTN_data1.npy','rb'))\n",
        "    validation_labels = np.array(\n",
        "        [0] * (nb_validation_samples // 2) + [1] * (nb_validation_samples // 2))\n",
        "\n",
        "    print(' validation_labels shape',validation_labels.shape)\n",
        "\n",
        "    #print(' validation_labels : \\n ',validation_labels)\n",
        "\n",
        "    model = Sequential()\n",
        "    model.add(Flatten(input_shape=train_data.shape[1:]))\n",
        "    model.add(Dense(256, activation='relu'))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "    model.compile(optimizer='rmsprop',\n",
        "                  loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    model.fit(train_data, train_labels,\n",
        "              epochs=epochs,\n",
        "              batch_size=batch_size,\n",
        "              validation_data=(validation_data, validation_labels))\n",
        "    model.save_weights(top_model_weights_path)\n",
        "    \n",
        "    print('-------------------------------------------------------------------')\n",
        "    print('------------------ Evalute the train set ------------------------')\n",
        "    print('-------------------------------------------------------------------')\n",
        "\n",
        "    train_score = model.evaluate(train_data, train_labels, verbose=0) # nb_train_samples // batch_size, workers=1)\n",
        "    print('Train loss:', train_score[0])\n",
        "    print('Train accuracy:', train_score[1])\n",
        "\n",
        "    y_pred_train = np.squeeze(model.predict(train_data))\n",
        "    print(' y_pred_train shape',y_pred_train.shape)\n",
        "    #print(' y_pred_train : \\n ',y_pred_train)\n",
        "\n",
        "    threshold = 0.5\n",
        "    y_pred_train = (y_pred_train > threshold)*1\n",
        "    #y_pred_train.astype(int)   \n",
        "    #print(' y_pred_train again: \\n ',y_pred_train)\n",
        "    \n",
        "    accuracy_score_train = accuracy_score(train_labels, y_pred_train, normalize=True)\n",
        "    print('accuracy_score_train with normalize=True: ', accuracy_score_train)\n",
        "\n",
        "    accuracy_score_train = accuracy_score(train_labels, y_pred_train, normalize=False)\n",
        "    print('accuracy_score_train with normalize=False : ', accuracy_score_train)\n",
        "\n",
        "    target_names = ['class 0', 'class 1']\n",
        "\n",
        "    print(classification_report(y_pred_train, train_labels, target_names=target_names))\n",
        "\n",
        "    cm1 = confusion_matrix(y_pred_train, train_labels)\n",
        "    \n",
        "    print('confusion_matrix : \\n', cm1)\n",
        "\n",
        "    total1=sum(sum(cm1))\n",
        "\n",
        "    #####from confusion matrix calculate accuracy\n",
        "    accuracy1=(cm1[0,0]+cm1[1,1])/total1\n",
        "    print ('Accuracy : ', accuracy1)\n",
        "\n",
        "    sensitivity = cm1[0,0]/(cm1[0,0]+cm1[1,0])\n",
        "    print('Sensitivity : ', sensitivity )\n",
        "\n",
        "    Specificity = cm1[1,1]/(cm1[1,1]+cm1[0,1])\n",
        "    print('Specificity : ', Specificity )\n",
        "\n",
        "    print('-------------------------------------------------------------------')\n",
        "    print('------------------ Evalute the validation set ------------------------')\n",
        "    print('-------------------------------------------------------------------')\n",
        "\n",
        "    validation_score = model.evaluate(validation_data, validation_labels, verbose=0) # nb_train_samples // batch_size, workers=1)\n",
        "    print('validation loss:', validation_score[0])\n",
        "    print('validation accuracy:', validation_score[1])\n",
        "\n",
        "    y_pred_validation = np.squeeze(model.predict(validation_data))\n",
        "    print(' y_pred_validation shape',y_pred_validation.shape)\n",
        "    #print(' y_pred_validation : \\n ',y_pred_validation)\n",
        "\n",
        "    y_pred_validation = (y_pred_validation > threshold)*1\n",
        "    #y_pred_train.astype(int)   \n",
        "    #print(' y_pred_validation again: \\n ',y_pred_validation)\n",
        "    \n",
        "    accuracy_score_validation = accuracy_score(validation_labels, y_pred_validation, normalize=True)\n",
        "    print('accuracy_score_validation with normalize=True: ', accuracy_score_validation)\n",
        "\n",
        "    accuracy_score_validation = accuracy_score(validation_labels, y_pred_validation, normalize=False)\n",
        "    print('accuracy_score_validation with normalize=False : ', accuracy_score_validation)\n",
        "\n",
        "    print(classification_report(y_pred_validation, validation_labels, target_names=target_names))\n",
        "\n",
        "    cm1 = confusion_matrix(y_pred_validation, validation_labels)\n",
        "    \n",
        "    print('confusion_matrix : \\n', cm1)\n",
        "\n",
        "    total1=sum(sum(cm1))\n",
        "\n",
        "    #####from confusion matrix calculate accuracy\n",
        "    accuracy1=(cm1[0,0]+cm1[1,1])/total1\n",
        "    print ('Accuracy : ', accuracy1)\n",
        "\n",
        "    sensitivity = cm1[0,0]/(cm1[0,0]+cm1[1,0])\n",
        "    print('Sensitivity : ', sensitivity )\n",
        "\n",
        "    Specificity = cm1[1,1]/(cm1[1,1]+cm1[0,1])\n",
        "    print('Specificity : ', Specificity )\n",
        "\n",
        "\n",
        "save_bottleneck_features()\n",
        "train_top_model()\n",
        "\n",
        "print('-------------------------------------------------------------------')\n",
        "print('------------------ Train Done -------------------------------------')\n",
        "print('-------------------------------------------------------------------')\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
            "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.8/densenet201_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "76398592/76391848 [==============================] - 1s 0us/step\n",
            "Found 3416 images belonging to 2 classes.\n",
            "Found 1464 images belonging to 2 classes.\n",
            " train_labels shape (3416,)\n",
            " validation_labels shape (1464,)\n",
            "Train on 3416 samples, validate on 1464 samples\n",
            "Epoch 1/50\n",
            "3416/3416 [==============================] - 17s 5ms/step - loss: 8.0505 - acc: 0.4991 - val_loss: 8.0590 - val_acc: 0.5000\n",
            "Epoch 2/50\n",
            "3416/3416 [==============================] - 13s 4ms/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0590 - val_acc: 0.5000\n",
            "Epoch 3/50\n",
            "3416/3416 [==============================] - 13s 4ms/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0590 - val_acc: 0.5000\n",
            "Epoch 4/50\n",
            "3416/3416 [==============================] - 13s 4ms/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0590 - val_acc: 0.5000\n",
            "Epoch 5/50\n",
            "3416/3416 [==============================] - 13s 4ms/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0590 - val_acc: 0.5000\n",
            "Epoch 6/50\n",
            "3416/3416 [==============================] - 13s 4ms/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0590 - val_acc: 0.5000\n",
            "Epoch 7/50\n",
            "3416/3416 [==============================] - 13s 4ms/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0590 - val_acc: 0.5000\n",
            "Epoch 8/50\n",
            "3416/3416 [==============================] - 13s 4ms/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0590 - val_acc: 0.5000\n",
            "Epoch 9/50\n",
            "3416/3416 [==============================] - 13s 4ms/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0590 - val_acc: 0.5000\n",
            "Epoch 10/50\n",
            "3416/3416 [==============================] - 13s 4ms/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0590 - val_acc: 0.5000\n",
            "Epoch 11/50\n",
            "3416/3416 [==============================] - 13s 4ms/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0590 - val_acc: 0.5000\n",
            "Epoch 12/50\n",
            "3416/3416 [==============================] - 13s 4ms/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0590 - val_acc: 0.5000\n",
            "Epoch 13/50\n",
            "3416/3416 [==============================] - 13s 4ms/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0590 - val_acc: 0.5000\n",
            "Epoch 14/50\n",
            "3416/3416 [==============================] - 13s 4ms/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0590 - val_acc: 0.5000\n",
            "Epoch 15/50\n",
            "3416/3416 [==============================] - 13s 4ms/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0590 - val_acc: 0.5000\n",
            "Epoch 16/50\n",
            "3416/3416 [==============================] - 13s 4ms/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0590 - val_acc: 0.5000\n",
            "Epoch 17/50\n",
            "3416/3416 [==============================] - 13s 4ms/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0590 - val_acc: 0.5000\n",
            "Epoch 18/50\n",
            "3416/3416 [==============================] - 13s 4ms/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0590 - val_acc: 0.5000\n",
            "Epoch 19/50\n",
            "3416/3416 [==============================] - 13s 4ms/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0590 - val_acc: 0.5000\n",
            "Epoch 20/50\n",
            "3416/3416 [==============================] - 13s 4ms/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0590 - val_acc: 0.5000\n",
            "Epoch 21/50\n",
            "3416/3416 [==============================] - 13s 4ms/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0590 - val_acc: 0.5000\n",
            "Epoch 22/50\n",
            "3416/3416 [==============================] - 13s 4ms/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0590 - val_acc: 0.5000\n",
            "Epoch 23/50\n",
            "3416/3416 [==============================] - 13s 4ms/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0590 - val_acc: 0.5000\n",
            "Epoch 24/50\n",
            "3416/3416 [==============================] - 13s 4ms/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0590 - val_acc: 0.5000\n",
            "Epoch 25/50\n",
            "3416/3416 [==============================] - 13s 4ms/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0590 - val_acc: 0.5000\n",
            "Epoch 26/50\n",
            "3416/3416 [==============================] - 13s 4ms/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0590 - val_acc: 0.5000\n",
            "Epoch 27/50\n",
            "3416/3416 [==============================] - 13s 4ms/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0590 - val_acc: 0.5000\n",
            "Epoch 28/50\n",
            "3416/3416 [==============================] - 13s 4ms/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0590 - val_acc: 0.5000\n",
            "Epoch 29/50\n",
            "3416/3416 [==============================] - 13s 4ms/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0590 - val_acc: 0.5000\n",
            "Epoch 30/50\n",
            "3416/3416 [==============================] - 13s 4ms/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0590 - val_acc: 0.5000\n",
            "Epoch 31/50\n",
            "3416/3416 [==============================] - 13s 4ms/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0590 - val_acc: 0.5000\n",
            "Epoch 32/50\n",
            "3416/3416 [==============================] - 13s 4ms/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0590 - val_acc: 0.5000\n",
            "Epoch 33/50\n",
            "3416/3416 [==============================] - 13s 4ms/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0590 - val_acc: 0.5000\n",
            "Epoch 34/50\n",
            "3416/3416 [==============================] - 13s 4ms/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0590 - val_acc: 0.5000\n",
            "Epoch 35/50\n",
            "3416/3416 [==============================] - 13s 4ms/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0590 - val_acc: 0.5000\n",
            "Epoch 36/50\n",
            "3416/3416 [==============================] - 13s 4ms/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0590 - val_acc: 0.5000\n",
            "Epoch 37/50\n",
            "3416/3416 [==============================] - 13s 4ms/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0590 - val_acc: 0.5000\n",
            "Epoch 38/50\n",
            "3416/3416 [==============================] - 13s 4ms/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0590 - val_acc: 0.5000\n",
            "Epoch 39/50\n",
            "3416/3416 [==============================] - 13s 4ms/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0590 - val_acc: 0.5000\n",
            "Epoch 40/50\n",
            "3416/3416 [==============================] - 13s 4ms/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0590 - val_acc: 0.5000\n",
            "Epoch 41/50\n",
            "3416/3416 [==============================] - 13s 4ms/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0590 - val_acc: 0.5000\n",
            "Epoch 42/50\n",
            "3416/3416 [==============================] - 13s 4ms/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0590 - val_acc: 0.5000\n",
            "Epoch 43/50\n",
            "3416/3416 [==============================] - 13s 4ms/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0590 - val_acc: 0.5000\n",
            "Epoch 44/50\n",
            "3416/3416 [==============================] - 13s 4ms/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0590 - val_acc: 0.5000\n",
            "Epoch 45/50\n",
            "3416/3416 [==============================] - 13s 4ms/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0590 - val_acc: 0.5000\n",
            "Epoch 46/50\n",
            "3416/3416 [==============================] - 13s 4ms/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0590 - val_acc: 0.5000\n",
            "Epoch 47/50\n",
            "3416/3416 [==============================] - 13s 4ms/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0590 - val_acc: 0.5000\n",
            "Epoch 48/50\n",
            "3416/3416 [==============================] - 13s 4ms/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0590 - val_acc: 0.5000\n",
            "Epoch 49/50\n",
            "3416/3416 [==============================] - 13s 4ms/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0590 - val_acc: 0.5000\n",
            "Epoch 50/50\n",
            "3416/3416 [==============================] - 13s 4ms/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0590 - val_acc: 0.5000\n",
            "-------------------------------------------------------------------\n",
            "------------------ Evalute the train set ------------------------\n",
            "-------------------------------------------------------------------\n",
            "Train loss: 8.059047748623328\n",
            "Train accuracy: 0.5\n",
            " y_pred_train shape (3416,)\n",
            "accuracy_score_train with normalize=True:  0.5\n",
            "accuracy_score_train with normalize=False :  1708\n",
            "             precision    recall  f1-score   support\n",
            "\n",
            "    class 0       1.00      0.50      0.67      3416\n",
            "    class 1       0.00      0.00      0.00         0\n",
            "\n",
            "avg / total       1.00      0.50      0.67      3416\n",
            "\n",
            "confusion_matrix : \n",
            " [[1708 1708]\n",
            " [   0    0]]\n",
            "Accuracy :  0.5\n",
            "Sensitivity :  1.0\n",
            "Specificity :  0.0\n",
            "-------------------------------------------------------------------\n",
            "------------------ Evalute the validation set ------------------------\n",
            "-------------------------------------------------------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1137: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
            "  'recall', 'true', average, warn_for)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "validation loss: 8.059047747062047\n",
            "validation accuracy: 0.5\n",
            " y_pred_validation shape (1464,)\n",
            "accuracy_score_validation with normalize=True:  0.5\n",
            "accuracy_score_validation with normalize=False :  732\n",
            "             precision    recall  f1-score   support\n",
            "\n",
            "    class 0       1.00      0.50      0.67      1464\n",
            "    class 1       0.00      0.00      0.00         0\n",
            "\n",
            "avg / total       1.00      0.50      0.67      1464\n",
            "\n",
            "confusion_matrix : \n",
            " [[732 732]\n",
            " [  0   0]]\n",
            "Accuracy :  0.5\n",
            "Sensitivity :  1.0\n",
            "Specificity :  0.0\n",
            "-------------------------------------------------------------------\n",
            "------------------ Train Done -------------------------------------\n",
            "-------------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "PtYfDivOBgJg",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Class 0 Vs Class 2"
      ]
    },
    {
      "metadata": {
        "id": "tFJ6XAxzBjVJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2764
        },
        "outputId": "be85bfbe-4c7d-45c6-8e09-ad967f649a99"
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf \n",
        "import keras\n",
        "import os\n",
        "\n",
        "from subprocess import check_output\n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "from PIL import Image\n",
        "            \n",
        "from keras.applications.densenet import preprocess_input     \n",
        "\n",
        "\n",
        "from keras.preprocessing import image\n",
        "\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "from sklearn.cross_validation import train_test_split\n",
        "\n",
        "from keras.utils import np_utils\n",
        "\n",
        "from keras.applications.densenet import DenseNet201    \n",
        "\n",
        "from keras.models import Model, load_model\n",
        "\n",
        "from keras.layers import Dense, Dropout, Reshape\n",
        "\n",
        "from keras.optimizers import SGD\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.metrics import classification_report,confusion_matrix\n",
        "\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "from keras.layers import Activation, Flatten \n",
        "from keras import backend as K\n",
        "from keras import applications\n",
        "\n",
        "\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0' #use GPU with ID=0\n",
        "config = tf.ConfigProto()\n",
        "config.gpu_options.per_process_gpu_memory_fraction = 0.7 # maximun alloc gpu50% of MEM\n",
        "sess = tf.Session(config = config)\n",
        "keras.backend.set_session(sess)\n",
        "\n",
        "\n",
        "# dimensions of our images.\n",
        "img_width, img_height = 224, 224\n",
        " \n",
        "top_model_weights_path = '/content/gdrive/My Drive/Colab Notebooks/bottleneck_fc_model_DenseNet201_PTN_data2.h5'\n",
        "\n",
        "train_data_dir = '/content/gdrive/My Drive/Colab Notebooks/data2_Train3704_Test1588/train'\n",
        "validation_data_dir = '/content/gdrive/My Drive/Colab Notebooks/data2_Train3704_Test1588/validation'\n",
        "nb_train_samples = 7408\n",
        "nb_validation_samples = 3176\n",
        "\n",
        "epochs = 50\n",
        "batch_size = 8  # batch size in flow_images_from_directory needs to correspond to the image number of the test images.\n",
        "\n",
        "\n",
        "def save_bottleneck_features():\n",
        "    datagen = ImageDataGenerator(rescale=1. / 255)\n",
        "\n",
        "    model = applications.DenseNet201(weights='imagenet', include_top=False)\n",
        "\n",
        "    #model.summary()\n",
        "    \n",
        "    generator = datagen.flow_from_directory(\n",
        "        train_data_dir,\n",
        "        target_size=(img_width, img_height),\n",
        "        batch_size=batch_size,\n",
        "        class_mode=None,\n",
        "        shuffle=False)\n",
        "    \n",
        "    bottleneck_features_train = model.predict_generator(\n",
        "        generator, nb_train_samples // batch_size)\n",
        "    np.save(open('DR1_bottleneck_features_train_DenseNet201_PTN_data2.npy', 'wb'),\n",
        "            bottleneck_features_train)\n",
        "\n",
        "    generator = datagen.flow_from_directory(\n",
        "        validation_data_dir,\n",
        "        target_size=(img_width, img_height),\n",
        "        batch_size=batch_size,\n",
        "        class_mode=None,\n",
        "        shuffle=False)\n",
        "    bottleneck_features_validation = model.predict_generator(\n",
        "        generator, nb_validation_samples // batch_size)\n",
        "    np.save(open('DR1_bottleneck_features_validation_DenseNet201_PTN_data2.npy', 'wb'),\n",
        "            bottleneck_features_validation)\n",
        "\n",
        "\n",
        "def train_top_model():\n",
        "    train_data = np.load(open('DR1_bottleneck_features_train_DenseNet201_PTN_data2.npy','rb'))\n",
        "    train_labels = np.array(\n",
        "        [0] * (nb_train_samples // 2) + [1] * (nb_train_samples // 2))\n",
        "    \n",
        "    print(' train_labels shape',train_labels.shape)\n",
        "\n",
        "    #print(' train_labels : \\n ',train_labels)\n",
        "\n",
        "    validation_data = np.load(open('DR1_bottleneck_features_validation_DenseNet201_PTN_data2.npy','rb'))\n",
        "    validation_labels = np.array(\n",
        "        [0] * (nb_validation_samples // 2) + [1] * (nb_validation_samples // 2))\n",
        "\n",
        "    print(' validation_labels shape',validation_labels.shape)\n",
        "\n",
        "    #print(' validation_labels : \\n ',validation_labels)\n",
        "\n",
        "    model = Sequential()\n",
        "    model.add(Flatten(input_shape=train_data.shape[1:]))\n",
        "    model.add(Dense(256, activation='relu'))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "    model.compile(optimizer='rmsprop',\n",
        "                  loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    model.fit(train_data, train_labels,\n",
        "              epochs=epochs,\n",
        "              batch_size=batch_size,\n",
        "              validation_data=(validation_data, validation_labels))\n",
        "    model.save_weights(top_model_weights_path)\n",
        "    \n",
        "    print('-------------------------------------------------------------------')\n",
        "    print('------------------ Evalute the train set ------------------------')\n",
        "    print('-------------------------------------------------------------------')\n",
        "\n",
        "    train_score = model.evaluate(train_data, train_labels, verbose=0) # nb_train_samples // batch_size, workers=1)\n",
        "    print('Train loss:', train_score[0])\n",
        "    print('Train accuracy:', train_score[1])\n",
        "\n",
        "    y_pred_train = np.squeeze(model.predict(train_data))\n",
        "    print(' y_pred_train shape',y_pred_train.shape)\n",
        "    #print(' y_pred_train : \\n ',y_pred_train)\n",
        "\n",
        "    threshold = 0.5\n",
        "    y_pred_train = (y_pred_train > threshold)*1\n",
        "    #y_pred_train.astype(int)   \n",
        "    #print(' y_pred_train again: \\n ',y_pred_train)\n",
        "    \n",
        "    accuracy_score_train = accuracy_score(train_labels, y_pred_train, normalize=True)\n",
        "    print('accuracy_score_train with normalize=True: ', accuracy_score_train)\n",
        "\n",
        "    accuracy_score_train = accuracy_score(train_labels, y_pred_train, normalize=False)\n",
        "    print('accuracy_score_train with normalize=False : ', accuracy_score_train)\n",
        "\n",
        "    target_names = ['class 0', 'class 1']\n",
        "\n",
        "    print(classification_report(y_pred_train, train_labels, target_names=target_names))\n",
        "\n",
        "    cm1 = confusion_matrix(y_pred_train, train_labels)\n",
        "    \n",
        "    print('confusion_matrix : \\n', cm1)\n",
        "\n",
        "    total1=sum(sum(cm1))\n",
        "\n",
        "    #####from confusion matrix calculate accuracy\n",
        "    accuracy1=(cm1[0,0]+cm1[1,1])/total1\n",
        "    print ('Accuracy : ', accuracy1)\n",
        "\n",
        "    sensitivity = cm1[0,0]/(cm1[0,0]+cm1[1,0])\n",
        "    print('Sensitivity : ', sensitivity )\n",
        "\n",
        "    Specificity = cm1[1,1]/(cm1[1,1]+cm1[0,1])\n",
        "    print('Specificity : ', Specificity )\n",
        "\n",
        "    print('-------------------------------------------------------------------')\n",
        "    print('------------------ Evalute the validation set ------------------------')\n",
        "    print('-------------------------------------------------------------------')\n",
        "\n",
        "    validation_score = model.evaluate(validation_data, validation_labels, verbose=0) # nb_train_samples // batch_size, workers=1)\n",
        "    print('validation loss:', validation_score[0])\n",
        "    print('validation accuracy:', validation_score[1])\n",
        "\n",
        "    y_pred_validation = np.squeeze(model.predict(validation_data))\n",
        "    print(' y_pred_validation shape',y_pred_validation.shape)\n",
        "    #print(' y_pred_validation : \\n ',y_pred_validation)\n",
        "\n",
        "    y_pred_validation = (y_pred_validation > threshold)*1\n",
        "    #y_pred_train.astype(int)   \n",
        "    #print(' y_pred_validation again: \\n ',y_pred_validation)\n",
        "    \n",
        "    accuracy_score_validation = accuracy_score(validation_labels, y_pred_validation, normalize=True)\n",
        "    print('accuracy_score_validation with normalize=True: ', accuracy_score_validation)\n",
        "\n",
        "    accuracy_score_validation = accuracy_score(validation_labels, y_pred_validation, normalize=False)\n",
        "    print('accuracy_score_validation with normalize=False : ', accuracy_score_validation)\n",
        "\n",
        "    print(classification_report(y_pred_validation, validation_labels, target_names=target_names))\n",
        "\n",
        "    cm1 = confusion_matrix(y_pred_validation, validation_labels)\n",
        "    \n",
        "    print('confusion_matrix : \\n', cm1)\n",
        "\n",
        "    total1=sum(sum(cm1))\n",
        "\n",
        "    #####from confusion matrix calculate accuracy\n",
        "    accuracy1=(cm1[0,0]+cm1[1,1])/total1\n",
        "    print ('Accuracy : ', accuracy1)\n",
        "\n",
        "    sensitivity = cm1[0,0]/(cm1[0,0]+cm1[1,0])\n",
        "    print('Sensitivity : ', sensitivity )\n",
        "\n",
        "    Specificity = cm1[1,1]/(cm1[1,1]+cm1[0,1])\n",
        "    print('Specificity : ', Specificity )\n",
        "\n",
        "\n",
        "save_bottleneck_features()\n",
        "train_top_model()\n",
        "\n",
        "print('-------------------------------------------------------------------')\n",
        "print('------------------ Train Done -------------------------------------')\n",
        "print('-------------------------------------------------------------------')\n",
        "\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
            "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.8/densenet201_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "76398592/76391848 [==============================] - 1s 0us/step\n",
            "Found 7408 images belonging to 2 classes.\n",
            "Found 3176 images belonging to 2 classes.\n",
            " train_labels shape (7408,)\n",
            " validation_labels shape (3176,)\n",
            "Train on 7408 samples, validate on 3176 samples\n",
            "Epoch 1/50\n",
            "7408/7408 [==============================] - 33s 4ms/step - loss: 7.9655 - acc: 0.4999 - val_loss: 7.9712 - val_acc: 0.5000\n",
            "Epoch 2/50\n",
            "7408/7408 [==============================] - 30s 4ms/step - loss: 7.9712 - acc: 0.5000 - val_loss: 7.9712 - val_acc: 0.5000\n",
            "Epoch 3/50\n",
            "7408/7408 [==============================] - 30s 4ms/step - loss: 7.9712 - acc: 0.5000 - val_loss: 7.9712 - val_acc: 0.5000\n",
            "Epoch 4/50\n",
            "7408/7408 [==============================] - 30s 4ms/step - loss: 7.9712 - acc: 0.5000 - val_loss: 7.9712 - val_acc: 0.5000\n",
            "Epoch 5/50\n",
            "7408/7408 [==============================] - 30s 4ms/step - loss: 7.9712 - acc: 0.5000 - val_loss: 7.9712 - val_acc: 0.5000\n",
            "Epoch 6/50\n",
            "7408/7408 [==============================] - 30s 4ms/step - loss: 7.9712 - acc: 0.5000 - val_loss: 7.9712 - val_acc: 0.5000\n",
            "Epoch 7/50\n",
            "7408/7408 [==============================] - 30s 4ms/step - loss: 7.9712 - acc: 0.5000 - val_loss: 7.9712 - val_acc: 0.5000\n",
            "Epoch 8/50\n",
            "7408/7408 [==============================] - 29s 4ms/step - loss: 7.9712 - acc: 0.5000 - val_loss: 7.9712 - val_acc: 0.5000\n",
            "Epoch 9/50\n",
            "7408/7408 [==============================] - 30s 4ms/step - loss: 7.9712 - acc: 0.5000 - val_loss: 7.9712 - val_acc: 0.5000\n",
            "Epoch 10/50\n",
            "7408/7408 [==============================] - 29s 4ms/step - loss: 7.9712 - acc: 0.5000 - val_loss: 7.9712 - val_acc: 0.5000\n",
            "Epoch 11/50\n",
            "7408/7408 [==============================] - 30s 4ms/step - loss: 7.9712 - acc: 0.5000 - val_loss: 7.9712 - val_acc: 0.5000\n",
            "Epoch 12/50\n",
            "7408/7408 [==============================] - 30s 4ms/step - loss: 7.9712 - acc: 0.5000 - val_loss: 7.9712 - val_acc: 0.5000\n",
            "Epoch 13/50\n",
            "7408/7408 [==============================] - 30s 4ms/step - loss: 7.9712 - acc: 0.5000 - val_loss: 7.9712 - val_acc: 0.5000\n",
            "Epoch 14/50\n",
            "7408/7408 [==============================] - 30s 4ms/step - loss: 7.9712 - acc: 0.5000 - val_loss: 7.9712 - val_acc: 0.5000\n",
            "Epoch 15/50\n",
            "7408/7408 [==============================] - 30s 4ms/step - loss: 7.9712 - acc: 0.5000 - val_loss: 7.9712 - val_acc: 0.5000\n",
            "Epoch 16/50\n",
            "7408/7408 [==============================] - 30s 4ms/step - loss: 7.9712 - acc: 0.5000 - val_loss: 7.9712 - val_acc: 0.5000\n",
            "Epoch 17/50\n",
            "7408/7408 [==============================] - 30s 4ms/step - loss: 7.9712 - acc: 0.5000 - val_loss: 7.9712 - val_acc: 0.5000\n",
            "Epoch 18/50\n",
            "7408/7408 [==============================] - 30s 4ms/step - loss: 7.9712 - acc: 0.5000 - val_loss: 7.9712 - val_acc: 0.5000\n",
            "Epoch 19/50\n",
            "7408/7408 [==============================] - 30s 4ms/step - loss: 7.9712 - acc: 0.5000 - val_loss: 7.9712 - val_acc: 0.5000\n",
            "Epoch 20/50\n",
            "7408/7408 [==============================] - 30s 4ms/step - loss: 7.9712 - acc: 0.5000 - val_loss: 7.9712 - val_acc: 0.5000\n",
            "Epoch 21/50\n",
            "7408/7408 [==============================] - 30s 4ms/step - loss: 7.9712 - acc: 0.5000 - val_loss: 7.9712 - val_acc: 0.5000\n",
            "Epoch 22/50\n",
            "7408/7408 [==============================] - 30s 4ms/step - loss: 7.9712 - acc: 0.5000 - val_loss: 7.9712 - val_acc: 0.5000\n",
            "Epoch 23/50\n",
            "7408/7408 [==============================] - 30s 4ms/step - loss: 7.9712 - acc: 0.5000 - val_loss: 7.9712 - val_acc: 0.5000\n",
            "Epoch 24/50\n",
            "7408/7408 [==============================] - 30s 4ms/step - loss: 7.9712 - acc: 0.5000 - val_loss: 7.9712 - val_acc: 0.5000\n",
            "Epoch 25/50\n",
            "7408/7408 [==============================] - 30s 4ms/step - loss: 7.9712 - acc: 0.5000 - val_loss: 7.9712 - val_acc: 0.5000\n",
            "Epoch 26/50\n",
            "7408/7408 [==============================] - 30s 4ms/step - loss: 7.9712 - acc: 0.5000 - val_loss: 7.9712 - val_acc: 0.5000\n",
            "Epoch 27/50\n",
            "7408/7408 [==============================] - 30s 4ms/step - loss: 7.9712 - acc: 0.5000 - val_loss: 7.9712 - val_acc: 0.5000\n",
            "Epoch 28/50\n",
            "7408/7408 [==============================] - 30s 4ms/step - loss: 7.9712 - acc: 0.5000 - val_loss: 7.9712 - val_acc: 0.5000\n",
            "Epoch 29/50\n",
            "7408/7408 [==============================] - 30s 4ms/step - loss: 7.9712 - acc: 0.5000 - val_loss: 7.9712 - val_acc: 0.5000\n",
            "Epoch 30/50\n",
            "7408/7408 [==============================] - 30s 4ms/step - loss: 7.9712 - acc: 0.5000 - val_loss: 7.9712 - val_acc: 0.5000\n",
            "Epoch 31/50\n",
            "7408/7408 [==============================] - 30s 4ms/step - loss: 7.9712 - acc: 0.5000 - val_loss: 7.9712 - val_acc: 0.5000\n",
            "Epoch 32/50\n",
            "7408/7408 [==============================] - 30s 4ms/step - loss: 7.9712 - acc: 0.5000 - val_loss: 7.9712 - val_acc: 0.5000\n",
            "Epoch 33/50\n",
            "7408/7408 [==============================] - 30s 4ms/step - loss: 7.9712 - acc: 0.5000 - val_loss: 7.9712 - val_acc: 0.5000\n",
            "Epoch 34/50\n",
            "7408/7408 [==============================] - 31s 4ms/step - loss: 7.9712 - acc: 0.5000 - val_loss: 7.9712 - val_acc: 0.5000\n",
            "Epoch 35/50\n",
            "7408/7408 [==============================] - 30s 4ms/step - loss: 7.9712 - acc: 0.5000 - val_loss: 7.9712 - val_acc: 0.5000\n",
            "Epoch 36/50\n",
            "7408/7408 [==============================] - 30s 4ms/step - loss: 7.9712 - acc: 0.5000 - val_loss: 7.9712 - val_acc: 0.5000\n",
            "Epoch 37/50\n",
            "7408/7408 [==============================] - 30s 4ms/step - loss: 7.9712 - acc: 0.5000 - val_loss: 7.9712 - val_acc: 0.5000\n",
            "Epoch 38/50\n",
            "7408/7408 [==============================] - 30s 4ms/step - loss: 7.9712 - acc: 0.5000 - val_loss: 7.9712 - val_acc: 0.5000\n",
            "Epoch 39/50\n",
            "7408/7408 [==============================] - 30s 4ms/step - loss: 7.9712 - acc: 0.5000 - val_loss: 7.9712 - val_acc: 0.5000\n",
            "Epoch 40/50\n",
            "7408/7408 [==============================] - 30s 4ms/step - loss: 7.9712 - acc: 0.5000 - val_loss: 7.9712 - val_acc: 0.5000\n",
            "Epoch 41/50\n",
            "7408/7408 [==============================] - 30s 4ms/step - loss: 7.9712 - acc: 0.5000 - val_loss: 7.9712 - val_acc: 0.5000\n",
            "Epoch 42/50\n",
            "7408/7408 [==============================] - 31s 4ms/step - loss: 7.9712 - acc: 0.5000 - val_loss: 7.9712 - val_acc: 0.5000\n",
            "Epoch 43/50\n",
            "7408/7408 [==============================] - 30s 4ms/step - loss: 7.9712 - acc: 0.5000 - val_loss: 7.9712 - val_acc: 0.5000\n",
            "Epoch 44/50\n",
            "7408/7408 [==============================] - 31s 4ms/step - loss: 7.9712 - acc: 0.5000 - val_loss: 7.9712 - val_acc: 0.5000\n",
            "Epoch 45/50\n",
            "7408/7408 [==============================] - 30s 4ms/step - loss: 7.9712 - acc: 0.5000 - val_loss: 7.9712 - val_acc: 0.5000\n",
            "Epoch 46/50\n",
            "7408/7408 [==============================] - 30s 4ms/step - loss: 7.9712 - acc: 0.5000 - val_loss: 7.9712 - val_acc: 0.5000\n",
            "Epoch 47/50\n",
            "7408/7408 [==============================] - 30s 4ms/step - loss: 7.9712 - acc: 0.5000 - val_loss: 7.9712 - val_acc: 0.5000\n",
            "Epoch 48/50\n",
            "7408/7408 [==============================] - 30s 4ms/step - loss: 7.9712 - acc: 0.5000 - val_loss: 7.9712 - val_acc: 0.5000\n",
            "Epoch 49/50\n",
            "7408/7408 [==============================] - 30s 4ms/step - loss: 7.9712 - acc: 0.5000 - val_loss: 7.9712 - val_acc: 0.5000\n",
            "Epoch 50/50\n",
            "7408/7408 [==============================] - 30s 4ms/step - loss: 7.9712 - acc: 0.5000 - val_loss: 7.9712 - val_acc: 0.5000\n",
            "-------------------------------------------------------------------\n",
            "------------------ Evalute the train set ------------------------\n",
            "-------------------------------------------------------------------\n",
            "Train loss: 7.971192421459997\n",
            "Train accuracy: 0.5\n",
            " y_pred_train shape (7408,)\n",
            "accuracy_score_train with normalize=True:  0.5\n",
            "accuracy_score_train with normalize=False :  3704\n",
            "             precision    recall  f1-score   support\n",
            "\n",
            "    class 0       0.00      0.00      0.00         0\n",
            "    class 1       1.00      0.50      0.67      7408\n",
            "\n",
            "avg / total       1.00      0.50      0.67      7408\n",
            "\n",
            "confusion_matrix : \n",
            " [[   0    0]\n",
            " [3704 3704]]\n",
            "Accuracy :  0.5\n",
            "Sensitivity :  0.0\n",
            "Specificity :  1.0\n",
            "-------------------------------------------------------------------\n",
            "------------------ Evalute the validation set ------------------------\n",
            "-------------------------------------------------------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1137: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
            "  'recall', 'true', average, warn_for)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "validation loss: 7.971192416676346\n",
            "validation accuracy: 0.5\n",
            " y_pred_validation shape (3176,)\n",
            "accuracy_score_validation with normalize=True:  0.5\n",
            "accuracy_score_validation with normalize=False :  1588\n",
            "             precision    recall  f1-score   support\n",
            "\n",
            "    class 0       0.00      0.00      0.00         0\n",
            "    class 1       1.00      0.50      0.67      3176\n",
            "\n",
            "avg / total       1.00      0.50      0.67      3176\n",
            "\n",
            "confusion_matrix : \n",
            " [[   0    0]\n",
            " [1588 1588]]\n",
            "Accuracy :  0.5\n",
            "Sensitivity :  0.0\n",
            "Specificity :  1.0\n",
            "-------------------------------------------------------------------\n",
            "------------------ Train Done -------------------------------------\n",
            "-------------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "n7OFjRRiBksp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Class 0 Vs Class 3"
      ]
    },
    {
      "metadata": {
        "id": "g3UFSt9pBnmq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2622
        },
        "outputId": "e7a5d53f-7552-423e-efb7-f4e132e56680"
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf \n",
        "import keras\n",
        "import os\n",
        "\n",
        "from subprocess import check_output\n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "from PIL import Image\n",
        "            \n",
        "from keras.applications.densenet import preprocess_input     \n",
        "\n",
        "\n",
        "from keras.preprocessing import image\n",
        "\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "from sklearn.cross_validation import train_test_split\n",
        "\n",
        "from keras.utils import np_utils\n",
        "\n",
        "from keras.applications.densenet import DenseNet201    \n",
        "\n",
        "from keras.models import Model, load_model\n",
        "\n",
        "from keras.layers import Dense, Dropout, Reshape\n",
        "\n",
        "from keras.optimizers import SGD\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.metrics import classification_report,confusion_matrix\n",
        "\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "from keras.layers import Activation, Flatten \n",
        "from keras import backend as K\n",
        "from keras import applications\n",
        "\n",
        "\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0' #use GPU with ID=0\n",
        "config = tf.ConfigProto()\n",
        "config.gpu_options.per_process_gpu_memory_fraction = 0.7 # maximun alloc gpu50% of MEM\n",
        "sess = tf.Session(config = config)\n",
        "keras.backend.set_session(sess)\n",
        "\n",
        "\n",
        "# dimensions of our images.\n",
        "img_width, img_height = 224, 224\n",
        " \n",
        "top_model_weights_path = '/content/gdrive/My Drive/Colab Notebooks/bottleneck_fc_model_DenseNet201_PTN_data3.h5'\n",
        "\n",
        "train_data_dir = '/content/gdrive/My Drive/Colab Notebooks/data3_Train608_Test260/train'\n",
        "validation_data_dir = '/content/gdrive/My Drive/Colab Notebooks/data3_Train608_Test260/validation'\n",
        "nb_train_samples = 1216\n",
        "nb_validation_samples = 520\n",
        "\n",
        "epochs = 50\n",
        "batch_size = 8  # batch size in flow_images_from_directory needs to correspond to the image number of the test images.\n",
        "\n",
        "\n",
        "def save_bottleneck_features():\n",
        "    datagen = ImageDataGenerator(rescale=1. / 255)\n",
        "\n",
        "    model = applications.DenseNet201(weights='imagenet', include_top=False)\n",
        "\n",
        "    #model.summary()\n",
        "    \n",
        "    generator = datagen.flow_from_directory(\n",
        "        train_data_dir,\n",
        "        target_size=(img_width, img_height),\n",
        "        batch_size=batch_size,\n",
        "        class_mode=None,\n",
        "        shuffle=False)\n",
        "    \n",
        "    bottleneck_features_train = model.predict_generator(\n",
        "        generator, nb_train_samples // batch_size)\n",
        "    np.save(open('DR1_bottleneck_features_train_DenseNet201_PTN_data3.npy', 'wb'),\n",
        "            bottleneck_features_train)\n",
        "\n",
        "    generator = datagen.flow_from_directory(\n",
        "        validation_data_dir,\n",
        "        target_size=(img_width, img_height),\n",
        "        batch_size=batch_size,\n",
        "        class_mode=None,\n",
        "        shuffle=False)\n",
        "    bottleneck_features_validation = model.predict_generator(\n",
        "        generator, nb_validation_samples // batch_size)\n",
        "    np.save(open('DR1_bottleneck_features_validation_DenseNet201_PTN_data3.npy', 'wb'),\n",
        "            bottleneck_features_validation)\n",
        "\n",
        "\n",
        "def train_top_model():\n",
        "    train_data = np.load(open('DR1_bottleneck_features_train_DenseNet201_PTN_data3.npy','rb'))\n",
        "    train_labels = np.array(\n",
        "        [0] * (nb_train_samples // 2) + [1] * (nb_train_samples // 2))\n",
        "    \n",
        "    print(' train_labels shape',train_labels.shape)\n",
        "\n",
        "    #print(' train_labels : \\n ',train_labels)\n",
        "\n",
        "    validation_data = np.load(open('DR1_bottleneck_features_validation_DenseNet201_PTN_data3.npy','rb'))\n",
        "    validation_labels = np.array(\n",
        "        [0] * (nb_validation_samples // 2) + [1] * (nb_validation_samples // 2))\n",
        "\n",
        "    print(' validation_labels shape',validation_labels.shape)\n",
        "\n",
        "    #print(' validation_labels : \\n ',validation_labels)\n",
        "\n",
        "    model = Sequential()\n",
        "    model.add(Flatten(input_shape=train_data.shape[1:]))\n",
        "    model.add(Dense(256, activation='relu'))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "    model.compile(optimizer='rmsprop',\n",
        "                  loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    model.fit(train_data, train_labels,\n",
        "              epochs=epochs,\n",
        "              batch_size=batch_size,\n",
        "              validation_data=(validation_data, validation_labels))\n",
        "    model.save_weights(top_model_weights_path)\n",
        "    \n",
        "    print('-------------------------------------------------------------------')\n",
        "    print('------------------ Evalute the train set ------------------------')\n",
        "    print('-------------------------------------------------------------------')\n",
        "\n",
        "    train_score = model.evaluate(train_data, train_labels, verbose=0) # nb_train_samples // batch_size, workers=1)\n",
        "    print('Train loss:', train_score[0])\n",
        "    print('Train accuracy:', train_score[1])\n",
        "\n",
        "    y_pred_train = np.squeeze(model.predict(train_data))\n",
        "    print(' y_pred_train shape',y_pred_train.shape)\n",
        "    #print(' y_pred_train : \\n ',y_pred_train)\n",
        "\n",
        "    threshold = 0.5\n",
        "    y_pred_train = (y_pred_train > threshold)*1\n",
        "    #y_pred_train.astype(int)   \n",
        "    #print(' y_pred_train again: \\n ',y_pred_train)\n",
        "    \n",
        "    accuracy_score_train = accuracy_score(train_labels, y_pred_train, normalize=True)\n",
        "    print('accuracy_score_train with normalize=True: ', accuracy_score_train)\n",
        "\n",
        "    accuracy_score_train = accuracy_score(train_labels, y_pred_train, normalize=False)\n",
        "    print('accuracy_score_train with normalize=False : ', accuracy_score_train)\n",
        "\n",
        "    target_names = ['class 0', 'class 1']\n",
        "\n",
        "    print(classification_report(y_pred_train, train_labels, target_names=target_names))\n",
        "\n",
        "    cm1 = confusion_matrix(y_pred_train, train_labels)\n",
        "    \n",
        "    print('confusion_matrix : \\n', cm1)\n",
        "\n",
        "    total1=sum(sum(cm1))\n",
        "\n",
        "    #####from confusion matrix calculate accuracy\n",
        "    accuracy1=(cm1[0,0]+cm1[1,1])/total1\n",
        "    print ('Accuracy : ', accuracy1)\n",
        "\n",
        "    sensitivity = cm1[0,0]/(cm1[0,0]+cm1[1,0])\n",
        "    print('Sensitivity : ', sensitivity )\n",
        "\n",
        "    Specificity = cm1[1,1]/(cm1[1,1]+cm1[0,1])\n",
        "    print('Specificity : ', Specificity )\n",
        "\n",
        "    print('-------------------------------------------------------------------')\n",
        "    print('------------------ Evalute the validation set ------------------------')\n",
        "    print('-------------------------------------------------------------------')\n",
        "\n",
        "    validation_score = model.evaluate(validation_data, validation_labels, verbose=0) # nb_train_samples // batch_size, workers=1)\n",
        "    print('validation loss:', validation_score[0])\n",
        "    print('validation accuracy:', validation_score[1])\n",
        "\n",
        "    y_pred_validation = np.squeeze(model.predict(validation_data))\n",
        "    print(' y_pred_validation shape',y_pred_validation.shape)\n",
        "    #print(' y_pred_validation : \\n ',y_pred_validation)\n",
        "\n",
        "    y_pred_validation = (y_pred_validation > threshold)*1\n",
        "    #y_pred_train.astype(int)   \n",
        "    #print(' y_pred_validation again: \\n ',y_pred_validation)\n",
        "    \n",
        "    accuracy_score_validation = accuracy_score(validation_labels, y_pred_validation, normalize=True)\n",
        "    print('accuracy_score_validation with normalize=True: ', accuracy_score_validation)\n",
        "\n",
        "    accuracy_score_validation = accuracy_score(validation_labels, y_pred_validation, normalize=False)\n",
        "    print('accuracy_score_validation with normalize=False : ', accuracy_score_validation)\n",
        "\n",
        "    print(classification_report(y_pred_validation, validation_labels, target_names=target_names))\n",
        "\n",
        "    cm1 = confusion_matrix(y_pred_validation, validation_labels)\n",
        "    \n",
        "    print('confusion_matrix : \\n', cm1)\n",
        "\n",
        "    total1=sum(sum(cm1))\n",
        "\n",
        "    #####from confusion matrix calculate accuracy\n",
        "    accuracy1=(cm1[0,0]+cm1[1,1])/total1\n",
        "    print ('Accuracy : ', accuracy1)\n",
        "\n",
        "    sensitivity = cm1[0,0]/(cm1[0,0]+cm1[1,0])\n",
        "    print('Sensitivity : ', sensitivity )\n",
        "\n",
        "    Specificity = cm1[1,1]/(cm1[1,1]+cm1[0,1])\n",
        "    print('Specificity : ', Specificity )\n",
        "\n",
        "\n",
        "save_bottleneck_features()\n",
        "train_top_model()\n",
        "\n",
        "print('-------------------------------------------------------------------')\n",
        "print('------------------ Train Done -------------------------------------')\n",
        "print('-------------------------------------------------------------------')\n",
        "\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 1216 images belonging to 2 classes.\n",
            "Found 520 images belonging to 2 classes.\n",
            " train_labels shape (1216,)\n",
            " validation_labels shape (520,)\n",
            "Train on 1216 samples, validate on 520 samples\n",
            "Epoch 1/50\n",
            "1216/1216 [==============================] - 12s 10ms/step - loss: 7.9265 - acc: 0.5058 - val_loss: 8.0590 - val_acc: 0.5000\n",
            "Epoch 2/50\n",
            "1216/1216 [==============================] - 5s 4ms/step - loss: 7.8828 - acc: 0.5090 - val_loss: 7.9953 - val_acc: 0.5019\n",
            "Epoch 3/50\n",
            "1216/1216 [==============================] - 5s 4ms/step - loss: 7.5716 - acc: 0.5238 - val_loss: 7.9712 - val_acc: 0.5000\n",
            "Epoch 4/50\n",
            "1216/1216 [==============================] - 5s 4ms/step - loss: 7.9495 - acc: 0.5000 - val_loss: 7.9712 - val_acc: 0.5000\n",
            "Epoch 5/50\n",
            "1216/1216 [==============================] - 5s 4ms/step - loss: 7.9113 - acc: 0.5025 - val_loss: 7.9712 - val_acc: 0.5000\n",
            "Epoch 6/50\n",
            "1216/1216 [==============================] - 5s 4ms/step - loss: 7.9581 - acc: 0.5008 - val_loss: 7.9712 - val_acc: 0.5000\n",
            "Epoch 7/50\n",
            "1216/1216 [==============================] - 5s 4ms/step - loss: 7.9460 - acc: 0.5008 - val_loss: 7.9712 - val_acc: 0.5000\n",
            "Epoch 8/50\n",
            "1216/1216 [==============================] - 5s 4ms/step - loss: 7.9404 - acc: 0.5008 - val_loss: 7.9712 - val_acc: 0.5000\n",
            "Epoch 9/50\n",
            "1216/1216 [==============================] - 5s 4ms/step - loss: 7.5504 - acc: 0.5238 - val_loss: 7.9712 - val_acc: 0.5000\n",
            "Epoch 10/50\n",
            "1216/1216 [==============================] - 5s 4ms/step - loss: 7.4873 - acc: 0.5222 - val_loss: 7.2018 - val_acc: 0.5462\n",
            "Epoch 11/50\n",
            "1216/1216 [==============================] - 5s 4ms/step - loss: 7.1944 - acc: 0.5411 - val_loss: 6.2814 - val_acc: 0.5904\n",
            "Epoch 12/50\n",
            "1216/1216 [==============================] - 5s 4ms/step - loss: 6.7557 - acc: 0.5699 - val_loss: 6.0607 - val_acc: 0.6058\n",
            "Epoch 13/50\n",
            "1216/1216 [==============================] - 5s 4ms/step - loss: 6.4003 - acc: 0.5921 - val_loss: 5.5280 - val_acc: 0.6442\n",
            "Epoch 14/50\n",
            "1216/1216 [==============================] - 5s 4ms/step - loss: 6.0833 - acc: 0.6110 - val_loss: 5.6918 - val_acc: 0.6288\n",
            "Epoch 15/50\n",
            "1216/1216 [==============================] - 5s 4ms/step - loss: 6.6236 - acc: 0.5831 - val_loss: 7.8896 - val_acc: 0.5077\n",
            "Epoch 16/50\n",
            "1216/1216 [==============================] - 5s 4ms/step - loss: 6.2416 - acc: 0.6053 - val_loss: 5.8825 - val_acc: 0.6269\n",
            "Epoch 17/50\n",
            "1216/1216 [==============================] - 5s 4ms/step - loss: 6.1439 - acc: 0.6094 - val_loss: 5.4862 - val_acc: 0.6519\n",
            "Epoch 18/50\n",
            "1216/1216 [==============================] - 5s 4ms/step - loss: 6.4431 - acc: 0.5913 - val_loss: 5.7845 - val_acc: 0.6365\n",
            "Epoch 19/50\n",
            "1216/1216 [==============================] - 5s 4ms/step - loss: 6.3592 - acc: 0.5962 - val_loss: 7.4981 - val_acc: 0.5346\n",
            "Epoch 20/50\n",
            "1216/1216 [==============================] - 5s 4ms/step - loss: 5.9403 - acc: 0.6242 - val_loss: 5.2038 - val_acc: 0.6654\n",
            "Epoch 21/50\n",
            "1216/1216 [==============================] - 5s 4ms/step - loss: 6.1670 - acc: 0.6102 - val_loss: 6.0903 - val_acc: 0.6115\n",
            "Epoch 22/50\n",
            "1216/1216 [==============================] - 5s 4ms/step - loss: 5.9996 - acc: 0.6160 - val_loss: 5.2054 - val_acc: 0.6692\n",
            "Epoch 23/50\n",
            "1216/1216 [==============================] - 5s 4ms/step - loss: 5.5070 - acc: 0.6513 - val_loss: 5.5076 - val_acc: 0.6481\n",
            "Epoch 24/50\n",
            "1216/1216 [==============================] - 5s 4ms/step - loss: 5.8028 - acc: 0.6340 - val_loss: 5.9046 - val_acc: 0.6288\n",
            "Epoch 25/50\n",
            "1216/1216 [==============================] - 5s 4ms/step - loss: 5.2574 - acc: 0.6653 - val_loss: 6.1715 - val_acc: 0.6077\n",
            "Epoch 26/50\n",
            "1216/1216 [==============================] - 5s 4ms/step - loss: 5.7261 - acc: 0.6349 - val_loss: 7.7352 - val_acc: 0.5135\n",
            "Epoch 27/50\n",
            "1216/1216 [==============================] - 5s 4ms/step - loss: 6.7639 - acc: 0.5740 - val_loss: 7.1085 - val_acc: 0.5500\n",
            "Epoch 28/50\n",
            "1216/1216 [==============================] - 5s 4ms/step - loss: 6.0591 - acc: 0.6143 - val_loss: 6.0210 - val_acc: 0.6250\n",
            "Epoch 29/50\n",
            "1216/1216 [==============================] - 5s 4ms/step - loss: 5.2359 - acc: 0.6669 - val_loss: 5.3961 - val_acc: 0.6577\n",
            "Epoch 30/50\n",
            "1216/1216 [==============================] - 5s 4ms/step - loss: 5.5190 - acc: 0.6530 - val_loss: 6.7539 - val_acc: 0.5808\n",
            "Epoch 31/50\n",
            "1216/1216 [==============================] - 5s 4ms/step - loss: 5.3236 - acc: 0.6628 - val_loss: 6.8229 - val_acc: 0.5750\n",
            "Epoch 32/50\n",
            "1216/1216 [==============================] - 5s 4ms/step - loss: 5.8466 - acc: 0.6340 - val_loss: 6.0309 - val_acc: 0.6231\n",
            "Epoch 33/50\n",
            "1216/1216 [==============================] - 5s 4ms/step - loss: 5.3445 - acc: 0.6628 - val_loss: 5.1327 - val_acc: 0.6750\n",
            "Epoch 34/50\n",
            "1216/1216 [==============================] - 5s 4ms/step - loss: 5.6441 - acc: 0.6431 - val_loss: 7.6556 - val_acc: 0.5192\n",
            "Epoch 35/50\n",
            "1216/1216 [==============================] - 5s 4ms/step - loss: 6.3708 - acc: 0.5979 - val_loss: 6.4618 - val_acc: 0.5923\n",
            "Epoch 36/50\n",
            "1216/1216 [==============================] - 5s 4ms/step - loss: 5.6847 - acc: 0.6398 - val_loss: 4.9596 - val_acc: 0.6865\n",
            "Epoch 37/50\n",
            "1216/1216 [==============================] - 5s 4ms/step - loss: 5.2434 - acc: 0.6653 - val_loss: 4.9410 - val_acc: 0.6885\n",
            "Epoch 38/50\n",
            "1216/1216 [==============================] - 5s 4ms/step - loss: 5.0357 - acc: 0.6817 - val_loss: 4.7401 - val_acc: 0.6962\n",
            "Epoch 39/50\n",
            "1216/1216 [==============================] - 5s 4ms/step - loss: 5.2221 - acc: 0.6669 - val_loss: 4.6460 - val_acc: 0.7058\n",
            "Epoch 40/50\n",
            "1216/1216 [==============================] - 5s 4ms/step - loss: 4.8999 - acc: 0.6875 - val_loss: 5.1095 - val_acc: 0.6788\n",
            "Epoch 41/50\n",
            "1216/1216 [==============================] - 5s 4ms/step - loss: 5.0401 - acc: 0.6817 - val_loss: 4.6374 - val_acc: 0.7058\n",
            "Epoch 42/50\n",
            "1216/1216 [==============================] - 5s 4ms/step - loss: 4.7011 - acc: 0.7015 - val_loss: 4.9351 - val_acc: 0.6885\n",
            "Epoch 43/50\n",
            "1216/1216 [==============================] - 5s 4ms/step - loss: 5.1383 - acc: 0.6752 - val_loss: 4.6041 - val_acc: 0.7058\n",
            "Epoch 44/50\n",
            "1216/1216 [==============================] - 5s 4ms/step - loss: 4.7579 - acc: 0.6998 - val_loss: 4.4071 - val_acc: 0.7212\n",
            "Epoch 45/50\n",
            "1216/1216 [==============================] - 5s 4ms/step - loss: 4.9661 - acc: 0.6867 - val_loss: 5.6817 - val_acc: 0.6385\n",
            "Epoch 46/50\n",
            "1216/1216 [==============================] - 5s 4ms/step - loss: 4.9272 - acc: 0.6891 - val_loss: 4.4958 - val_acc: 0.7154\n",
            "Epoch 47/50\n",
            "1216/1216 [==============================] - 5s 4ms/step - loss: 4.9082 - acc: 0.6891 - val_loss: 4.6983 - val_acc: 0.7019\n",
            "Epoch 48/50\n",
            "1216/1216 [==============================] - 5s 4ms/step - loss: 4.6225 - acc: 0.7081 - val_loss: 4.8458 - val_acc: 0.6885\n",
            "Epoch 49/50\n",
            "1216/1216 [==============================] - 5s 4ms/step - loss: 5.0026 - acc: 0.6842 - val_loss: 4.9346 - val_acc: 0.6904\n",
            "Epoch 50/50\n",
            "1216/1216 [==============================] - 5s 4ms/step - loss: 4.5446 - acc: 0.7146 - val_loss: 4.7756 - val_acc: 0.6981\n",
            "-------------------------------------------------------------------\n",
            "------------------ Evalute the train set ------------------------\n",
            "-------------------------------------------------------------------\n",
            "Train loss: 3.9205700033589412\n",
            "Train accuracy: 0.7491776315789473\n",
            " y_pred_train shape (1216,)\n",
            "accuracy_score_train with normalize=True:  0.7491776315789473\n",
            "accuracy_score_train with normalize=False :  911\n",
            "             precision    recall  f1-score   support\n",
            "\n",
            "    class 0       0.69      0.78      0.73       535\n",
            "    class 1       0.81      0.72      0.76       681\n",
            "\n",
            "avg / total       0.76      0.75      0.75      1216\n",
            "\n",
            "confusion_matrix : \n",
            " [[419 116]\n",
            " [189 492]]\n",
            "Accuracy :  0.7491776315789473\n",
            "Sensitivity :  0.6891447368421053\n",
            "Specificity :  0.8092105263157895\n",
            "-------------------------------------------------------------------\n",
            "------------------ Evalute the validation set ------------------------\n",
            "-------------------------------------------------------------------\n",
            "validation loss: 4.775568729180557\n",
            "validation accuracy: 0.698076923076923\n",
            " y_pred_validation shape (520,)\n",
            "accuracy_score_validation with normalize=True:  0.698076923076923\n",
            "accuracy_score_validation with normalize=False :  363\n",
            "             precision    recall  f1-score   support\n",
            "\n",
            "    class 0       0.63      0.73      0.68       225\n",
            "    class 1       0.77      0.67      0.72       295\n",
            "\n",
            "avg / total       0.71      0.70      0.70       520\n",
            "\n",
            "confusion_matrix : \n",
            " [[164  61]\n",
            " [ 96 199]]\n",
            "Accuracy :  0.698076923076923\n",
            "Sensitivity :  0.6307692307692307\n",
            "Specificity :  0.7653846153846153\n",
            "-------------------------------------------------------------------\n",
            "------------------ Train Done -------------------------------------\n",
            "-------------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "YQqSAZC6v4_D",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Class 0 Vs Class 4"
      ]
    },
    {
      "metadata": {
        "id": "mr6xFG0guXic",
        "colab_type": "code",
        "outputId": "e9aec8fe-1135-41ca-f872-f7832245e223",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2772
        }
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf \n",
        "import keras\n",
        "import os\n",
        "\n",
        "from subprocess import check_output\n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "from PIL import Image\n",
        "            \n",
        "from keras.applications.densenet import preprocess_input     \n",
        "\n",
        "\n",
        "from keras.preprocessing import image\n",
        "\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "from sklearn.cross_validation import train_test_split\n",
        "\n",
        "from keras.utils import np_utils\n",
        "\n",
        "from keras.applications.densenet import DenseNet201    \n",
        "\n",
        "from keras.models import Model, load_model\n",
        "\n",
        "from keras.layers import Dense, Dropout, Reshape\n",
        "\n",
        "from keras.optimizers import SGD\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.metrics import classification_report,confusion_matrix\n",
        "\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "from keras.layers import Activation, Flatten \n",
        "from keras import backend as K\n",
        "from keras import applications\n",
        "\n",
        "\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0' #use GPU with ID=0\n",
        "config = tf.ConfigProto()\n",
        "config.gpu_options.per_process_gpu_memory_fraction = 0.7 # maximun alloc gpu50% of MEM\n",
        "sess = tf.Session(config = config)\n",
        "keras.backend.set_session(sess)\n",
        "\n",
        "\n",
        "# dimensions of our images.\n",
        "img_width, img_height = 224, 224\n",
        " \n",
        "top_model_weights_path = '/content/gdrive/My Drive/Colab Notebooks/bottleneck_fc_model_DenseNet201_PTN_data4.h5'\n",
        "\n",
        "train_data_dir = '/content/gdrive/My Drive/Colab Notebooks/data4_Train496_Test212/train'\n",
        "validation_data_dir = '/content/gdrive/My Drive/Colab Notebooks/data4_Train496_Test212/validation'\n",
        "nb_train_samples = 992\n",
        "nb_validation_samples = 424\n",
        "epochs = 50\n",
        "batch_size = 8  # batch size in flow_images_from_directory needs to correspond to the image number of the test images.\n",
        "\n",
        "\n",
        "def save_bottleneck_features():\n",
        "    datagen = ImageDataGenerator(rescale=1. / 255)\n",
        "\n",
        "    model = applications.DenseNet201(weights='imagenet', include_top=False)\n",
        "\n",
        "    #model.summary()\n",
        "    \n",
        "    generator = datagen.flow_from_directory(\n",
        "        train_data_dir,\n",
        "        target_size=(img_width, img_height),\n",
        "        batch_size=batch_size,\n",
        "        class_mode=None,\n",
        "        shuffle=False)\n",
        "    \n",
        "    bottleneck_features_train = model.predict_generator(\n",
        "        generator, nb_train_samples // batch_size)\n",
        "    np.save(open('DR1_bottleneck_features_train_DenseNet201_PTN_data4.npy', 'wb'),\n",
        "            bottleneck_features_train)\n",
        "\n",
        "    generator = datagen.flow_from_directory(\n",
        "        validation_data_dir,\n",
        "        target_size=(img_width, img_height),\n",
        "        batch_size=batch_size,\n",
        "        class_mode=None,\n",
        "        shuffle=False)\n",
        "    bottleneck_features_validation = model.predict_generator(\n",
        "        generator, nb_validation_samples // batch_size)\n",
        "    np.save(open('DR1_bottleneck_features_validation_DenseNet201_PTN_data4.npy', 'wb'),\n",
        "            bottleneck_features_validation)\n",
        "\n",
        "\n",
        "def train_top_model():\n",
        "    train_data = np.load(open('DR1_bottleneck_features_train_DenseNet201_PTN_data4.npy','rb'))\n",
        "    train_labels = np.array(\n",
        "        [0] * (nb_train_samples // 2) + [1] * (nb_train_samples // 2))\n",
        "    \n",
        "    print(' train_labels shape',train_labels.shape)\n",
        "\n",
        "    #print(' train_labels : \\n ',train_labels)\n",
        "\n",
        "    validation_data = np.load(open('DR1_bottleneck_features_validation_DenseNet201_PTN_data4.npy','rb'))\n",
        "    validation_labels = np.array(\n",
        "        [0] * (nb_validation_samples // 2) + [1] * (nb_validation_samples // 2))\n",
        "\n",
        "    print(' validation_labels shape',validation_labels.shape)\n",
        "\n",
        "    #print(' validation_labels : \\n ',validation_labels)\n",
        "\n",
        "    model = Sequential()\n",
        "    model.add(Flatten(input_shape=train_data.shape[1:]))\n",
        "    model.add(Dense(256, activation='relu'))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "    model.compile(optimizer='rmsprop',\n",
        "                  loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    model.fit(train_data, train_labels,\n",
        "              epochs=epochs,\n",
        "              batch_size=batch_size,\n",
        "              validation_data=(validation_data, validation_labels))\n",
        "    model.save_weights(top_model_weights_path)\n",
        "    \n",
        "    print('-------------------------------------------------------------------')\n",
        "    print('------------------ Evalute the train set ------------------------')\n",
        "    print('-------------------------------------------------------------------')\n",
        "\n",
        "    train_score = model.evaluate(train_data, train_labels, verbose=0) # nb_train_samples // batch_size, workers=1)\n",
        "    print('Train loss:', train_score[0])\n",
        "    print('Train accuracy:', train_score[1])\n",
        "\n",
        "    y_pred_train = np.squeeze(model.predict(train_data))\n",
        "    print(' y_pred_train shape',y_pred_train.shape)\n",
        "    #print(' y_pred_train : \\n ',y_pred_train)\n",
        "\n",
        "    threshold = 0.5\n",
        "    y_pred_train = (y_pred_train > threshold)*1\n",
        "    #y_pred_train.astype(int)   \n",
        "    #print(' y_pred_train again: \\n ',y_pred_train)\n",
        "    \n",
        "    accuracy_score_train = accuracy_score(train_labels, y_pred_train, normalize=True)\n",
        "    print('accuracy_score_train with normalize=True: ', accuracy_score_train)\n",
        "\n",
        "    accuracy_score_train = accuracy_score(train_labels, y_pred_train, normalize=False)\n",
        "    print('accuracy_score_train with normalize=False : ', accuracy_score_train)\n",
        "\n",
        "    target_names = ['class 0', 'class 1']\n",
        "\n",
        "    print(classification_report(y_pred_train, train_labels, target_names=target_names))\n",
        "\n",
        "    cm1 = confusion_matrix(y_pred_train, train_labels)\n",
        "    \n",
        "    print('confusion_matrix : \\n', cm1)\n",
        "\n",
        "    total1=sum(sum(cm1))\n",
        "\n",
        "    #####from confusion matrix calculate accuracy\n",
        "    accuracy1=(cm1[0,0]+cm1[1,1])/total1\n",
        "    print ('Accuracy : ', accuracy1)\n",
        "\n",
        "    sensitivity = cm1[0,0]/(cm1[0,0]+cm1[1,0])\n",
        "    print('Sensitivity : ', sensitivity )\n",
        "\n",
        "    Specificity = cm1[1,1]/(cm1[1,1]+cm1[0,1])\n",
        "    print('Specificity : ', Specificity )\n",
        "\n",
        "    print('-------------------------------------------------------------------')\n",
        "    print('------------------ Evalute the validation set ------------------------')\n",
        "    print('-------------------------------------------------------------------')\n",
        "\n",
        "    validation_score = model.evaluate(validation_data, validation_labels, verbose=0) # nb_train_samples // batch_size, workers=1)\n",
        "    print('validation loss:', validation_score[0])\n",
        "    print('validation accuracy:', validation_score[1])\n",
        "\n",
        "    y_pred_validation = np.squeeze(model.predict(validation_data))\n",
        "    print(' y_pred_validation shape',y_pred_validation.shape)\n",
        "    #print(' y_pred_validation : \\n ',y_pred_validation)\n",
        "\n",
        "    y_pred_validation = (y_pred_validation > threshold)*1\n",
        "    #y_pred_train.astype(int)   \n",
        "    #print(' y_pred_validation again: \\n ',y_pred_validation)\n",
        "    \n",
        "    accuracy_score_validation = accuracy_score(validation_labels, y_pred_validation, normalize=True)\n",
        "    print('accuracy_score_validation with normalize=True: ', accuracy_score_validation)\n",
        "\n",
        "    accuracy_score_validation = accuracy_score(validation_labels, y_pred_validation, normalize=False)\n",
        "    print('accuracy_score_validation with normalize=False : ', accuracy_score_validation)\n",
        "\n",
        "    print(classification_report(y_pred_validation, validation_labels, target_names=target_names))\n",
        "\n",
        "    cm1 = confusion_matrix(y_pred_validation, validation_labels)\n",
        "    \n",
        "    print('confusion_matrix : \\n', cm1)\n",
        "\n",
        "    total1=sum(sum(cm1))\n",
        "\n",
        "    #####from confusion matrix calculate accuracy\n",
        "    accuracy1=(cm1[0,0]+cm1[1,1])/total1\n",
        "    print ('Accuracy : ', accuracy1)\n",
        "\n",
        "    sensitivity = cm1[0,0]/(cm1[0,0]+cm1[1,0])\n",
        "    print('Sensitivity : ', sensitivity )\n",
        "\n",
        "    Specificity = cm1[1,1]/(cm1[1,1]+cm1[0,1])\n",
        "    print('Specificity : ', Specificity )\n",
        "\n",
        "\n",
        "save_bottleneck_features()\n",
        "train_top_model()\n",
        "\n",
        "print('-------------------------------------------------------------------')\n",
        "print('------------------ Train Done -------------------------------------')\n",
        "print('-------------------------------------------------------------------')\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 996 images belonging to 2 classes.\n",
            "Found 424 images belonging to 2 classes.\n",
            " train_labels shape (992,)\n",
            " validation_labels shape (424,)\n",
            "Train on 992 samples, validate on 424 samples\n",
            "Epoch 1/50\n",
            "992/992 [==============================] - 10s 10ms/step - loss: 6.6915 - acc: 0.5716 - val_loss: 4.5156 - val_acc: 0.7028\n",
            "Epoch 2/50\n",
            "992/992 [==============================] - 4s 4ms/step - loss: 4.6441 - acc: 0.6986 - val_loss: 4.5302 - val_acc: 0.7146\n",
            "Epoch 3/50\n",
            "992/992 [==============================] - 4s 4ms/step - loss: 4.2788 - acc: 0.7268 - val_loss: 4.7961 - val_acc: 0.6910\n",
            "Epoch 4/50\n",
            "992/992 [==============================] - 4s 4ms/step - loss: 4.4289 - acc: 0.7177 - val_loss: 4.5464 - val_acc: 0.7123\n",
            "Epoch 5/50\n",
            "992/992 [==============================] - 4s 4ms/step - loss: 3.9441 - acc: 0.7450 - val_loss: 3.1569 - val_acc: 0.7925\n",
            "Epoch 6/50\n",
            "992/992 [==============================] - 4s 4ms/step - loss: 4.0645 - acc: 0.7399 - val_loss: 3.2903 - val_acc: 0.7877\n",
            "Epoch 7/50\n",
            "992/992 [==============================] - 4s 4ms/step - loss: 3.9946 - acc: 0.7490 - val_loss: 3.4426 - val_acc: 0.7807\n",
            "Epoch 8/50\n",
            "992/992 [==============================] - 4s 4ms/step - loss: 3.5485 - acc: 0.7712 - val_loss: 4.1403 - val_acc: 0.7358\n",
            "Epoch 9/50\n",
            "992/992 [==============================] - 4s 4ms/step - loss: 4.1637 - acc: 0.7319 - val_loss: 5.7679 - val_acc: 0.6415\n",
            "Epoch 10/50\n",
            "992/992 [==============================] - 4s 4ms/step - loss: 3.7248 - acc: 0.7601 - val_loss: 5.8385 - val_acc: 0.6297\n",
            "Epoch 11/50\n",
            "992/992 [==============================] - 4s 4ms/step - loss: 3.8050 - acc: 0.7591 - val_loss: 3.2380 - val_acc: 0.7925\n",
            "Epoch 12/50\n",
            "992/992 [==============================] - 4s 4ms/step - loss: 3.5730 - acc: 0.7702 - val_loss: 3.4026 - val_acc: 0.7830\n",
            "Epoch 13/50\n",
            "992/992 [==============================] - 4s 4ms/step - loss: 4.0876 - acc: 0.7429 - val_loss: 3.4572 - val_acc: 0.7759\n",
            "Epoch 14/50\n",
            "992/992 [==============================] - 4s 4ms/step - loss: 3.7727 - acc: 0.7591 - val_loss: 3.2953 - val_acc: 0.7925\n",
            "Epoch 15/50\n",
            "992/992 [==============================] - 4s 4ms/step - loss: 3.5386 - acc: 0.7722 - val_loss: 3.2900 - val_acc: 0.7925\n",
            "Epoch 16/50\n",
            "992/992 [==============================] - 4s 4ms/step - loss: 3.3767 - acc: 0.7843 - val_loss: 3.4235 - val_acc: 0.7830\n",
            "Epoch 17/50\n",
            "992/992 [==============================] - 4s 4ms/step - loss: 3.4647 - acc: 0.7802 - val_loss: 3.5463 - val_acc: 0.7736\n",
            "Epoch 18/50\n",
            "992/992 [==============================] - 4s 4ms/step - loss: 3.4401 - acc: 0.7823 - val_loss: 3.5944 - val_acc: 0.7759\n",
            "Epoch 19/50\n",
            "992/992 [==============================] - 4s 4ms/step - loss: 3.8323 - acc: 0.7530 - val_loss: 5.9270 - val_acc: 0.6321\n",
            "Epoch 20/50\n",
            "992/992 [==============================] - 4s 4ms/step - loss: 4.1262 - acc: 0.7389 - val_loss: 3.6108 - val_acc: 0.7712\n",
            "Epoch 21/50\n",
            "992/992 [==============================] - 4s 4ms/step - loss: 3.3328 - acc: 0.7903 - val_loss: 3.0205 - val_acc: 0.8090\n",
            "Epoch 22/50\n",
            "992/992 [==============================] - 4s 4ms/step - loss: 3.4161 - acc: 0.7823 - val_loss: 2.9972 - val_acc: 0.8113\n",
            "Epoch 23/50\n",
            "992/992 [==============================] - 4s 4ms/step - loss: 3.4463 - acc: 0.7812 - val_loss: 3.1470 - val_acc: 0.8019\n",
            "Epoch 24/50\n",
            "992/992 [==============================] - 4s 4ms/step - loss: 3.1828 - acc: 0.7974 - val_loss: 3.2793 - val_acc: 0.7925\n",
            "Epoch 25/50\n",
            "992/992 [==============================] - 4s 4ms/step - loss: 3.1087 - acc: 0.8044 - val_loss: 3.0313 - val_acc: 0.8090\n",
            "Epoch 26/50\n",
            "992/992 [==============================] - 4s 4ms/step - loss: 3.2168 - acc: 0.7974 - val_loss: 2.9635 - val_acc: 0.8113\n",
            "Epoch 27/50\n",
            "992/992 [==============================] - 4s 4ms/step - loss: 2.9193 - acc: 0.8165 - val_loss: 3.1640 - val_acc: 0.7972\n",
            "Epoch 28/50\n",
            "992/992 [==============================] - 4s 4ms/step - loss: 3.1177 - acc: 0.8044 - val_loss: 3.1556 - val_acc: 0.8019\n",
            "Epoch 29/50\n",
            "992/992 [==============================] - 4s 4ms/step - loss: 3.0532 - acc: 0.8065 - val_loss: 3.2007 - val_acc: 0.7995\n",
            "Epoch 30/50\n",
            "992/992 [==============================] - 4s 4ms/step - loss: 3.0019 - acc: 0.8115 - val_loss: 3.2122 - val_acc: 0.7995\n",
            "Epoch 31/50\n",
            "992/992 [==============================] - 4s 4ms/step - loss: 2.9816 - acc: 0.8095 - val_loss: 3.9444 - val_acc: 0.7500\n",
            "Epoch 32/50\n",
            "992/992 [==============================] - 4s 4ms/step - loss: 3.3008 - acc: 0.7923 - val_loss: 4.4416 - val_acc: 0.7241\n",
            "Epoch 33/50\n",
            "992/992 [==============================] - 4s 4ms/step - loss: 3.5707 - acc: 0.7762 - val_loss: 3.4367 - val_acc: 0.7854\n",
            "Epoch 34/50\n",
            "992/992 [==============================] - 4s 4ms/step - loss: 3.5329 - acc: 0.7732 - val_loss: 3.6769 - val_acc: 0.7689\n",
            "Epoch 35/50\n",
            "992/992 [==============================] - 4s 4ms/step - loss: 3.0353 - acc: 0.8085 - val_loss: 3.0653 - val_acc: 0.8066\n",
            "Epoch 36/50\n",
            "992/992 [==============================] - 4s 4ms/step - loss: 3.0022 - acc: 0.8095 - val_loss: 3.3712 - val_acc: 0.7877\n",
            "Epoch 37/50\n",
            "992/992 [==============================] - 4s 4ms/step - loss: 3.2538 - acc: 0.7964 - val_loss: 5.3022 - val_acc: 0.6651\n",
            "Epoch 38/50\n",
            "992/992 [==============================] - 4s 4ms/step - loss: 3.8850 - acc: 0.7560 - val_loss: 3.3699 - val_acc: 0.7854\n",
            "Epoch 39/50\n",
            "992/992 [==============================] - 4s 4ms/step - loss: 3.1373 - acc: 0.8014 - val_loss: 3.2573 - val_acc: 0.7948\n",
            "Epoch 40/50\n",
            "992/992 [==============================] - 4s 4ms/step - loss: 3.1644 - acc: 0.8024 - val_loss: 3.0242 - val_acc: 0.8113\n",
            "Epoch 41/50\n",
            "992/992 [==============================] - 4s 4ms/step - loss: 2.9683 - acc: 0.8125 - val_loss: 3.4265 - val_acc: 0.7854\n",
            "Epoch 42/50\n",
            "992/992 [==============================] - 4s 4ms/step - loss: 3.0212 - acc: 0.8105 - val_loss: 3.1353 - val_acc: 0.8042\n",
            "Epoch 43/50\n",
            "992/992 [==============================] - 4s 4ms/step - loss: 2.7944 - acc: 0.8236 - val_loss: 3.2473 - val_acc: 0.7972\n",
            "Epoch 44/50\n",
            "992/992 [==============================] - 4s 4ms/step - loss: 2.8570 - acc: 0.8196 - val_loss: 2.8734 - val_acc: 0.8184\n",
            "Epoch 45/50\n",
            "992/992 [==============================] - 4s 4ms/step - loss: 2.8136 - acc: 0.8226 - val_loss: 3.7603 - val_acc: 0.7642\n",
            "Epoch 46/50\n",
            "992/992 [==============================] - 4s 4ms/step - loss: 2.9858 - acc: 0.8125 - val_loss: 3.0399 - val_acc: 0.8090\n",
            "Epoch 47/50\n",
            "992/992 [==============================] - 4s 4ms/step - loss: 3.0008 - acc: 0.8105 - val_loss: 4.1225 - val_acc: 0.7382\n",
            "Epoch 48/50\n",
            "992/992 [==============================] - 4s 4ms/step - loss: 3.2388 - acc: 0.7954 - val_loss: 3.3664 - val_acc: 0.7901\n",
            "Epoch 49/50\n",
            "992/992 [==============================] - 4s 4ms/step - loss: 3.0537 - acc: 0.8044 - val_loss: 3.4120 - val_acc: 0.7854\n",
            "Epoch 50/50\n",
            "992/992 [==============================] - 4s 4ms/step - loss: 2.8773 - acc: 0.8196 - val_loss: 3.6435 - val_acc: 0.7689\n",
            "-------------------------------------------------------------------\n",
            "------------------ Evalute the train set ------------------------\n",
            "-------------------------------------------------------------------\n",
            "Train loss: 2.914128166052603\n",
            "Train accuracy: 0.8185483870967742\n",
            " y_pred_train shape (992,)\n",
            "accuracy_score_train with normalize=True:  0.8185483870967742\n",
            "accuracy_score_train with normalize=False :  812\n",
            "             precision    recall  f1-score   support\n",
            "\n",
            "    class 0       0.90      0.77      0.83       576\n",
            "    class 1       0.74      0.88      0.80       416\n",
            "\n",
            "avg / total       0.83      0.82      0.82       992\n",
            "\n",
            "confusion_matrix : \n",
            " [[446 130]\n",
            " [ 50 366]]\n",
            "Accuracy :  0.8185483870967742\n",
            "Sensitivity :  0.8991935483870968\n",
            "Specificity :  0.7379032258064516\n",
            "-------------------------------------------------------------------\n",
            "------------------ Evalute the validation set ------------------------\n",
            "-------------------------------------------------------------------\n",
            "validation loss: 3.643498279013724\n",
            "validation accuracy: 0.7688679245283019\n",
            " y_pred_validation shape (424,)\n",
            "accuracy_score_validation with normalize=True:  0.7688679245283019\n",
            "accuracy_score_validation with normalize=False :  326\n",
            "             precision    recall  f1-score   support\n",
            "\n",
            "    class 0       0.87      0.72      0.79       254\n",
            "    class 1       0.67      0.84      0.74       170\n",
            "\n",
            "avg / total       0.79      0.77      0.77       424\n",
            "\n",
            "confusion_matrix : \n",
            " [[184  70]\n",
            " [ 28 142]]\n",
            "Accuracy :  0.7688679245283019\n",
            "Sensitivity :  0.8679245283018868\n",
            "Specificity :  0.6698113207547169\n",
            "-------------------------------------------------------------------\n",
            "------------------ Train Done -------------------------------------\n",
            "-------------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "XyrcK8mFuXiv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "scrolled": false,
        "id": "Ps0BgU7RuXi8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OW_FyVIluXjH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KrIVAQaGuXjP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}