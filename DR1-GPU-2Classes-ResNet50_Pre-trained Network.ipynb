{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Without Pre Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.2/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "94658560/94653016 [==============================] - 440s 5us/step\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_6 (InputLayer)            (None, None, None, 3 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1_pad (ZeroPadding2D)       (None, None, None, 3 0           input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1 (Conv2D)                  (None, None, None, 6 9472        conv1_pad[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "bn_conv1 (BatchNormalization)   (None, None, None, 6 256         conv1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_246 (Activation)     (None, None, None, 6 0           bn_conv1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2D)  (None, None, None, 6 0           activation_246[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "res2a_branch2a (Conv2D)         (None, None, None, 6 4160        max_pooling2d_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "bn2a_branch2a (BatchNormalizati (None, None, None, 6 256         res2a_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_247 (Activation)     (None, None, None, 6 0           bn2a_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res2a_branch2b (Conv2D)         (None, None, None, 6 36928       activation_247[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn2a_branch2b (BatchNormalizati (None, None, None, 6 256         res2a_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_248 (Activation)     (None, None, None, 6 0           bn2a_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res2a_branch2c (Conv2D)         (None, None, None, 2 16640       activation_248[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "res2a_branch1 (Conv2D)          (None, None, None, 2 16640       max_pooling2d_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "bn2a_branch2c (BatchNormalizati (None, None, None, 2 1024        res2a_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn2a_branch1 (BatchNormalizatio (None, None, None, 2 1024        res2a_branch1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_81 (Add)                    (None, None, None, 2 0           bn2a_branch2c[0][0]              \n",
      "                                                                 bn2a_branch1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_249 (Activation)     (None, None, None, 2 0           add_81[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res2b_branch2a (Conv2D)         (None, None, None, 6 16448       activation_249[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn2b_branch2a (BatchNormalizati (None, None, None, 6 256         res2b_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_250 (Activation)     (None, None, None, 6 0           bn2b_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res2b_branch2b (Conv2D)         (None, None, None, 6 36928       activation_250[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn2b_branch2b (BatchNormalizati (None, None, None, 6 256         res2b_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_251 (Activation)     (None, None, None, 6 0           bn2b_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res2b_branch2c (Conv2D)         (None, None, None, 2 16640       activation_251[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn2b_branch2c (BatchNormalizati (None, None, None, 2 1024        res2b_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_82 (Add)                    (None, None, None, 2 0           bn2b_branch2c[0][0]              \n",
      "                                                                 activation_249[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_252 (Activation)     (None, None, None, 2 0           add_82[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res2c_branch2a (Conv2D)         (None, None, None, 6 16448       activation_252[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn2c_branch2a (BatchNormalizati (None, None, None, 6 256         res2c_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_253 (Activation)     (None, None, None, 6 0           bn2c_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res2c_branch2b (Conv2D)         (None, None, None, 6 36928       activation_253[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn2c_branch2b (BatchNormalizati (None, None, None, 6 256         res2c_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_254 (Activation)     (None, None, None, 6 0           bn2c_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res2c_branch2c (Conv2D)         (None, None, None, 2 16640       activation_254[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn2c_branch2c (BatchNormalizati (None, None, None, 2 1024        res2c_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_83 (Add)                    (None, None, None, 2 0           bn2c_branch2c[0][0]              \n",
      "                                                                 activation_252[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_255 (Activation)     (None, None, None, 2 0           add_83[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res3a_branch2a (Conv2D)         (None, None, None, 1 32896       activation_255[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn3a_branch2a (BatchNormalizati (None, None, None, 1 512         res3a_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_256 (Activation)     (None, None, None, 1 0           bn3a_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3a_branch2b (Conv2D)         (None, None, None, 1 147584      activation_256[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn3a_branch2b (BatchNormalizati (None, None, None, 1 512         res3a_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_257 (Activation)     (None, None, None, 1 0           bn3a_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3a_branch2c (Conv2D)         (None, None, None, 5 66048       activation_257[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "res3a_branch1 (Conv2D)          (None, None, None, 5 131584      activation_255[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn3a_branch2c (BatchNormalizati (None, None, None, 5 2048        res3a_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn3a_branch1 (BatchNormalizatio (None, None, None, 5 2048        res3a_branch1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_84 (Add)                    (None, None, None, 5 0           bn3a_branch2c[0][0]              \n",
      "                                                                 bn3a_branch1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_258 (Activation)     (None, None, None, 5 0           add_84[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res3b_branch2a (Conv2D)         (None, None, None, 1 65664       activation_258[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn3b_branch2a (BatchNormalizati (None, None, None, 1 512         res3b_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_259 (Activation)     (None, None, None, 1 0           bn3b_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3b_branch2b (Conv2D)         (None, None, None, 1 147584      activation_259[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn3b_branch2b (BatchNormalizati (None, None, None, 1 512         res3b_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_260 (Activation)     (None, None, None, 1 0           bn3b_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3b_branch2c (Conv2D)         (None, None, None, 5 66048       activation_260[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn3b_branch2c (BatchNormalizati (None, None, None, 5 2048        res3b_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_85 (Add)                    (None, None, None, 5 0           bn3b_branch2c[0][0]              \n",
      "                                                                 activation_258[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_261 (Activation)     (None, None, None, 5 0           add_85[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res3c_branch2a (Conv2D)         (None, None, None, 1 65664       activation_261[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn3c_branch2a (BatchNormalizati (None, None, None, 1 512         res3c_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_262 (Activation)     (None, None, None, 1 0           bn3c_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3c_branch2b (Conv2D)         (None, None, None, 1 147584      activation_262[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn3c_branch2b (BatchNormalizati (None, None, None, 1 512         res3c_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_263 (Activation)     (None, None, None, 1 0           bn3c_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3c_branch2c (Conv2D)         (None, None, None, 5 66048       activation_263[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn3c_branch2c (BatchNormalizati (None, None, None, 5 2048        res3c_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_86 (Add)                    (None, None, None, 5 0           bn3c_branch2c[0][0]              \n",
      "                                                                 activation_261[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_264 (Activation)     (None, None, None, 5 0           add_86[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res3d_branch2a (Conv2D)         (None, None, None, 1 65664       activation_264[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn3d_branch2a (BatchNormalizati (None, None, None, 1 512         res3d_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_265 (Activation)     (None, None, None, 1 0           bn3d_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3d_branch2b (Conv2D)         (None, None, None, 1 147584      activation_265[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn3d_branch2b (BatchNormalizati (None, None, None, 1 512         res3d_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_266 (Activation)     (None, None, None, 1 0           bn3d_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3d_branch2c (Conv2D)         (None, None, None, 5 66048       activation_266[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn3d_branch2c (BatchNormalizati (None, None, None, 5 2048        res3d_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_87 (Add)                    (None, None, None, 5 0           bn3d_branch2c[0][0]              \n",
      "                                                                 activation_264[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_267 (Activation)     (None, None, None, 5 0           add_87[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res4a_branch2a (Conv2D)         (None, None, None, 2 131328      activation_267[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn4a_branch2a (BatchNormalizati (None, None, None, 2 1024        res4a_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_268 (Activation)     (None, None, None, 2 0           bn4a_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4a_branch2b (Conv2D)         (None, None, None, 2 590080      activation_268[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn4a_branch2b (BatchNormalizati (None, None, None, 2 1024        res4a_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_269 (Activation)     (None, None, None, 2 0           bn4a_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4a_branch2c (Conv2D)         (None, None, None, 1 263168      activation_269[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "res4a_branch1 (Conv2D)          (None, None, None, 1 525312      activation_267[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn4a_branch2c (BatchNormalizati (None, None, None, 1 4096        res4a_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn4a_branch1 (BatchNormalizatio (None, None, None, 1 4096        res4a_branch1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_88 (Add)                    (None, None, None, 1 0           bn4a_branch2c[0][0]              \n",
      "                                                                 bn4a_branch1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_270 (Activation)     (None, None, None, 1 0           add_88[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res4b_branch2a (Conv2D)         (None, None, None, 2 262400      activation_270[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn4b_branch2a (BatchNormalizati (None, None, None, 2 1024        res4b_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_271 (Activation)     (None, None, None, 2 0           bn4b_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4b_branch2b (Conv2D)         (None, None, None, 2 590080      activation_271[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn4b_branch2b (BatchNormalizati (None, None, None, 2 1024        res4b_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_272 (Activation)     (None, None, None, 2 0           bn4b_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4b_branch2c (Conv2D)         (None, None, None, 1 263168      activation_272[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn4b_branch2c (BatchNormalizati (None, None, None, 1 4096        res4b_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_89 (Add)                    (None, None, None, 1 0           bn4b_branch2c[0][0]              \n",
      "                                                                 activation_270[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_273 (Activation)     (None, None, None, 1 0           add_89[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res4c_branch2a (Conv2D)         (None, None, None, 2 262400      activation_273[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn4c_branch2a (BatchNormalizati (None, None, None, 2 1024        res4c_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_274 (Activation)     (None, None, None, 2 0           bn4c_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4c_branch2b (Conv2D)         (None, None, None, 2 590080      activation_274[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn4c_branch2b (BatchNormalizati (None, None, None, 2 1024        res4c_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_275 (Activation)     (None, None, None, 2 0           bn4c_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4c_branch2c (Conv2D)         (None, None, None, 1 263168      activation_275[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn4c_branch2c (BatchNormalizati (None, None, None, 1 4096        res4c_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_90 (Add)                    (None, None, None, 1 0           bn4c_branch2c[0][0]              \n",
      "                                                                 activation_273[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_276 (Activation)     (None, None, None, 1 0           add_90[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res4d_branch2a (Conv2D)         (None, None, None, 2 262400      activation_276[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn4d_branch2a (BatchNormalizati (None, None, None, 2 1024        res4d_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_277 (Activation)     (None, None, None, 2 0           bn4d_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4d_branch2b (Conv2D)         (None, None, None, 2 590080      activation_277[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn4d_branch2b (BatchNormalizati (None, None, None, 2 1024        res4d_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_278 (Activation)     (None, None, None, 2 0           bn4d_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4d_branch2c (Conv2D)         (None, None, None, 1 263168      activation_278[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn4d_branch2c (BatchNormalizati (None, None, None, 1 4096        res4d_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_91 (Add)                    (None, None, None, 1 0           bn4d_branch2c[0][0]              \n",
      "                                                                 activation_276[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_279 (Activation)     (None, None, None, 1 0           add_91[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res4e_branch2a (Conv2D)         (None, None, None, 2 262400      activation_279[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn4e_branch2a (BatchNormalizati (None, None, None, 2 1024        res4e_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_280 (Activation)     (None, None, None, 2 0           bn4e_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4e_branch2b (Conv2D)         (None, None, None, 2 590080      activation_280[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn4e_branch2b (BatchNormalizati (None, None, None, 2 1024        res4e_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_281 (Activation)     (None, None, None, 2 0           bn4e_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4e_branch2c (Conv2D)         (None, None, None, 1 263168      activation_281[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn4e_branch2c (BatchNormalizati (None, None, None, 1 4096        res4e_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_92 (Add)                    (None, None, None, 1 0           bn4e_branch2c[0][0]              \n",
      "                                                                 activation_279[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_282 (Activation)     (None, None, None, 1 0           add_92[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res4f_branch2a (Conv2D)         (None, None, None, 2 262400      activation_282[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn4f_branch2a (BatchNormalizati (None, None, None, 2 1024        res4f_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_283 (Activation)     (None, None, None, 2 0           bn4f_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4f_branch2b (Conv2D)         (None, None, None, 2 590080      activation_283[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn4f_branch2b (BatchNormalizati (None, None, None, 2 1024        res4f_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_284 (Activation)     (None, None, None, 2 0           bn4f_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4f_branch2c (Conv2D)         (None, None, None, 1 263168      activation_284[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn4f_branch2c (BatchNormalizati (None, None, None, 1 4096        res4f_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_93 (Add)                    (None, None, None, 1 0           bn4f_branch2c[0][0]              \n",
      "                                                                 activation_282[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_285 (Activation)     (None, None, None, 1 0           add_93[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res5a_branch2a (Conv2D)         (None, None, None, 5 524800      activation_285[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn5a_branch2a (BatchNormalizati (None, None, None, 5 2048        res5a_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_286 (Activation)     (None, None, None, 5 0           bn5a_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res5a_branch2b (Conv2D)         (None, None, None, 5 2359808     activation_286[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn5a_branch2b (BatchNormalizati (None, None, None, 5 2048        res5a_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_287 (Activation)     (None, None, None, 5 0           bn5a_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res5a_branch2c (Conv2D)         (None, None, None, 2 1050624     activation_287[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "res5a_branch1 (Conv2D)          (None, None, None, 2 2099200     activation_285[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn5a_branch2c (BatchNormalizati (None, None, None, 2 8192        res5a_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn5a_branch1 (BatchNormalizatio (None, None, None, 2 8192        res5a_branch1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_94 (Add)                    (None, None, None, 2 0           bn5a_branch2c[0][0]              \n",
      "                                                                 bn5a_branch1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_288 (Activation)     (None, None, None, 2 0           add_94[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res5b_branch2a (Conv2D)         (None, None, None, 5 1049088     activation_288[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn5b_branch2a (BatchNormalizati (None, None, None, 5 2048        res5b_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_289 (Activation)     (None, None, None, 5 0           bn5b_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res5b_branch2b (Conv2D)         (None, None, None, 5 2359808     activation_289[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn5b_branch2b (BatchNormalizati (None, None, None, 5 2048        res5b_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_290 (Activation)     (None, None, None, 5 0           bn5b_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res5b_branch2c (Conv2D)         (None, None, None, 2 1050624     activation_290[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn5b_branch2c (BatchNormalizati (None, None, None, 2 8192        res5b_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_95 (Add)                    (None, None, None, 2 0           bn5b_branch2c[0][0]              \n",
      "                                                                 activation_288[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_291 (Activation)     (None, None, None, 2 0           add_95[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res5c_branch2a (Conv2D)         (None, None, None, 5 1049088     activation_291[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn5c_branch2a (BatchNormalizati (None, None, None, 5 2048        res5c_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_292 (Activation)     (None, None, None, 5 0           bn5c_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res5c_branch2b (Conv2D)         (None, None, None, 5 2359808     activation_292[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn5c_branch2b (BatchNormalizati (None, None, None, 5 2048        res5c_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_293 (Activation)     (None, None, None, 5 0           bn5c_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res5c_branch2c (Conv2D)         (None, None, None, 2 1050624     activation_293[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn5c_branch2c (BatchNormalizati (None, None, None, 2 8192        res5c_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_96 (Add)                    (None, None, None, 2 0           bn5c_branch2c[0][0]              \n",
      "                                                                 activation_291[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_294 (Activation)     (None, None, None, 2 0           add_96[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "avg_pool (AveragePooling2D)     (None, None, None, 2 0           activation_294[0][0]             \n",
      "==================================================================================================\n",
      "Total params: 23,587,712\n",
      "Trainable params: 23,534,592\n",
      "Non-trainable params: 53,120\n",
      "__________________________________________________________________________________________________\n",
      "Found 1000 images belonging to 2 classes.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400 images belonging to 2 classes.\n",
      " train_labels shape (1000,)\n",
      " train_labels : \n",
      "  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1]\n",
      " validation_labels shape (400,)\n",
      " validation_labels : \n",
      "  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "Train on 1000 samples, validate on 400 samples\n",
      "Epoch 1/50\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 0.7932 - acc: 0.5050 - val_loss: 0.6947 - val_acc: 0.5000\n",
      "Epoch 2/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.7016 - acc: 0.4950 - val_loss: 0.6930 - val_acc: 0.5000\n",
      "Epoch 3/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.6961 - acc: 0.4730 - val_loss: 0.6931 - val_acc: 0.5000\n",
      "Epoch 4/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.6947 - acc: 0.5070 - val_loss: 0.6931 - val_acc: 0.5000\n",
      "Epoch 5/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.6938 - acc: 0.5120 - val_loss: 0.6931 - val_acc: 0.5000\n",
      "Epoch 6/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.6935 - acc: 0.4860 - val_loss: 0.6932 - val_acc: 0.5000\n",
      "Epoch 7/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.6932 - acc: 0.5000 - val_loss: 0.6932 - val_acc: 0.5000\n",
      "Epoch 8/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.6932 - acc: 0.5090 - val_loss: 0.6931 - val_acc: 0.5000\n",
      "Epoch 9/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.6932 - acc: 0.5130 - val_loss: 0.6931 - val_acc: 0.5000\n",
      "Epoch 10/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.6936 - acc: 0.4930 - val_loss: 0.6931 - val_acc: 0.4975\n",
      "Epoch 11/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.6933 - acc: 0.5010 - val_loss: 0.6932 - val_acc: 0.5000\n",
      "Epoch 12/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.6932 - acc: 0.5000 - val_loss: 0.6932 - val_acc: 0.5000\n",
      "Epoch 13/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.6932 - acc: 0.5000 - val_loss: 0.6932 - val_acc: 0.5000\n",
      "Epoch 14/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.6932 - acc: 0.5000 - val_loss: 0.6931 - val_acc: 0.5000\n",
      "Epoch 15/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.6932 - acc: 0.4840 - val_loss: 0.6931 - val_acc: 0.5000\n",
      "Epoch 16/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.6932 - acc: 0.4580 - val_loss: 0.6931 - val_acc: 0.5000\n",
      "Epoch 17/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.6932 - acc: 0.4880 - val_loss: 0.6931 - val_acc: 0.5000\n",
      "Epoch 18/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.6932 - acc: 0.5000 - val_loss: 0.6931 - val_acc: 0.5000\n",
      "Epoch 19/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.6932 - acc: 0.4840 - val_loss: 0.6931 - val_acc: 0.5000\n",
      "Epoch 20/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.6932 - acc: 0.4740 - val_loss: 0.6931 - val_acc: 0.5000\n",
      "Epoch 21/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.6933 - acc: 0.4920 - val_loss: 0.6931 - val_acc: 0.5000\n",
      "Epoch 22/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.6932 - acc: 0.4920 - val_loss: 0.6931 - val_acc: 0.5000\n",
      "Epoch 23/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.6932 - acc: 0.4820 - val_loss: 0.6931 - val_acc: 0.5000\n",
      "Epoch 24/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.6932 - acc: 0.4920 - val_loss: 0.6931 - val_acc: 0.5000\n",
      "Epoch 25/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.6932 - acc: 0.4900 - val_loss: 0.6931 - val_acc: 0.5000\n",
      "Epoch 26/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.6932 - acc: 0.4720 - val_loss: 0.6931 - val_acc: 0.5000\n",
      "Epoch 27/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.6932 - acc: 0.4720 - val_loss: 0.6931 - val_acc: 0.5000\n",
      "Epoch 28/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.6932 - acc: 0.4740 - val_loss: 0.6931 - val_acc: 0.5000\n",
      "Epoch 29/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.6932 - acc: 0.4980 - val_loss: 0.6931 - val_acc: 0.5000\n",
      "Epoch 30/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.6932 - acc: 0.4980 - val_loss: 0.6931 - val_acc: 0.5000\n",
      "Epoch 31/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.6932 - acc: 0.4740 - val_loss: 0.6931 - val_acc: 0.5000\n",
      "Epoch 32/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.6932 - acc: 0.4880 - val_loss: 0.6931 - val_acc: 0.5000\n",
      "Epoch 33/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.6932 - acc: 0.4680 - val_loss: 0.6931 - val_acc: 0.5000\n",
      "Epoch 34/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.6932 - acc: 0.4700 - val_loss: 0.6931 - val_acc: 0.5000\n",
      "Epoch 35/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.6932 - acc: 0.4860 - val_loss: 0.6931 - val_acc: 0.5000\n",
      "Epoch 36/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.6932 - acc: 0.5000 - val_loss: 0.6931 - val_acc: 0.5000\n",
      "Epoch 37/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.6932 - acc: 0.4900 - val_loss: 0.6931 - val_acc: 0.5000\n",
      "Epoch 38/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.6932 - acc: 0.5020 - val_loss: 0.6931 - val_acc: 0.5000\n",
      "Epoch 39/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.6932 - acc: 0.4940 - val_loss: 0.6931 - val_acc: 0.5000\n",
      "Epoch 40/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.6932 - acc: 0.4900 - val_loss: 0.6932 - val_acc: 0.5000\n",
      "Epoch 41/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.6932 - acc: 0.4960 - val_loss: 0.6932 - val_acc: 0.5000\n",
      "Epoch 42/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.6932 - acc: 0.4820 - val_loss: 0.6932 - val_acc: 0.5000\n",
      "Epoch 43/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.6933 - acc: 0.5000 - val_loss: 0.6932 - val_acc: 0.5000\n",
      "Epoch 44/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.6932 - acc: 0.4960 - val_loss: 0.6931 - val_acc: 0.5000\n",
      "Epoch 45/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.6932 - acc: 0.4920 - val_loss: 0.6931 - val_acc: 0.5000\n",
      "Epoch 46/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.6932 - acc: 0.4820 - val_loss: 0.6931 - val_acc: 0.5000\n",
      "Epoch 47/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.6932 - acc: 0.4780 - val_loss: 0.6931 - val_acc: 0.5000\n",
      "Epoch 48/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.6932 - acc: 0.4780 - val_loss: 0.6931 - val_acc: 0.5000\n",
      "Epoch 49/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.6932 - acc: 0.4780 - val_loss: 0.6931 - val_acc: 0.5000\n",
      "Epoch 50/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.6932 - acc: 0.5000 - val_loss: 0.6931 - val_acc: 0.5000\n",
      "-------------------------------------------------------------------\n",
      "------------------ Evalute the train set ------------------------\n",
      "-------------------------------------------------------------------\n",
      "Train loss: 0.6931473307609558\n",
      "Train accuracy: 0.5\n",
      " y_pred_train shape (1000,)\n",
      " y_pred_train : \n",
      "  [0.49972472 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472\n",
      " 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472\n",
      " 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472\n",
      " 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472\n",
      " 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472\n",
      " 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472\n",
      " 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472\n",
      " 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472\n",
      " 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472\n",
      " 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472\n",
      " 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472\n",
      " 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472\n",
      " 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472\n",
      " 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472\n",
      " 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472\n",
      " 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472\n",
      " 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472\n",
      " 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472\n",
      " 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472\n",
      " 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472\n",
      " 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472\n",
      " 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472\n",
      " 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472\n",
      " 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472\n",
      " 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472\n",
      " 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472\n",
      " 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472\n",
      " 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472\n",
      " 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472\n",
      " 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472\n",
      " 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472\n",
      " 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472\n",
      " 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472\n",
      " 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472\n",
      " 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472\n",
      " 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472\n",
      " 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472\n",
      " 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472\n",
      " 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472\n",
      " 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472\n",
      " 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472\n",
      " 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472\n",
      " 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472\n",
      " 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472\n",
      " 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472\n",
      " 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472\n",
      " 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472\n",
      " 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472\n",
      " 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472\n",
      " 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472\n",
      " 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472\n",
      " 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472\n",
      " 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472\n",
      " 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472\n",
      " 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472\n",
      " 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472\n",
      " 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472\n",
      " 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472\n",
      " 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472\n",
      " 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472\n",
      " 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472\n",
      " 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472\n",
      " 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472\n",
      " 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472\n",
      " 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472\n",
      " 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472\n",
      " 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472\n",
      " 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472\n",
      " 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472\n",
      " 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472\n",
      " 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472\n",
      " 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472\n",
      " 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472\n",
      " 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472\n",
      " 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472\n",
      " 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472\n",
      " 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472\n",
      " 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472\n",
      " 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472\n",
      " 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472\n",
      " 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472\n",
      " 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472\n",
      " 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472\n",
      " 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472\n",
      " 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472\n",
      " 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472\n",
      " 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472\n",
      " 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472\n",
      " 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472\n",
      " 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472\n",
      " 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472\n",
      " 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472\n",
      " 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472\n",
      " 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472\n",
      " 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472\n",
      " 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472\n",
      " 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472\n",
      " 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472\n",
      " 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472\n",
      " 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472\n",
      " 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472\n",
      " 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472\n",
      " 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472\n",
      " 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472\n",
      " 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472\n",
      " 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472\n",
      " 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472\n",
      " 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472\n",
      " 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472\n",
      " 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472\n",
      " 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472\n",
      " 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472\n",
      " 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472\n",
      " 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472\n",
      " 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472\n",
      " 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472\n",
      " 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472\n",
      " 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472\n",
      " 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472\n",
      " 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472\n",
      " 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472\n",
      " 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472\n",
      " 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472\n",
      " 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472\n",
      " 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472\n",
      " 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472\n",
      " 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472\n",
      " 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472\n",
      " 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472\n",
      " 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472\n",
      " 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472\n",
      " 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472\n",
      " 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472\n",
      " 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472\n",
      " 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472\n",
      " 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472\n",
      " 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472\n",
      " 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472\n",
      " 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472\n",
      " 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472\n",
      " 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472\n",
      " 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472\n",
      " 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472\n",
      " 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472\n",
      " 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472\n",
      " 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472\n",
      " 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472\n",
      " 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472\n",
      " 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472\n",
      " 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472\n",
      " 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472\n",
      " 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472\n",
      " 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472\n",
      " 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472\n",
      " 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472\n",
      " 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472\n",
      " 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472\n",
      " 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472\n",
      " 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472\n",
      " 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472\n",
      " 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472\n",
      " 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472\n",
      " 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472\n",
      " 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472\n",
      " 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472\n",
      " 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472\n",
      " 0.49972472 0.49972472 0.49972472 0.49972472]\n",
      " y_pred_train again: \n",
      "  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0]\n",
      "accuracy_score_train with normalize=True:  0.5\n",
      "accuracy_score_train with normalize=False :  500\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    class 0       1.00      0.50      0.67      1000\n",
      "    class 1       0.00      0.00      0.00         0\n",
      "\n",
      "avg / total       1.00      0.50      0.67      1000\n",
      "\n",
      "confusion_matrix : \n",
      " [[500 500]\n",
      " [  0   0]]\n",
      "Accuracy :  0.5\n",
      "Sensitivity :  1.0\n",
      "Specificity :  0.0\n",
      "-------------------------------------------------------------------\n",
      "------------------ Evalute the validation set ------------------------\n",
      "-------------------------------------------------------------------\n",
      "validation loss: 0.6931473135948181\n",
      "validation accuracy: 0.5\n",
      " y_pred_validation shape (400,)\n",
      " y_pred_validation : \n",
      "  [0.49972472 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472\n",
      " 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472\n",
      " 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472\n",
      " 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472\n",
      " 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472\n",
      " 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472\n",
      " 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472\n",
      " 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472\n",
      " 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472\n",
      " 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472\n",
      " 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472\n",
      " 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472\n",
      " 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472\n",
      " 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472\n",
      " 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472\n",
      " 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472\n",
      " 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472\n",
      " 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472\n",
      " 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472\n",
      " 0.49972472 0.49972197 0.49972472 0.49972472 0.49972472 0.49972472\n",
      " 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472\n",
      " 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472\n",
      " 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472\n",
      " 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472\n",
      " 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472\n",
      " 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472\n",
      " 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472\n",
      " 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472\n",
      " 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472\n",
      " 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472\n",
      " 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472\n",
      " 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472\n",
      " 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472\n",
      " 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472\n",
      " 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472\n",
      " 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472\n",
      " 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472\n",
      " 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472\n",
      " 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472\n",
      " 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472\n",
      " 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472\n",
      " 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472\n",
      " 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472\n",
      " 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472\n",
      " 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472\n",
      " 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472\n",
      " 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472\n",
      " 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472\n",
      " 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472\n",
      " 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472\n",
      " 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472\n",
      " 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472\n",
      " 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472\n",
      " 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472\n",
      " 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472\n",
      " 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472\n",
      " 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472\n",
      " 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472\n",
      " 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472\n",
      " 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472\n",
      " 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472\n",
      " 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472\n",
      " 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472\n",
      " 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472\n",
      " 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472\n",
      " 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472 0.49972472\n",
      " 0.49972472 0.49972472 0.49972472 0.49972472]\n",
      " y_pred_validation again: \n",
      "  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "accuracy_score_validation with normalize=True:  0.5\n",
      "accuracy_score_validation with normalize=False :  200\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    class 0       1.00      0.50      0.67       400\n",
      "    class 1       0.00      0.00      0.00         0\n",
      "\n",
      "avg / total       1.00      0.50      0.67       400\n",
      "\n",
      "confusion_matrix : \n",
      " [[200 200]\n",
      " [  0   0]]\n",
      "Accuracy :  0.5\n",
      "Sensitivity :  1.0\n",
      "Specificity :  0.0\n",
      "-------------------------------------------------------------------\n",
      "------------------ Train Done -------------------------------------\n",
      "-------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fatma-sayed/anaconda3/envs/keras-test/lib/python3.6/site-packages/sklearn/metrics/classification.py:1137: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf \n",
    "import keras\n",
    "import os\n",
    "\n",
    "from subprocess import check_output\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "from PIL import Image\n",
    "#from keras.applications.vgg16 import preprocess_input\n",
    "#from keras.applications.xception import preprocess_input\n",
    "#from keras.applications.inception_v3 import preprocess_input\n",
    "#from keras.applications.vgg19 import preprocess_input                \n",
    "from keras.applications.resnet50 import preprocess_input                \n",
    "\n",
    "from keras.preprocessing import image\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "from keras.utils import np_utils\n",
    "\n",
    "#from keras.applications.vgg16 import VGG16\n",
    "\n",
    "#from keras.applications.xception import Xception\n",
    "\n",
    "#from keras.applications.inception_v3 import InceptionV3\n",
    "\n",
    "#from keras.applications.vgg19 import VGG19\n",
    "\n",
    "from keras.applications.resnet50 import ResNet50\n",
    "\n",
    "from keras.models import Model, load_model\n",
    "\n",
    "from keras.layers import Dense, Dropout, Reshape\n",
    "\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers import Activation, Flatten \n",
    "from keras import backend as K\n",
    "from keras import applications\n",
    "\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0' #use GPU with ID=0\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.7 # maximun alloc gpu50% of MEM\n",
    "#config.gpu_options.allow_growth = True #allocate dynamically\n",
    "sess = tf.Session(config = config)\n",
    "keras.backend.set_session(sess)\n",
    "\n",
    "\n",
    "# dimensions of our images.\n",
    "img_width, img_height = 224, 224\n",
    "top_model_weights_path = 'bottleneck_fc_model_ResNet50_data4.h5'  \n",
    "train_data_dir = '../data4_Train500_Test200/train'\n",
    "validation_data_dir = '../data4_Train500_Test200/validation'\n",
    "nb_train_samples = 1000\n",
    "nb_validation_samples = 400\n",
    "epochs = 50\n",
    "batch_size = 8  # batch size in flow_images_from_directory needs to correspond to the image number of the test images.\n",
    "\n",
    "\n",
    "def save_bottleneck_features():\n",
    "    datagen = ImageDataGenerator(rescale=1. / 255)\n",
    "\n",
    "    # build the ResNet50 network\n",
    "\n",
    "    model = applications.ResNet50(weights='imagenet', include_top=False)\n",
    "\n",
    "    model.summary()\n",
    "    \n",
    "    generator = datagen.flow_from_directory(\n",
    "        train_data_dir,\n",
    "        target_size=(img_width, img_height),\n",
    "        batch_size=batch_size,\n",
    "        class_mode=None,\n",
    "        shuffle=False)\n",
    "    \n",
    "    bottleneck_features_train = model.predict_generator(\n",
    "        generator, nb_train_samples // batch_size)\n",
    "    np.save(open('DR1_bottleneck_features_train_ResNet50_data4.npy', 'wb'),\n",
    "            bottleneck_features_train)\n",
    "\n",
    "    generator = datagen.flow_from_directory(\n",
    "        validation_data_dir,\n",
    "        target_size=(img_width, img_height),\n",
    "        batch_size=batch_size,\n",
    "        class_mode=None,\n",
    "        shuffle=False)\n",
    "    bottleneck_features_validation = model.predict_generator(\n",
    "        generator, nb_validation_samples // batch_size)\n",
    "    np.save(open('DR1_bottleneck_features_validation_ResNet50_data4.npy', 'wb'),\n",
    "            bottleneck_features_validation)\n",
    "\n",
    "\n",
    "def train_top_model():\n",
    "    train_data = np.load(open('DR1_bottleneck_features_train_ResNet50_data4.npy','rb'))\n",
    "    train_labels = np.array(\n",
    "        [0] * (nb_train_samples // 2) + [1] * (nb_train_samples // 2))\n",
    "\n",
    "    print(' train_labels shape',train_labels.shape)\n",
    "\n",
    "    print(' train_labels : \\n ',train_labels)\n",
    "\n",
    "    validation_data = np.load(open('DR1_bottleneck_features_validation_ResNet50_data4.npy','rb'))\n",
    "    validation_labels = np.array(\n",
    "        [0] * (nb_validation_samples // 2) + [1] * (nb_validation_samples // 2))\n",
    "\n",
    "    print(' validation_labels shape',validation_labels.shape)\n",
    "\n",
    "    print(' validation_labels : \\n ',validation_labels)\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Flatten(input_shape=train_data.shape[1:]))\n",
    "    model.add(Dense(256, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    model.compile(optimizer='rmsprop',\n",
    "                  loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    model.fit(train_data, train_labels,\n",
    "              epochs=epochs,\n",
    "              batch_size=batch_size,\n",
    "              validation_data=(validation_data, validation_labels))\n",
    "    model.save_weights(top_model_weights_path)\n",
    "    \n",
    "    print('-------------------------------------------------------------------')\n",
    "    print('------------------ Evalute the train set ------------------------')\n",
    "    print('-------------------------------------------------------------------')\n",
    "\n",
    "    train_score = model.evaluate(train_data, train_labels, verbose=0) # nb_train_samples // batch_size, workers=1)\n",
    "    print('Train loss:', train_score[0])\n",
    "    print('Train accuracy:', train_score[1])\n",
    "\n",
    "    y_pred_train = np.squeeze(model.predict(train_data))\n",
    "    print(' y_pred_train shape',y_pred_train.shape)\n",
    "    print(' y_pred_train : \\n ',y_pred_train)\n",
    "\n",
    "    threshold = 0.5\n",
    "    y_pred_train = (y_pred_train > threshold)*1\n",
    "    #y_pred_train.astype(int)   \n",
    "    print(' y_pred_train again: \\n ',y_pred_train)\n",
    "    \n",
    "    accuracy_score_train = accuracy_score(train_labels, y_pred_train, normalize=True)\n",
    "    print('accuracy_score_train with normalize=True: ', accuracy_score_train)\n",
    "\n",
    "    accuracy_score_train = accuracy_score(train_labels, y_pred_train, normalize=False)\n",
    "    print('accuracy_score_train with normalize=False : ', accuracy_score_train)\n",
    "\n",
    "    target_names = ['class 0', 'class 1']\n",
    "\n",
    "    print(classification_report(y_pred_train, train_labels, target_names=target_names))\n",
    "\n",
    "    cm1 = confusion_matrix(y_pred_train, train_labels)\n",
    "    \n",
    "    print('confusion_matrix : \\n', cm1)\n",
    "\n",
    "    total1=sum(sum(cm1))\n",
    "\n",
    "    #####from confusion matrix calculate accuracy\n",
    "    accuracy1=(cm1[0,0]+cm1[1,1])/total1\n",
    "    print ('Accuracy : ', accuracy1)\n",
    "\n",
    "    sensitivity = cm1[0,0]/(cm1[0,0]+cm1[1,0])\n",
    "    print('Sensitivity : ', sensitivity )\n",
    "\n",
    "    Specificity = cm1[1,1]/(cm1[1,1]+cm1[0,1])\n",
    "    print('Specificity : ', Specificity )\n",
    "\n",
    "    print('-------------------------------------------------------------------')\n",
    "    print('------------------ Evalute the validation set ------------------------')\n",
    "    print('-------------------------------------------------------------------')\n",
    "\n",
    "    validation_score = model.evaluate(validation_data, validation_labels, verbose=0) # nb_train_samples // batch_size, workers=1)\n",
    "    print('validation loss:', validation_score[0])\n",
    "    print('validation accuracy:', validation_score[1])\n",
    "\n",
    "    y_pred_validation = np.squeeze(model.predict(validation_data))\n",
    "    print(' y_pred_validation shape',y_pred_validation.shape)\n",
    "    print(' y_pred_validation : \\n ',y_pred_validation)\n",
    "\n",
    "    y_pred_validation = (y_pred_validation > threshold)*1\n",
    "    #y_pred_train.astype(int)   \n",
    "    print(' y_pred_validation again: \\n ',y_pred_validation)\n",
    "    \n",
    "    accuracy_score_validation = accuracy_score(validation_labels, y_pred_validation, normalize=True)\n",
    "    print('accuracy_score_validation with normalize=True: ', accuracy_score_validation)\n",
    "\n",
    "    accuracy_score_validation = accuracy_score(validation_labels, y_pred_validation, normalize=False)\n",
    "    print('accuracy_score_validation with normalize=False : ', accuracy_score_validation)\n",
    "\n",
    "    print(classification_report(y_pred_validation, validation_labels, target_names=target_names))\n",
    "\n",
    "    cm1 = confusion_matrix(y_pred_validation, validation_labels)\n",
    "    \n",
    "    print('confusion_matrix : \\n', cm1)\n",
    "\n",
    "    total1=sum(sum(cm1))\n",
    "\n",
    "    #####from confusion matrix calculate accuracy\n",
    "    accuracy1=(cm1[0,0]+cm1[1,1])/total1\n",
    "    print ('Accuracy : ', accuracy1)\n",
    "\n",
    "    sensitivity = cm1[0,0]/(cm1[0,0]+cm1[1,0])\n",
    "    print('Sensitivity : ', sensitivity )\n",
    "\n",
    "    Specificity = cm1[1,1]/(cm1[1,1]+cm1[0,1])\n",
    "    print('Specificity : ', Specificity )\n",
    "\n",
    "\n",
    "save_bottleneck_features()\n",
    "train_top_model()\n",
    "\n",
    "print('-------------------------------------------------------------------')\n",
    "print('------------------ Train Done -------------------------------------')\n",
    "print('-------------------------------------------------------------------')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### With Pre Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fatma-sayed/anaconda3/envs/keras-test/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "/home/fatma-sayed/anaconda3/envs/keras-test/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, None, None, 3 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1_pad (ZeroPadding2D)       (None, None, None, 3 0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1 (Conv2D)                  (None, None, None, 6 9472        conv1_pad[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "bn_conv1 (BatchNormalization)   (None, None, None, 6 256         conv1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, None, None, 6 0           bn_conv1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, None, None, 6 0           activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "res2a_branch2a (Conv2D)         (None, None, None, 6 4160        max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "bn2a_branch2a (BatchNormalizati (None, None, None, 6 256         res2a_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, None, None, 6 0           bn2a_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res2a_branch2b (Conv2D)         (None, None, None, 6 36928       activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "bn2a_branch2b (BatchNormalizati (None, None, None, 6 256         res2a_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, None, None, 6 0           bn2a_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res2a_branch2c (Conv2D)         (None, None, None, 2 16640       activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "res2a_branch1 (Conv2D)          (None, None, None, 2 16640       max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "bn2a_branch2c (BatchNormalizati (None, None, None, 2 1024        res2a_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn2a_branch1 (BatchNormalizatio (None, None, None, 2 1024        res2a_branch1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, None, None, 2 0           bn2a_branch2c[0][0]              \n",
      "                                                                 bn2a_branch1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, None, None, 2 0           add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "res2b_branch2a (Conv2D)         (None, None, None, 6 16448       activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "bn2b_branch2a (BatchNormalizati (None, None, None, 6 256         res2b_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, None, None, 6 0           bn2b_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res2b_branch2b (Conv2D)         (None, None, None, 6 36928       activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "bn2b_branch2b (BatchNormalizati (None, None, None, 6 256         res2b_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, None, None, 6 0           bn2b_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res2b_branch2c (Conv2D)         (None, None, None, 2 16640       activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "bn2b_branch2c (BatchNormalizati (None, None, None, 2 1024        res2b_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, None, None, 2 0           bn2b_branch2c[0][0]              \n",
      "                                                                 activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, None, None, 2 0           add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "res2c_branch2a (Conv2D)         (None, None, None, 6 16448       activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "bn2c_branch2a (BatchNormalizati (None, None, None, 6 256         res2c_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, None, None, 6 0           bn2c_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res2c_branch2b (Conv2D)         (None, None, None, 6 36928       activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "bn2c_branch2b (BatchNormalizati (None, None, None, 6 256         res2c_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, None, None, 6 0           bn2c_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res2c_branch2c (Conv2D)         (None, None, None, 2 16640       activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "bn2c_branch2c (BatchNormalizati (None, None, None, 2 1024        res2c_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, None, None, 2 0           bn2c_branch2c[0][0]              \n",
      "                                                                 activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, None, None, 2 0           add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "res3a_branch2a (Conv2D)         (None, None, None, 1 32896       activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn3a_branch2a (BatchNormalizati (None, None, None, 1 512         res3a_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, None, None, 1 0           bn3a_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3a_branch2b (Conv2D)         (None, None, None, 1 147584      activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn3a_branch2b (BatchNormalizati (None, None, None, 1 512         res3a_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, None, None, 1 0           bn3a_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3a_branch2c (Conv2D)         (None, None, None, 5 66048       activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3a_branch1 (Conv2D)          (None, None, None, 5 131584      activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn3a_branch2c (BatchNormalizati (None, None, None, 5 2048        res3a_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn3a_branch1 (BatchNormalizatio (None, None, None, 5 2048        res3a_branch1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_4 (Add)                     (None, None, None, 5 0           bn3a_branch2c[0][0]              \n",
      "                                                                 bn3a_branch1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, None, None, 5 0           add_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "res3b_branch2a (Conv2D)         (None, None, None, 1 65664       activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn3b_branch2a (BatchNormalizati (None, None, None, 1 512         res3b_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, None, None, 1 0           bn3b_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3b_branch2b (Conv2D)         (None, None, None, 1 147584      activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn3b_branch2b (BatchNormalizati (None, None, None, 1 512         res3b_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, None, None, 1 0           bn3b_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3b_branch2c (Conv2D)         (None, None, None, 5 66048       activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn3b_branch2c (BatchNormalizati (None, None, None, 5 2048        res3b_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_5 (Add)                     (None, None, None, 5 0           bn3b_branch2c[0][0]              \n",
      "                                                                 activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, None, None, 5 0           add_5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "res3c_branch2a (Conv2D)         (None, None, None, 1 65664       activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn3c_branch2a (BatchNormalizati (None, None, None, 1 512         res3c_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, None, None, 1 0           bn3c_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3c_branch2b (Conv2D)         (None, None, None, 1 147584      activation_17[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn3c_branch2b (BatchNormalizati (None, None, None, 1 512         res3c_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_18 (Activation)      (None, None, None, 1 0           bn3c_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3c_branch2c (Conv2D)         (None, None, None, 5 66048       activation_18[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn3c_branch2c (BatchNormalizati (None, None, None, 5 2048        res3c_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_6 (Add)                     (None, None, None, 5 0           bn3c_branch2c[0][0]              \n",
      "                                                                 activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_19 (Activation)      (None, None, None, 5 0           add_6[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "res3d_branch2a (Conv2D)         (None, None, None, 1 65664       activation_19[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn3d_branch2a (BatchNormalizati (None, None, None, 1 512         res3d_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_20 (Activation)      (None, None, None, 1 0           bn3d_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3d_branch2b (Conv2D)         (None, None, None, 1 147584      activation_20[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn3d_branch2b (BatchNormalizati (None, None, None, 1 512         res3d_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_21 (Activation)      (None, None, None, 1 0           bn3d_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3d_branch2c (Conv2D)         (None, None, None, 5 66048       activation_21[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn3d_branch2c (BatchNormalizati (None, None, None, 5 2048        res3d_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_7 (Add)                     (None, None, None, 5 0           bn3d_branch2c[0][0]              \n",
      "                                                                 activation_19[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_22 (Activation)      (None, None, None, 5 0           add_7[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "res4a_branch2a (Conv2D)         (None, None, None, 2 131328      activation_22[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4a_branch2a (BatchNormalizati (None, None, None, 2 1024        res4a_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_23 (Activation)      (None, None, None, 2 0           bn4a_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4a_branch2b (Conv2D)         (None, None, None, 2 590080      activation_23[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4a_branch2b (BatchNormalizati (None, None, None, 2 1024        res4a_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_24 (Activation)      (None, None, None, 2 0           bn4a_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4a_branch2c (Conv2D)         (None, None, None, 1 263168      activation_24[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4a_branch1 (Conv2D)          (None, None, None, 1 525312      activation_22[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4a_branch2c (BatchNormalizati (None, None, None, 1 4096        res4a_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn4a_branch1 (BatchNormalizatio (None, None, None, 1 4096        res4a_branch1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_8 (Add)                     (None, None, None, 1 0           bn4a_branch2c[0][0]              \n",
      "                                                                 bn4a_branch1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_25 (Activation)      (None, None, None, 1 0           add_8[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "res4b_branch2a (Conv2D)         (None, None, None, 2 262400      activation_25[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4b_branch2a (BatchNormalizati (None, None, None, 2 1024        res4b_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_26 (Activation)      (None, None, None, 2 0           bn4b_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4b_branch2b (Conv2D)         (None, None, None, 2 590080      activation_26[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4b_branch2b (BatchNormalizati (None, None, None, 2 1024        res4b_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_27 (Activation)      (None, None, None, 2 0           bn4b_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4b_branch2c (Conv2D)         (None, None, None, 1 263168      activation_27[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4b_branch2c (BatchNormalizati (None, None, None, 1 4096        res4b_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_9 (Add)                     (None, None, None, 1 0           bn4b_branch2c[0][0]              \n",
      "                                                                 activation_25[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_28 (Activation)      (None, None, None, 1 0           add_9[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "res4c_branch2a (Conv2D)         (None, None, None, 2 262400      activation_28[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4c_branch2a (BatchNormalizati (None, None, None, 2 1024        res4c_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_29 (Activation)      (None, None, None, 2 0           bn4c_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4c_branch2b (Conv2D)         (None, None, None, 2 590080      activation_29[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4c_branch2b (BatchNormalizati (None, None, None, 2 1024        res4c_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_30 (Activation)      (None, None, None, 2 0           bn4c_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4c_branch2c (Conv2D)         (None, None, None, 1 263168      activation_30[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4c_branch2c (BatchNormalizati (None, None, None, 1 4096        res4c_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_10 (Add)                    (None, None, None, 1 0           bn4c_branch2c[0][0]              \n",
      "                                                                 activation_28[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_31 (Activation)      (None, None, None, 1 0           add_10[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res4d_branch2a (Conv2D)         (None, None, None, 2 262400      activation_31[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4d_branch2a (BatchNormalizati (None, None, None, 2 1024        res4d_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_32 (Activation)      (None, None, None, 2 0           bn4d_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4d_branch2b (Conv2D)         (None, None, None, 2 590080      activation_32[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4d_branch2b (BatchNormalizati (None, None, None, 2 1024        res4d_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_33 (Activation)      (None, None, None, 2 0           bn4d_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4d_branch2c (Conv2D)         (None, None, None, 1 263168      activation_33[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4d_branch2c (BatchNormalizati (None, None, None, 1 4096        res4d_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_11 (Add)                    (None, None, None, 1 0           bn4d_branch2c[0][0]              \n",
      "                                                                 activation_31[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_34 (Activation)      (None, None, None, 1 0           add_11[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res4e_branch2a (Conv2D)         (None, None, None, 2 262400      activation_34[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4e_branch2a (BatchNormalizati (None, None, None, 2 1024        res4e_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_35 (Activation)      (None, None, None, 2 0           bn4e_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4e_branch2b (Conv2D)         (None, None, None, 2 590080      activation_35[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4e_branch2b (BatchNormalizati (None, None, None, 2 1024        res4e_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_36 (Activation)      (None, None, None, 2 0           bn4e_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4e_branch2c (Conv2D)         (None, None, None, 1 263168      activation_36[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4e_branch2c (BatchNormalizati (None, None, None, 1 4096        res4e_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_12 (Add)                    (None, None, None, 1 0           bn4e_branch2c[0][0]              \n",
      "                                                                 activation_34[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_37 (Activation)      (None, None, None, 1 0           add_12[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res4f_branch2a (Conv2D)         (None, None, None, 2 262400      activation_37[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4f_branch2a (BatchNormalizati (None, None, None, 2 1024        res4f_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_38 (Activation)      (None, None, None, 2 0           bn4f_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4f_branch2b (Conv2D)         (None, None, None, 2 590080      activation_38[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4f_branch2b (BatchNormalizati (None, None, None, 2 1024        res4f_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_39 (Activation)      (None, None, None, 2 0           bn4f_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4f_branch2c (Conv2D)         (None, None, None, 1 263168      activation_39[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4f_branch2c (BatchNormalizati (None, None, None, 1 4096        res4f_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_13 (Add)                    (None, None, None, 1 0           bn4f_branch2c[0][0]              \n",
      "                                                                 activation_37[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_40 (Activation)      (None, None, None, 1 0           add_13[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res5a_branch2a (Conv2D)         (None, None, None, 5 524800      activation_40[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn5a_branch2a (BatchNormalizati (None, None, None, 5 2048        res5a_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_41 (Activation)      (None, None, None, 5 0           bn5a_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res5a_branch2b (Conv2D)         (None, None, None, 5 2359808     activation_41[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn5a_branch2b (BatchNormalizati (None, None, None, 5 2048        res5a_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_42 (Activation)      (None, None, None, 5 0           bn5a_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res5a_branch2c (Conv2D)         (None, None, None, 2 1050624     activation_42[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res5a_branch1 (Conv2D)          (None, None, None, 2 2099200     activation_40[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn5a_branch2c (BatchNormalizati (None, None, None, 2 8192        res5a_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn5a_branch1 (BatchNormalizatio (None, None, None, 2 8192        res5a_branch1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_14 (Add)                    (None, None, None, 2 0           bn5a_branch2c[0][0]              \n",
      "                                                                 bn5a_branch1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_43 (Activation)      (None, None, None, 2 0           add_14[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res5b_branch2a (Conv2D)         (None, None, None, 5 1049088     activation_43[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn5b_branch2a (BatchNormalizati (None, None, None, 5 2048        res5b_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_44 (Activation)      (None, None, None, 5 0           bn5b_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res5b_branch2b (Conv2D)         (None, None, None, 5 2359808     activation_44[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn5b_branch2b (BatchNormalizati (None, None, None, 5 2048        res5b_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_45 (Activation)      (None, None, None, 5 0           bn5b_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res5b_branch2c (Conv2D)         (None, None, None, 2 1050624     activation_45[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn5b_branch2c (BatchNormalizati (None, None, None, 2 8192        res5b_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_15 (Add)                    (None, None, None, 2 0           bn5b_branch2c[0][0]              \n",
      "                                                                 activation_43[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_46 (Activation)      (None, None, None, 2 0           add_15[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res5c_branch2a (Conv2D)         (None, None, None, 5 1049088     activation_46[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn5c_branch2a (BatchNormalizati (None, None, None, 5 2048        res5c_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_47 (Activation)      (None, None, None, 5 0           bn5c_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res5c_branch2b (Conv2D)         (None, None, None, 5 2359808     activation_47[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn5c_branch2b (BatchNormalizati (None, None, None, 5 2048        res5c_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_48 (Activation)      (None, None, None, 5 0           bn5c_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res5c_branch2c (Conv2D)         (None, None, None, 2 1050624     activation_48[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn5c_branch2c (BatchNormalizati (None, None, None, 2 8192        res5c_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_16 (Add)                    (None, None, None, 2 0           bn5c_branch2c[0][0]              \n",
      "                                                                 activation_46[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_49 (Activation)      (None, None, None, 2 0           add_16[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "avg_pool (AveragePooling2D)     (None, None, None, 2 0           activation_49[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 23,587,712\n",
      "Trainable params: 23,534,592\n",
      "Non-trainable params: 53,120\n",
      "__________________________________________________________________________________________________\n",
      "Found 1000 images belonging to 2 classes.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400 images belonging to 2 classes.\n",
      " train_labels shape (1000,)\n",
      " train_labels : \n",
      "  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1]\n",
      " validation_labels shape (400,)\n",
      " validation_labels : \n",
      "  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "Train on 1000 samples, validate on 400 samples\n",
      "Epoch 1/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 1.7283 - acc: 0.4930 - val_loss: 0.9767 - val_acc: 0.5000\n",
      "Epoch 2/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.7884 - acc: 0.5070 - val_loss: 0.6987 - val_acc: 0.5000\n",
      "Epoch 3/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.7002 - acc: 0.4900 - val_loss: 0.6932 - val_acc: 0.5000\n",
      "Epoch 4/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.6915 - acc: 0.5240 - val_loss: 0.6953 - val_acc: 0.5000\n",
      "Epoch 5/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.6964 - acc: 0.4940 - val_loss: 0.6934 - val_acc: 0.5000\n",
      "Epoch 6/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.6940 - acc: 0.4870 - val_loss: 0.6932 - val_acc: 0.5000\n",
      "Epoch 7/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.6933 - acc: 0.4810 - val_loss: 0.6932 - val_acc: 0.5000\n",
      "Epoch 8/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.6932 - acc: 0.4880 - val_loss: 0.6932 - val_acc: 0.4975\n",
      "Epoch 9/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.6932 - acc: 0.4820 - val_loss: 0.6932 - val_acc: 0.4975\n",
      "Epoch 10/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.6932 - acc: 0.4820 - val_loss: 0.6932 - val_acc: 0.5000\n",
      "Epoch 11/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.6932 - acc: 0.4780 - val_loss: 0.6932 - val_acc: 0.5000\n",
      "Epoch 12/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.6932 - acc: 0.4860 - val_loss: 0.6932 - val_acc: 0.5000\n",
      "Epoch 13/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.6932 - acc: 0.4960 - val_loss: 0.6932 - val_acc: 0.5000\n",
      "Epoch 14/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.6933 - acc: 0.4820 - val_loss: 0.6932 - val_acc: 0.4975\n",
      "Epoch 15/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.6932 - acc: 0.4780 - val_loss: 0.6932 - val_acc: 0.4975\n",
      "Epoch 16/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.6932 - acc: 0.4960 - val_loss: 0.6932 - val_acc: 0.4975\n",
      "Epoch 17/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.6932 - acc: 0.4980 - val_loss: 0.6932 - val_acc: 0.4975\n",
      "Epoch 18/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.6932 - acc: 0.5000 - val_loss: 0.6932 - val_acc: 0.4975\n",
      "Epoch 19/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.6932 - acc: 0.5040 - val_loss: 0.6932 - val_acc: 0.5000\n",
      "Epoch 20/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.6932 - acc: 0.5000 - val_loss: 0.6932 - val_acc: 0.5000\n",
      "Epoch 21/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.6932 - acc: 0.4940 - val_loss: 0.6932 - val_acc: 0.4975\n",
      "Epoch 22/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.6932 - acc: 0.4960 - val_loss: 0.6932 - val_acc: 0.4975\n",
      "Epoch 23/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.6932 - acc: 0.4980 - val_loss: 0.6932 - val_acc: 0.4975\n",
      "Epoch 24/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.6932 - acc: 0.4840 - val_loss: 0.6932 - val_acc: 0.4975\n",
      "Epoch 25/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.6932 - acc: 0.4680 - val_loss: 0.6932 - val_acc: 0.4975\n",
      "Epoch 26/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.6932 - acc: 0.4940 - val_loss: 0.6932 - val_acc: 0.5000\n",
      "Epoch 27/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.6932 - acc: 0.4800 - val_loss: 0.6932 - val_acc: 0.5000\n",
      "Epoch 28/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.6932 - acc: 0.5000 - val_loss: 0.6932 - val_acc: 0.4975\n",
      "Epoch 29/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.6932 - acc: 0.4700 - val_loss: 0.6932 - val_acc: 0.4975\n",
      "Epoch 30/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.6932 - acc: 0.4900 - val_loss: 0.6932 - val_acc: 0.5000\n",
      "Epoch 31/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.6932 - acc: 0.5000 - val_loss: 0.6932 - val_acc: 0.4975\n",
      "Epoch 32/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.6932 - acc: 0.4960 - val_loss: 0.6932 - val_acc: 0.4975\n",
      "Epoch 33/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.6932 - acc: 0.5000 - val_loss: 0.6932 - val_acc: 0.5000\n",
      "Epoch 34/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.6932 - acc: 0.4780 - val_loss: 0.6932 - val_acc: 0.4975\n",
      "Epoch 35/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.6932 - acc: 0.4920 - val_loss: 0.6932 - val_acc: 0.4975\n",
      "Epoch 36/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.6932 - acc: 0.4980 - val_loss: 0.6932 - val_acc: 0.5000\n",
      "Epoch 37/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.6933 - acc: 0.4900 - val_loss: 0.6932 - val_acc: 0.4975\n",
      "Epoch 38/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.6932 - acc: 0.5000 - val_loss: 0.6932 - val_acc: 0.4975\n",
      "Epoch 39/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.6932 - acc: 0.4980 - val_loss: 0.6932 - val_acc: 0.4975\n",
      "Epoch 40/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.6932 - acc: 0.4880 - val_loss: 0.6932 - val_acc: 0.4975\n",
      "Epoch 41/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.6932 - acc: 0.4980 - val_loss: 0.6932 - val_acc: 0.4975\n",
      "Epoch 42/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.6932 - acc: 0.4820 - val_loss: 0.6932 - val_acc: 0.4975\n",
      "Epoch 43/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.6932 - acc: 0.4900 - val_loss: 0.6932 - val_acc: 0.4975\n",
      "Epoch 44/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.6932 - acc: 0.4940 - val_loss: 0.6932 - val_acc: 0.4975\n",
      "Epoch 45/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.6932 - acc: 0.4840 - val_loss: 0.6932 - val_acc: 0.4975\n",
      "Epoch 46/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.6933 - acc: 0.4900 - val_loss: 0.6932 - val_acc: 0.4975\n",
      "Epoch 47/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.6932 - acc: 0.4680 - val_loss: 0.6932 - val_acc: 0.4975\n",
      "Epoch 48/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.6932 - acc: 0.4780 - val_loss: 0.6932 - val_acc: 0.4975\n",
      "Epoch 49/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.6932 - acc: 0.4740 - val_loss: 0.6932 - val_acc: 0.4975\n",
      "Epoch 50/50\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.6932 - acc: 0.4920 - val_loss: 0.6932 - val_acc: 0.4975\n",
      "-------------------------------------------------------------------\n",
      "------------------ Evalute the train set ------------------------\n",
      "-------------------------------------------------------------------\n",
      "Train loss: 0.6931476593017578\n",
      "Train accuracy: 0.5\n",
      " y_pred_train shape (1000,)\n",
      " y_pred_train : \n",
      "  [0.500476 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476\n",
      " 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476\n",
      " 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476\n",
      " 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476\n",
      " 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476\n",
      " 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476\n",
      " 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476\n",
      " 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476\n",
      " 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476\n",
      " 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476\n",
      " 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476\n",
      " 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476\n",
      " 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476\n",
      " 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476\n",
      " 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476\n",
      " 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476\n",
      " 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476\n",
      " 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476\n",
      " 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476\n",
      " 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476\n",
      " 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476\n",
      " 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476\n",
      " 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476\n",
      " 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476\n",
      " 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476\n",
      " 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476\n",
      " 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476\n",
      " 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476\n",
      " 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476\n",
      " 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476\n",
      " 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476\n",
      " 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476\n",
      " 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476\n",
      " 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476\n",
      " 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476\n",
      " 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476\n",
      " 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476\n",
      " 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476\n",
      " 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476\n",
      " 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476\n",
      " 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476\n",
      " 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476\n",
      " 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476\n",
      " 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476\n",
      " 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476\n",
      " 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476\n",
      " 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476\n",
      " 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476\n",
      " 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476\n",
      " 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476\n",
      " 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476\n",
      " 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476\n",
      " 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476\n",
      " 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476\n",
      " 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476\n",
      " 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476\n",
      " 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476\n",
      " 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476\n",
      " 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476\n",
      " 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476\n",
      " 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476\n",
      " 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476\n",
      " 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476\n",
      " 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476\n",
      " 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476\n",
      " 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476\n",
      " 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476\n",
      " 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476\n",
      " 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476\n",
      " 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476\n",
      " 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476\n",
      " 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476\n",
      " 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476\n",
      " 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476\n",
      " 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476\n",
      " 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476\n",
      " 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476\n",
      " 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476\n",
      " 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476\n",
      " 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476\n",
      " 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476\n",
      " 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476\n",
      " 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476\n",
      " 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476\n",
      " 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476\n",
      " 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476\n",
      " 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476\n",
      " 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476\n",
      " 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476\n",
      " 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476\n",
      " 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476\n",
      " 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476\n",
      " 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476\n",
      " 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476\n",
      " 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476\n",
      " 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476\n",
      " 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476\n",
      " 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476\n",
      " 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476\n",
      " 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476\n",
      " 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476\n",
      " 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476\n",
      " 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476\n",
      " 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476\n",
      " 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476\n",
      " 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476\n",
      " 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476\n",
      " 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476\n",
      " 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476\n",
      " 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476\n",
      " 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476\n",
      " 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476\n",
      " 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476\n",
      " 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476\n",
      " 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476\n",
      " 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476\n",
      " 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476\n",
      " 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476\n",
      " 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476\n",
      " 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476\n",
      " 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476\n",
      " 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476\n",
      " 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476\n",
      " 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476\n",
      " 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476 0.500476]\n",
      " y_pred_train again: \n",
      "  [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1]\n",
      "accuracy_score_train with normalize=True:  0.5\n",
      "accuracy_score_train with normalize=False :  500\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    class 0       0.00      0.00      0.00         0\n",
      "    class 1       1.00      0.50      0.67      1000\n",
      "\n",
      "avg / total       1.00      0.50      0.67      1000\n",
      "\n",
      "confusion_matrix : \n",
      " [[  0   0]\n",
      " [500 500]]\n",
      "Accuracy :  0.5\n",
      "Sensitivity :  0.0\n",
      "Specificity :  1.0\n",
      "-------------------------------------------------------------------\n",
      "------------------ Evalute the validation set ------------------------\n",
      "-------------------------------------------------------------------\n",
      "validation loss: 0.693152813911438\n",
      "validation accuracy: 0.4975\n",
      " y_pred_validation shape (400,)\n",
      " y_pred_validation : \n",
      "  [0.500476  0.500476  0.500476  0.500476  0.500476  0.500476  0.500476\n",
      " 0.500476  0.500476  0.500476  0.500476  0.500476  0.500476  0.500476\n",
      " 0.500476  0.500476  0.500476  0.500476  0.500476  0.500476  0.500476\n",
      " 0.500476  0.500476  0.500476  0.500476  0.500476  0.500476  0.500476\n",
      " 0.500476  0.500476  0.500476  0.500476  0.500476  0.500476  0.500476\n",
      " 0.500476  0.500476  0.500476  0.500476  0.500476  0.500476  0.500476\n",
      " 0.500476  0.500476  0.500476  0.500476  0.500476  0.500476  0.500476\n",
      " 0.500476  0.500476  0.500476  0.500476  0.500476  0.500476  0.500476\n",
      " 0.500476  0.500476  0.500476  0.500476  0.500476  0.500476  0.500476\n",
      " 0.500476  0.500476  0.500476  0.500476  0.500476  0.500476  0.500476\n",
      " 0.500476  0.500476  0.500476  0.500476  0.500476  0.500476  0.500476\n",
      " 0.500476  0.500476  0.500476  0.500476  0.500476  0.500476  0.500476\n",
      " 0.500476  0.500476  0.500476  0.500476  0.500476  0.500476  0.500476\n",
      " 0.500476  0.500476  0.500476  0.500476  0.500476  0.500476  0.500476\n",
      " 0.500476  0.500476  0.500476  0.500476  0.500476  0.500476  0.500476\n",
      " 0.500476  0.500476  0.500476  0.500476  0.500476  0.500476  0.500476\n",
      " 0.500476  0.500476  0.500476  0.500476  0.500476  0.500476  0.500476\n",
      " 0.500476  0.500476  0.500476  0.500476  0.500476  0.500476  0.500476\n",
      " 0.500476  0.500476  0.500476  0.500476  0.500476  0.500476  0.500476\n",
      " 0.500476  0.500476  0.500476  0.500476  0.500476  0.500476  0.500476\n",
      " 0.500476  0.500476  0.500476  0.500476  0.500476  0.500476  0.500476\n",
      " 0.500476  0.500476  0.500476  0.500476  0.500476  0.500476  0.500476\n",
      " 0.500476  0.500476  0.500476  0.500476  0.500476  0.500476  0.500476\n",
      " 0.500476  0.500476  0.500476  0.500476  0.500476  0.500476  0.500476\n",
      " 0.500476  0.500476  0.500476  0.500476  0.500476  0.500476  0.500476\n",
      " 0.500476  0.500476  0.500476  0.500476  0.500476  0.500476  0.500476\n",
      " 0.500476  0.500476  0.500476  0.500476  0.500476  0.500476  0.500476\n",
      " 0.500476  0.500476  0.500476  0.500476  0.500476  0.500476  0.500476\n",
      " 0.500476  0.500476  0.500476  0.500476  0.500476  0.500476  0.500476\n",
      " 0.500476  0.500476  0.500476  0.500476  0.500476  0.500476  0.500476\n",
      " 0.500476  0.500476  0.500476  0.500476  0.500476  0.500476  0.500476\n",
      " 0.500476  0.500476  0.500476  0.500476  0.500476  0.500476  0.500476\n",
      " 0.500476  0.500476  0.500476  0.500476  0.500476  0.500476  0.500476\n",
      " 0.500476  0.500476  0.500476  0.500476  0.500476  0.500476  0.500476\n",
      " 0.500476  0.500476  0.500476  0.500476  0.500476  0.500476  0.500476\n",
      " 0.500476  0.500476  0.500476  0.500476  0.500476  0.500476  0.500476\n",
      " 0.500476  0.500476  0.500476  0.500476  0.500476  0.500476  0.500476\n",
      " 0.500476  0.500476  0.500476  0.500476  0.500476  0.500476  0.500476\n",
      " 0.500476  0.500476  0.500476  0.500476  0.500476  0.500476  0.500476\n",
      " 0.500476  0.500476  0.500476  0.500476  0.500476  0.500476  0.500476\n",
      " 0.500476  0.500476  0.500476  0.500476  0.500476  0.500476  0.500476\n",
      " 0.500476  0.500476  0.500476  0.500476  0.500476  0.500476  0.500476\n",
      " 0.500476  0.500476  0.500476  0.500476  0.500476  0.500476  0.500476\n",
      " 0.500476  0.500476  0.500476  0.500476  0.500476  0.500476  0.500476\n",
      " 0.500476  0.500476  0.500476  0.500476  0.500476  0.500476  0.500476\n",
      " 0.500476  0.500476  0.4994451 0.500476  0.500476  0.500476  0.500476\n",
      " 0.500476  0.500476  0.500476  0.500476  0.500476  0.500476  0.500476\n",
      " 0.500476  0.500476  0.500476  0.500476  0.500476  0.500476  0.500476\n",
      " 0.500476  0.500476  0.500476  0.500476  0.500476  0.500476  0.500476\n",
      " 0.500476  0.500476  0.500476  0.500476  0.500476  0.500476  0.500476\n",
      " 0.500476  0.500476  0.500476  0.500476  0.500476  0.500476  0.500476\n",
      " 0.500476  0.500476  0.500476  0.500476  0.500476  0.500476  0.500476\n",
      " 0.500476  0.500476  0.500476  0.500476  0.500476  0.500476  0.500476\n",
      " 0.500476  0.500476  0.500476  0.500476  0.500476  0.500476  0.500476\n",
      " 0.500476  0.500476  0.500476  0.500476  0.500476  0.500476  0.500476\n",
      " 0.500476  0.500476  0.500476  0.500476  0.500476  0.500476  0.500476\n",
      " 0.500476  0.500476  0.500476  0.500476  0.500476  0.500476  0.500476\n",
      " 0.500476 ]\n",
      " y_pred_validation again: \n",
      "  [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "accuracy_score_validation with normalize=True:  0.4975\n",
      "accuracy_score_validation with normalize=False :  199\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    class 0       0.00      0.00      0.00         1\n",
      "    class 1       0.99      0.50      0.66       399\n",
      "\n",
      "avg / total       0.99      0.50      0.66       400\n",
      "\n",
      "confusion_matrix : \n",
      " [[  0   1]\n",
      " [200 199]]\n",
      "Accuracy :  0.4975\n",
      "Sensitivity :  0.0\n",
      "Specificity :  0.995\n",
      "-------------------------------------------------------------------\n",
      "------------------ Train Done -------------------------------------\n",
      "-------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fatma-sayed/anaconda3/envs/keras-test/lib/python3.6/site-packages/sklearn/metrics/classification.py:1137: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf \n",
    "import keras\n",
    "import os\n",
    "\n",
    "from subprocess import check_output\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "from PIL import Image\n",
    "#from keras.applications.vgg16 import preprocess_input\n",
    "#from keras.applications.xception import preprocess_input\n",
    "#from keras.applications.inception_v3 import preprocess_input\n",
    "#from keras.applications.vgg19 import preprocess_input                \n",
    "from keras.applications.resnet50 import preprocess_input                \n",
    "\n",
    "from keras.preprocessing import image\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "from keras.utils import np_utils\n",
    "\n",
    "#from keras.applications.vgg16 import VGG16\n",
    "\n",
    "#from keras.applications.xception import Xception\n",
    "\n",
    "#from keras.applications.inception_v3 import InceptionV3\n",
    "\n",
    "#from keras.applications.vgg19 import VGG19\n",
    "\n",
    "from keras.applications.resnet50 import ResNet50\n",
    "\n",
    "from keras.models import Model, load_model\n",
    "\n",
    "from keras.layers import Dense, Dropout, Reshape\n",
    "\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers import Activation, Flatten \n",
    "from keras import backend as K\n",
    "from keras import applications\n",
    "\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0' #use GPU with ID=0\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.7 # maximun alloc gpu50% of MEM\n",
    "#config.gpu_options.allow_growth = True #allocate dynamically\n",
    "sess = tf.Session(config = config)\n",
    "keras.backend.set_session(sess)\n",
    "\n",
    "\n",
    "# dimensions of our images.\n",
    "img_width, img_height = 224, 224\n",
    "top_model_weights_path = 'bottleneck_fc_model_ResNet50_PP_data4.h5'  \n",
    "train_data_dir = '../data4_Train500_Test200/train'\n",
    "validation_data_dir = '../data4_Train500_Test200/validation'\n",
    "nb_train_samples = 1000\n",
    "nb_validation_samples = 400\n",
    "epochs = 50\n",
    "batch_size = 8  # batch size in flow_images_from_directory needs to correspond to the image number of the test images.\n",
    "\n",
    "\n",
    "def save_bottleneck_features():\n",
    "    datagen = ImageDataGenerator(rescale=1. / 255)\n",
    "\n",
    "    # build the ResNet50 network\n",
    "\n",
    "    model = applications.ResNet50(weights='imagenet', include_top=False)\n",
    "\n",
    "    model.summary()\n",
    "    \n",
    "    generator = datagen.flow_from_directory(\n",
    "        train_data_dir,\n",
    "        target_size=(img_width, img_height),\n",
    "        batch_size=batch_size,\n",
    "        class_mode=None,\n",
    "        shuffle=False)\n",
    "    \n",
    "    bottleneck_features_train = model.predict_generator(\n",
    "        generator, nb_train_samples // batch_size)\n",
    "    np.save(open('DR1_bottleneck_features_train_ResNet50_PP_data4.npy', 'wb'),\n",
    "            bottleneck_features_train)\n",
    "\n",
    "    generator = datagen.flow_from_directory(\n",
    "        validation_data_dir,\n",
    "        target_size=(img_width, img_height),\n",
    "        batch_size=batch_size,\n",
    "        class_mode=None,\n",
    "        shuffle=False)\n",
    "    bottleneck_features_validation = model.predict_generator(\n",
    "        generator, nb_validation_samples // batch_size)\n",
    "    np.save(open('DR1_bottleneck_features_validation_ResNet50_PP_data4.npy', 'wb'),\n",
    "            bottleneck_features_validation)\n",
    "\n",
    "\n",
    "def train_top_model():\n",
    "    train_data = np.load(open('DR1_bottleneck_features_train_ResNet50_PP_data4.npy','rb'))\n",
    "    train_labels = np.array(\n",
    "        [0] * (nb_train_samples // 2) + [1] * (nb_train_samples // 2))\n",
    "\n",
    "    preprocess_input(train_data, data_format = 'channels_last')   \n",
    "\n",
    "    print(' train_labels shape',train_labels.shape)\n",
    "\n",
    "    print(' train_labels : \\n ',train_labels)\n",
    "\n",
    "    validation_data = np.load(open('DR1_bottleneck_features_validation_ResNet50_data4.npy','rb'))\n",
    "    validation_labels = np.array(\n",
    "        [0] * (nb_validation_samples // 2) + [1] * (nb_validation_samples // 2))\n",
    "\n",
    "    preprocess_input(validation_data, data_format = 'channels_last')   \n",
    "\n",
    "    print(' validation_labels shape',validation_labels.shape)\n",
    "\n",
    "    print(' validation_labels : \\n ',validation_labels)\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Flatten(input_shape=train_data.shape[1:]))\n",
    "    model.add(Dense(256, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    model.compile(optimizer='rmsprop',\n",
    "                  loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    model.fit(train_data, train_labels,\n",
    "              epochs=epochs,\n",
    "              batch_size=batch_size,\n",
    "              validation_data=(validation_data, validation_labels))\n",
    "    model.save_weights(top_model_weights_path)\n",
    "    \n",
    "    print('-------------------------------------------------------------------')\n",
    "    print('------------------ Evalute the train set ------------------------')\n",
    "    print('-------------------------------------------------------------------')\n",
    "\n",
    "    train_score = model.evaluate(train_data, train_labels, verbose=0) # nb_train_samples // batch_size, workers=1)\n",
    "    print('Train loss:', train_score[0])\n",
    "    print('Train accuracy:', train_score[1])\n",
    "\n",
    "    y_pred_train = np.squeeze(model.predict(train_data))\n",
    "    print(' y_pred_train shape',y_pred_train.shape)\n",
    "    print(' y_pred_train : \\n ',y_pred_train)\n",
    "\n",
    "    threshold = 0.5\n",
    "    y_pred_train = (y_pred_train > threshold)*1\n",
    "    #y_pred_train.astype(int)   \n",
    "    print(' y_pred_train again: \\n ',y_pred_train)\n",
    "    \n",
    "    accuracy_score_train = accuracy_score(train_labels, y_pred_train, normalize=True)\n",
    "    print('accuracy_score_train with normalize=True: ', accuracy_score_train)\n",
    "\n",
    "    accuracy_score_train = accuracy_score(train_labels, y_pred_train, normalize=False)\n",
    "    print('accuracy_score_train with normalize=False : ', accuracy_score_train)\n",
    "\n",
    "    target_names = ['class 0', 'class 1']\n",
    "\n",
    "    print(classification_report(y_pred_train, train_labels, target_names=target_names))\n",
    "\n",
    "    cm1 = confusion_matrix(y_pred_train, train_labels)\n",
    "    \n",
    "    print('confusion_matrix : \\n', cm1)\n",
    "\n",
    "    total1=sum(sum(cm1))\n",
    "\n",
    "    #####from confusion matrix calculate accuracy\n",
    "    accuracy1=(cm1[0,0]+cm1[1,1])/total1\n",
    "    print ('Accuracy : ', accuracy1)\n",
    "\n",
    "    sensitivity = cm1[0,0]/(cm1[0,0]+cm1[1,0])\n",
    "    print('Sensitivity : ', sensitivity )\n",
    "\n",
    "    Specificity = cm1[1,1]/(cm1[1,1]+cm1[0,1])\n",
    "    print('Specificity : ', Specificity )\n",
    "\n",
    "    print('-------------------------------------------------------------------')\n",
    "    print('------------------ Evalute the validation set ------------------------')\n",
    "    print('-------------------------------------------------------------------')\n",
    "\n",
    "    validation_score = model.evaluate(validation_data, validation_labels, verbose=0) # nb_train_samples // batch_size, workers=1)\n",
    "    print('validation loss:', validation_score[0])\n",
    "    print('validation accuracy:', validation_score[1])\n",
    "\n",
    "    y_pred_validation = np.squeeze(model.predict(validation_data))\n",
    "    print(' y_pred_validation shape',y_pred_validation.shape)\n",
    "    print(' y_pred_validation : \\n ',y_pred_validation)\n",
    "\n",
    "    y_pred_validation = (y_pred_validation > threshold)*1\n",
    "    #y_pred_train.astype(int)   \n",
    "    print(' y_pred_validation again: \\n ',y_pred_validation)\n",
    "    \n",
    "    accuracy_score_validation = accuracy_score(validation_labels, y_pred_validation, normalize=True)\n",
    "    print('accuracy_score_validation with normalize=True: ', accuracy_score_validation)\n",
    "\n",
    "    accuracy_score_validation = accuracy_score(validation_labels, y_pred_validation, normalize=False)\n",
    "    print('accuracy_score_validation with normalize=False : ', accuracy_score_validation)\n",
    "\n",
    "    print(classification_report(y_pred_validation, validation_labels, target_names=target_names))\n",
    "\n",
    "    cm1 = confusion_matrix(y_pred_validation, validation_labels)\n",
    "    \n",
    "    print('confusion_matrix : \\n', cm1)\n",
    "\n",
    "    total1=sum(sum(cm1))\n",
    "\n",
    "    #####from confusion matrix calculate accuracy\n",
    "    accuracy1=(cm1[0,0]+cm1[1,1])/total1\n",
    "    print ('Accuracy : ', accuracy1)\n",
    "\n",
    "    sensitivity = cm1[0,0]/(cm1[0,0]+cm1[1,0])\n",
    "    print('Sensitivity : ', sensitivity )\n",
    "\n",
    "    Specificity = cm1[1,1]/(cm1[1,1]+cm1[0,1])\n",
    "    print('Specificity : ', Specificity )\n",
    "\n",
    "\n",
    "save_bottleneck_features()\n",
    "train_top_model()\n",
    "\n",
    "print('-------------------------------------------------------------------')\n",
    "print('------------------ Train Done -------------------------------------')\n",
    "print('-------------------------------------------------------------------')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
