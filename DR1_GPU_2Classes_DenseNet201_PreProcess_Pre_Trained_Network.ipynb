{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DR1-GPU-2Classes-DenseNet201-PreProcess_Pre_Trained_Network.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/FatmaSayedAhmed/PhDResearch/blob/master/DR1_GPU_2Classes_DenseNet201_PreProcess_Pre_Trained_Network.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "rJi1-YSdxY4J",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "C3TyLqFPxibk",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###Class 0 Vs Class 1"
      ]
    },
    {
      "metadata": {
        "id": "VdDQ1NnGxldu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf \n",
        "import keras\n",
        "import os\n",
        "\n",
        "from subprocess import check_output\n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "from PIL import Image\n",
        "            \n",
        "from keras.applications.densenet import preprocess_input     \n",
        "\n",
        "\n",
        "from keras.preprocessing import image\n",
        "\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "from sklearn.cross_validation import train_test_split\n",
        "\n",
        "from keras.utils import np_utils\n",
        "\n",
        "from keras.applications.densenet import DenseNet201    \n",
        "\n",
        "from keras.models import Model, load_model\n",
        "\n",
        "from keras.layers import Dense, Dropout, Reshape\n",
        "\n",
        "from keras.optimizers import SGD\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.metrics import classification_report,confusion_matrix\n",
        "\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "from keras.layers import Activation, Flatten \n",
        "from keras import backend as K\n",
        "from keras import applications\n",
        "\n",
        "\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0' #use GPU with ID=0\n",
        "config = tf.ConfigProto()\n",
        "config.gpu_options.per_process_gpu_memory_fraction = 0.7 # maximun alloc gpu50% of MEM\n",
        "sess = tf.Session(config = config)\n",
        "keras.backend.set_session(sess)\n",
        "\n",
        "\n",
        "# dimensions of our images.\n",
        "img_width, img_height = 224, 224\n",
        " \n",
        "top_model_weights_path = '/content/gdrive/My Drive/Colab Notebooks/bottleneck_fc_model_DenseNet201_PP_PTN_data1.h5'\n",
        "\n",
        "train_data_dir = '/content/gdrive/My Drive/Colab Notebooks/data1_Train1708_Test732/train'\n",
        "validation_data_dir = '/content/gdrive/My Drive/Colab Notebooks/data1_Train1708_Test732/validation'\n",
        "nb_train_samples = 3416\n",
        "nb_validation_samples = 1464\n",
        "\n",
        "epochs = 50\n",
        "batch_size = 8  # batch size in flow_images_from_directory needs to correspond to the image number of the test images.\n",
        "\n",
        "\n",
        "def save_bottleneck_features():\n",
        "    datagen = ImageDataGenerator(rescale=1. / 255)\n",
        "\n",
        "    model = applications.DenseNet201(weights='imagenet', include_top=False)\n",
        "\n",
        "    #model.summary()\n",
        "    \n",
        "    generator = datagen.flow_from_directory(\n",
        "        train_data_dir,\n",
        "        target_size=(img_width, img_height),\n",
        "        batch_size=batch_size,\n",
        "        class_mode=None,\n",
        "        shuffle=False)\n",
        "    \n",
        "    bottleneck_features_train = model.predict_generator(\n",
        "        generator, nb_train_samples // batch_size)\n",
        "    np.save(open('DR1_bottleneck_features_train_DenseNet201_PP_PTN_data1.npy', 'wb'),\n",
        "            bottleneck_features_train)\n",
        "\n",
        "    generator = datagen.flow_from_directory(\n",
        "        validation_data_dir,\n",
        "        target_size=(img_width, img_height),\n",
        "        batch_size=batch_size,\n",
        "        class_mode=None,\n",
        "        shuffle=False)\n",
        "    bottleneck_features_validation = model.predict_generator(\n",
        "        generator, nb_validation_samples // batch_size)\n",
        "    np.save(open('DR1_bottleneck_features_validation_DenseNet201_PP_PTN_data1.npy', 'wb'),\n",
        "            bottleneck_features_validation)\n",
        "\n",
        "\n",
        "def train_top_model():\n",
        "    train_data = np.load(open('DR1_bottleneck_features_train_DenseNet201_PP_PTN_data1.npy','rb'))\n",
        "    train_labels = np.array(\n",
        "        [0] * (nb_train_samples // 2) + [1] * (nb_train_samples // 2))\n",
        "    \n",
        "    print(' train_labels shape',train_labels.shape)\n",
        "\n",
        "    preprocess_input(train_data, data_format = 'channels_last')   \n",
        "\n",
        "    #print(' train_labels : \\n ',train_labels)\n",
        "\n",
        "    validation_data = np.load(open('DR1_bottleneck_features_validation_DenseNet201_PP_PTN_data1.npy','rb'))\n",
        "    validation_labels = np.array(\n",
        "        [0] * (nb_validation_samples // 2) + [1] * (nb_validation_samples // 2))\n",
        "\n",
        "    preprocess_input(validation_data, data_format = 'channels_last')   \n",
        "\n",
        "    print(' validation_labels shape',validation_labels.shape)\n",
        "\n",
        "    #print(' validation_labels : \\n ',validation_labels)\n",
        "\n",
        "    model = Sequential()\n",
        "    model.add(Flatten(input_shape=train_data.shape[1:]))\n",
        "    model.add(Dense(256, activation='relu'))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "    model.compile(optimizer='rmsprop',\n",
        "                  loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    model.fit(train_data, train_labels,\n",
        "              epochs=epochs,\n",
        "              batch_size=batch_size,\n",
        "              validation_data=(validation_data, validation_labels))\n",
        "    model.save_weights(top_model_weights_path)\n",
        "    \n",
        "    print('-------------------------------------------------------------------')\n",
        "    print('------------------ Evalute the train set ------------------------')\n",
        "    print('-------------------------------------------------------------------')\n",
        "\n",
        "    train_score = model.evaluate(train_data, train_labels, verbose=0) # nb_train_samples // batch_size, workers=1)\n",
        "    print('Train loss:', train_score[0])\n",
        "    print('Train accuracy:', train_score[1])\n",
        "\n",
        "    y_pred_train = np.squeeze(model.predict(train_data))\n",
        "    print(' y_pred_train shape',y_pred_train.shape)\n",
        "    #print(' y_pred_train : \\n ',y_pred_train)\n",
        "\n",
        "    threshold = 0.5\n",
        "    y_pred_train = (y_pred_train > threshold)*1\n",
        "    #y_pred_train.astype(int)   \n",
        "    #print(' y_pred_train again: \\n ',y_pred_train)\n",
        "    \n",
        "    accuracy_score_train = accuracy_score(train_labels, y_pred_train, normalize=True)\n",
        "    print('accuracy_score_train with normalize=True: ', accuracy_score_train)\n",
        "\n",
        "    accuracy_score_train = accuracy_score(train_labels, y_pred_train, normalize=False)\n",
        "    print('accuracy_score_train with normalize=False : ', accuracy_score_train)\n",
        "\n",
        "    target_names = ['class 0', 'class 1']\n",
        "\n",
        "    print(classification_report(y_pred_train, train_labels, target_names=target_names))\n",
        "\n",
        "    cm1 = confusion_matrix(y_pred_train, train_labels)\n",
        "    \n",
        "    print('confusion_matrix : \\n', cm1)\n",
        "\n",
        "    total1=sum(sum(cm1))\n",
        "\n",
        "    #####from confusion matrix calculate accuracy\n",
        "    accuracy1=(cm1[0,0]+cm1[1,1])/total1\n",
        "    print ('Accuracy : ', accuracy1)\n",
        "\n",
        "    sensitivity = cm1[0,0]/(cm1[0,0]+cm1[1,0])\n",
        "    print('Sensitivity : ', sensitivity )\n",
        "\n",
        "    Specificity = cm1[1,1]/(cm1[1,1]+cm1[0,1])\n",
        "    print('Specificity : ', Specificity )\n",
        "\n",
        "    print('-------------------------------------------------------------------')\n",
        "    print('------------------ Evalute the validation set ------------------------')\n",
        "    print('-------------------------------------------------------------------')\n",
        "\n",
        "    validation_score = model.evaluate(validation_data, validation_labels, verbose=0) # nb_train_samples // batch_size, workers=1)\n",
        "    print('validation loss:', validation_score[0])\n",
        "    print('validation accuracy:', validation_score[1])\n",
        "\n",
        "    y_pred_validation = np.squeeze(model.predict(validation_data))\n",
        "    print(' y_pred_validation shape',y_pred_validation.shape)\n",
        "    #print(' y_pred_validation : \\n ',y_pred_validation)\n",
        "\n",
        "    y_pred_validation = (y_pred_validation > threshold)*1\n",
        "    #y_pred_train.astype(int)   \n",
        "    #print(' y_pred_validation again: \\n ',y_pred_validation)\n",
        "    \n",
        "    accuracy_score_validation = accuracy_score(validation_labels, y_pred_validation, normalize=True)\n",
        "    print('accuracy_score_validation with normalize=True: ', accuracy_score_validation)\n",
        "\n",
        "    accuracy_score_validation = accuracy_score(validation_labels, y_pred_validation, normalize=False)\n",
        "    print('accuracy_score_validation with normalize=False : ', accuracy_score_validation)\n",
        "\n",
        "    print(classification_report(y_pred_validation, validation_labels, target_names=target_names))\n",
        "\n",
        "    cm1 = confusion_matrix(y_pred_validation, validation_labels)\n",
        "    \n",
        "    print('confusion_matrix : \\n', cm1)\n",
        "\n",
        "    total1=sum(sum(cm1))\n",
        "\n",
        "    #####from confusion matrix calculate accuracy\n",
        "    accuracy1=(cm1[0,0]+cm1[1,1])/total1\n",
        "    print ('Accuracy : ', accuracy1)\n",
        "\n",
        "    sensitivity = cm1[0,0]/(cm1[0,0]+cm1[1,0])\n",
        "    print('Sensitivity : ', sensitivity )\n",
        "\n",
        "    Specificity = cm1[1,1]/(cm1[1,1]+cm1[0,1])\n",
        "    print('Specificity : ', Specificity )\n",
        "\n",
        "\n",
        "save_bottleneck_features()\n",
        "train_top_model()\n",
        "\n",
        "print('-------------------------------------------------------------------')\n",
        "print('------------------ Train Done -------------------------------------')\n",
        "print('-------------------------------------------------------------------')\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "leJMbAjoxmMO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Class 0 Vs Class 4"
      ]
    },
    {
      "metadata": {
        "id": "oIEyyZwSukFD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf \n",
        "import keras\n",
        "import os\n",
        "\n",
        "from subprocess import check_output\n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "from PIL import Image\n",
        "            \n",
        "from keras.applications.densenet import preprocess_input     \n",
        "\n",
        "\n",
        "from keras.preprocessing import image\n",
        "\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "from sklearn.cross_validation import train_test_split\n",
        "\n",
        "from keras.utils import np_utils\n",
        "\n",
        "from keras.applications.densenet import DenseNet201    \n",
        "\n",
        "from keras.models import Model, load_model\n",
        "\n",
        "from keras.layers import Dense, Dropout, Reshape\n",
        "\n",
        "from keras.optimizers import SGD\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.metrics import classification_report,confusion_matrix\n",
        "\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "from keras.layers import Activation, Flatten \n",
        "from keras import backend as K\n",
        "from keras import applications\n",
        "\n",
        "\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0' #use GPU with ID=0\n",
        "config = tf.ConfigProto()\n",
        "config.gpu_options.per_process_gpu_memory_fraction = 0.7 # maximun alloc gpu50% of MEM\n",
        "sess = tf.Session(config = config)\n",
        "keras.backend.set_session(sess)\n",
        "\n",
        "\n",
        "# dimensions of our images.\n",
        "img_width, img_height = 224, 224\n",
        " \n",
        "top_model_weights_path = '/content/gdrive/My Drive/Colab Notebooks/bottleneck_fc_model_DenseNet201_PP_PTN_data4.h5'\n",
        "\n",
        "train_data_dir = '/content/gdrive/My Drive/Colab Notebooks/data4_Train496_Test212/train'\n",
        "validation_data_dir = '/content/gdrive/My Drive/Colab Notebooks/data4_Train496_Test212/validation'\n",
        "nb_train_samples = 992\n",
        "nb_validation_samples = 424\n",
        "\n",
        "epochs = 50\n",
        "batch_size = 8  # batch size in flow_images_from_directory needs to correspond to the image number of the test images.\n",
        "\n",
        "\n",
        "def save_bottleneck_features():\n",
        "    datagen = ImageDataGenerator(rescale=1. / 255)\n",
        "\n",
        "    model = applications.DenseNet201(weights='imagenet', include_top=False)\n",
        "\n",
        "    #model.summary()\n",
        "    \n",
        "    generator = datagen.flow_from_directory(\n",
        "        train_data_dir,\n",
        "        target_size=(img_width, img_height),\n",
        "        batch_size=batch_size,\n",
        "        class_mode=None,\n",
        "        shuffle=False)\n",
        "    \n",
        "    bottleneck_features_train = model.predict_generator(\n",
        "        generator, nb_train_samples // batch_size)\n",
        "    np.save(open('DR1_bottleneck_features_train_DenseNet201_PP_PTN_data4.npy', 'wb'),\n",
        "            bottleneck_features_train)\n",
        "\n",
        "    generator = datagen.flow_from_directory(\n",
        "        validation_data_dir,\n",
        "        target_size=(img_width, img_height),\n",
        "        batch_size=batch_size,\n",
        "        class_mode=None,\n",
        "        shuffle=False)\n",
        "    bottleneck_features_validation = model.predict_generator(\n",
        "        generator, nb_validation_samples // batch_size)\n",
        "    np.save(open('DR1_bottleneck_features_validation_DenseNet201_PP_PTN_data4.npy', 'wb'),\n",
        "            bottleneck_features_validation)\n",
        "\n",
        "\n",
        "def train_top_model():\n",
        "    train_data = np.load(open('DR1_bottleneck_features_train_DenseNet201_PP_PTN_data4.npy','rb'))\n",
        "    train_labels = np.array(\n",
        "        [0] * (nb_train_samples // 2) + [1] * (nb_train_samples // 2))\n",
        "    \n",
        "    print(' train_labels shape',train_labels.shape)\n",
        "\n",
        "    preprocess_input(train_data, data_format = 'channels_last')   \n",
        "\n",
        "    #print(' train_labels : \\n ',train_labels)\n",
        "\n",
        "    validation_data = np.load(open('DR1_bottleneck_features_validation_DenseNet201_PP_PTN_data4.npy','rb'))\n",
        "    validation_labels = np.array(\n",
        "        [0] * (nb_validation_samples // 2) + [1] * (nb_validation_samples // 2))\n",
        "\n",
        "    preprocess_input(validation_data, data_format = 'channels_last')   \n",
        "\n",
        "    print(' validation_labels shape',validation_labels.shape)\n",
        "\n",
        "    #print(' validation_labels : \\n ',validation_labels)\n",
        "\n",
        "    model = Sequential()\n",
        "    model.add(Flatten(input_shape=train_data.shape[1:]))\n",
        "    model.add(Dense(256, activation='relu'))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "    model.compile(optimizer='rmsprop',\n",
        "                  loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    model.fit(train_data, train_labels,\n",
        "              epochs=epochs,\n",
        "              batch_size=batch_size,\n",
        "              validation_data=(validation_data, validation_labels))\n",
        "    model.save_weights(top_model_weights_path)\n",
        "    \n",
        "    print('-------------------------------------------------------------------')\n",
        "    print('------------------ Evalute the train set ------------------------')\n",
        "    print('-------------------------------------------------------------------')\n",
        "\n",
        "    train_score = model.evaluate(train_data, train_labels, verbose=0) # nb_train_samples // batch_size, workers=1)\n",
        "    print('Train loss:', train_score[0])\n",
        "    print('Train accuracy:', train_score[1])\n",
        "\n",
        "    y_pred_train = np.squeeze(model.predict(train_data))\n",
        "    print(' y_pred_train shape',y_pred_train.shape)\n",
        "    #print(' y_pred_train : \\n ',y_pred_train)\n",
        "\n",
        "    threshold = 0.5\n",
        "    y_pred_train = (y_pred_train > threshold)*1\n",
        "    #y_pred_train.astype(int)   \n",
        "    #print(' y_pred_train again: \\n ',y_pred_train)\n",
        "    \n",
        "    accuracy_score_train = accuracy_score(train_labels, y_pred_train, normalize=True)\n",
        "    print('accuracy_score_train with normalize=True: ', accuracy_score_train)\n",
        "\n",
        "    accuracy_score_train = accuracy_score(train_labels, y_pred_train, normalize=False)\n",
        "    print('accuracy_score_train with normalize=False : ', accuracy_score_train)\n",
        "\n",
        "    target_names = ['class 0', 'class 1']\n",
        "\n",
        "    print(classification_report(y_pred_train, train_labels, target_names=target_names))\n",
        "\n",
        "    cm1 = confusion_matrix(y_pred_train, train_labels)\n",
        "    \n",
        "    print('confusion_matrix : \\n', cm1)\n",
        "\n",
        "    total1=sum(sum(cm1))\n",
        "\n",
        "    #####from confusion matrix calculate accuracy\n",
        "    accuracy1=(cm1[0,0]+cm1[1,1])/total1\n",
        "    print ('Accuracy : ', accuracy1)\n",
        "\n",
        "    sensitivity = cm1[0,0]/(cm1[0,0]+cm1[1,0])\n",
        "    print('Sensitivity : ', sensitivity )\n",
        "\n",
        "    Specificity = cm1[1,1]/(cm1[1,1]+cm1[0,1])\n",
        "    print('Specificity : ', Specificity )\n",
        "\n",
        "    print('-------------------------------------------------------------------')\n",
        "    print('------------------ Evalute the validation set ------------------------')\n",
        "    print('-------------------------------------------------------------------')\n",
        "\n",
        "    validation_score = model.evaluate(validation_data, validation_labels, verbose=0) # nb_train_samples // batch_size, workers=1)\n",
        "    print('validation loss:', validation_score[0])\n",
        "    print('validation accuracy:', validation_score[1])\n",
        "\n",
        "    y_pred_validation = np.squeeze(model.predict(validation_data))\n",
        "    print(' y_pred_validation shape',y_pred_validation.shape)\n",
        "    #print(' y_pred_validation : \\n ',y_pred_validation)\n",
        "\n",
        "    y_pred_validation = (y_pred_validation > threshold)*1\n",
        "    #y_pred_train.astype(int)   \n",
        "    #print(' y_pred_validation again: \\n ',y_pred_validation)\n",
        "    \n",
        "    accuracy_score_validation = accuracy_score(validation_labels, y_pred_validation, normalize=True)\n",
        "    print('accuracy_score_validation with normalize=True: ', accuracy_score_validation)\n",
        "\n",
        "    accuracy_score_validation = accuracy_score(validation_labels, y_pred_validation, normalize=False)\n",
        "    print('accuracy_score_validation with normalize=False : ', accuracy_score_validation)\n",
        "\n",
        "    print(classification_report(y_pred_validation, validation_labels, target_names=target_names))\n",
        "\n",
        "    cm1 = confusion_matrix(y_pred_validation, validation_labels)\n",
        "    \n",
        "    print('confusion_matrix : \\n', cm1)\n",
        "\n",
        "    total1=sum(sum(cm1))\n",
        "\n",
        "    #####from confusion matrix calculate accuracy\n",
        "    accuracy1=(cm1[0,0]+cm1[1,1])/total1\n",
        "    print ('Accuracy : ', accuracy1)\n",
        "\n",
        "    sensitivity = cm1[0,0]/(cm1[0,0]+cm1[1,0])\n",
        "    print('Sensitivity : ', sensitivity )\n",
        "\n",
        "    Specificity = cm1[1,1]/(cm1[1,1]+cm1[0,1])\n",
        "    print('Specificity : ', Specificity )\n",
        "\n",
        "\n",
        "save_bottleneck_features()\n",
        "train_top_model()\n",
        "\n",
        "print('-------------------------------------------------------------------')\n",
        "print('------------------ Train Done -------------------------------------')\n",
        "print('-------------------------------------------------------------------')\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}