{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DR1-GPU-2Classes-MobileNet_Pre-trained Network.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "6cibKSZkkjyl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "c41047e7-3a26-4054-e76c-16d5489d3a07"
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "-U5vHQa9jUCj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "### Class 0 Vs Class 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NHYpl52ijUC3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2957
        },
        "outputId": "9b535dc0-c7d9-486a-82b6-23059cf906bd"
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf \n",
        "import keras\n",
        "import os\n",
        "\n",
        "from subprocess import check_output\n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "from PIL import Image\n",
        "            \n",
        "from keras.applications.mobilenet import preprocess_input               \n",
        "\n",
        "from keras.preprocessing import image\n",
        "\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "from sklearn.cross_validation import train_test_split\n",
        "\n",
        "from keras.utils import np_utils\n",
        "\n",
        "\n",
        "from keras.applications.mobilenet import MobileNet    \n",
        "\n",
        "from keras.models import Model, load_model\n",
        "\n",
        "from keras.layers import Dense, Dropout, Reshape\n",
        "\n",
        "from keras.optimizers import SGD\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.metrics import classification_report,confusion_matrix\n",
        "\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "from keras.layers import Activation, Flatten \n",
        "from keras import backend as K\n",
        "from keras import applications\n",
        "\n",
        "\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0' #use GPU with ID=0\n",
        "config = tf.ConfigProto()\n",
        "config.gpu_options.per_process_gpu_memory_fraction = 0.7 # maximun alloc gpu50% of MEM\n",
        "#config.gpu_options.allow_growth = True #allocate dynamically\n",
        "sess = tf.Session(config = config)\n",
        "keras.backend.set_session(sess)\n",
        "\n",
        "\n",
        "# dimensions of our images.\n",
        "img_width, img_height = 224, 224\n",
        "\n",
        "top_model_weights_path = '/content/gdrive/My Drive/Colab Notebooks/bottleneck_fc_model_MobileNet_PTN_data1.h5'\n",
        "\n",
        "train_data_dir = '/content/gdrive/My Drive/Colab Notebooks/data1_Train1708_Test732/train'\n",
        "validation_data_dir = '/content/gdrive/My Drive/Colab Notebooks/data1_Train1708_Test732/validation'\n",
        "nb_train_samples = 3416\n",
        "nb_validation_samples = 1464\n",
        "epochs = 50\n",
        "batch_size = 8  # batch size in flow_images_from_directory needs to correspond to the image number of the test images.\n",
        "\n",
        "\n",
        "def save_bottleneck_features():\n",
        "    datagen = ImageDataGenerator(rescale=1. / 255)\n",
        "\n",
        "    # build the Inception_ResNet_V2 network\n",
        "\n",
        "    model = applications.MobileNet(weights='imagenet', include_top=False)\n",
        "\n",
        "    #model.summary()\n",
        "    \n",
        "    generator = datagen.flow_from_directory(\n",
        "        train_data_dir,\n",
        "        target_size=(img_width, img_height),\n",
        "        batch_size=batch_size,\n",
        "        class_mode=None,\n",
        "        shuffle=False)\n",
        "    \n",
        "    bottleneck_features_train = model.predict_generator(\n",
        "        generator, nb_train_samples // batch_size)\n",
        "    np.save(open('DR1_bottleneck_features_train_MobileNet_PTN_data1.npy', 'wb'),\n",
        "            bottleneck_features_train)\n",
        "\n",
        "    generator = datagen.flow_from_directory(\n",
        "        validation_data_dir,\n",
        "        target_size=(img_width, img_height),\n",
        "        batch_size=batch_size,\n",
        "        class_mode=None,\n",
        "        shuffle=False)\n",
        "    bottleneck_features_validation = model.predict_generator(\n",
        "        generator, nb_validation_samples // batch_size)\n",
        "    np.save(open('DR1_bottleneck_features_validation_MobileNet_PTN_data1.npy', 'wb'),\n",
        "            bottleneck_features_validation)\n",
        "\n",
        "\n",
        "def train_top_model():\n",
        "    train_data = np.load(open('DR1_bottleneck_features_train_MobileNet_PTN_data1.npy','rb'))\n",
        "    train_labels = np.array(\n",
        "        [0] * (nb_train_samples // 2) + [1] * (nb_train_samples // 2))\n",
        "\n",
        "    print(' train_labels shape',train_labels.shape)\n",
        "\n",
        "    #print(' train_labels : \\n ',train_labels)\n",
        "\n",
        "    validation_data = np.load(open('DR1_bottleneck_features_validation_MobileNet_PTN_data1.npy','rb'))\n",
        "    validation_labels = np.array(\n",
        "        [0] * (nb_validation_samples // 2) + [1] * (nb_validation_samples // 2))\n",
        "\n",
        "    print(' validation_labels shape',validation_labels.shape)\n",
        "\n",
        "    #print(' validation_labels : \\n ',validation_labels)\n",
        "\n",
        "    model = Sequential()\n",
        "    model.add(Flatten(input_shape=train_data.shape[1:]))\n",
        "    model.add(Dense(256, activation='relu'))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "    model.compile(optimizer='rmsprop',\n",
        "                  loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    model.fit(train_data, train_labels,\n",
        "              epochs=epochs,\n",
        "              batch_size=batch_size,\n",
        "              validation_data=(validation_data, validation_labels))\n",
        "    model.save_weights(top_model_weights_path)\n",
        "    \n",
        "    print('-------------------------------------------------------------------')\n",
        "    print('------------------ Evalute the train set ------------------------')\n",
        "    print('-------------------------------------------------------------------')\n",
        "\n",
        "    train_score = model.evaluate(train_data, train_labels, verbose=0) # nb_train_samples // batch_size, workers=1)\n",
        "    print('Train loss:', train_score[0])\n",
        "    print('Train accuracy:', train_score[1])\n",
        "\n",
        "    y_pred_train = np.squeeze(model.predict(train_data))\n",
        "    print(' y_pred_train shape',y_pred_train.shape)\n",
        "    #print(' y_pred_train : \\n ',y_pred_train)\n",
        "\n",
        "    threshold = 0.5\n",
        "    y_pred_train = (y_pred_train > threshold)*1\n",
        "    #y_pred_train.astype(int)   \n",
        "    #print(' y_pred_train again: \\n ',y_pred_train)\n",
        "    \n",
        "    accuracy_score_train = accuracy_score(train_labels, y_pred_train, normalize=True)\n",
        "    print('accuracy_score_train with normalize=True: ', accuracy_score_train)\n",
        "\n",
        "    accuracy_score_train = accuracy_score(train_labels, y_pred_train, normalize=False)\n",
        "    print('accuracy_score_train with normalize=False : ', accuracy_score_train)\n",
        "\n",
        "    target_names = ['class 0', 'class 1']\n",
        "\n",
        "    print(classification_report(y_pred_train, train_labels, target_names=target_names))\n",
        "\n",
        "    cm1 = confusion_matrix(y_pred_train, train_labels)\n",
        "    \n",
        "    print('confusion_matrix : \\n', cm1)\n",
        "\n",
        "    total1=sum(sum(cm1))\n",
        "\n",
        "    #####from confusion matrix calculate accuracy\n",
        "    accuracy1=(cm1[0,0]+cm1[1,1])/total1\n",
        "    print ('Accuracy : ', accuracy1)\n",
        "\n",
        "    sensitivity = cm1[0,0]/(cm1[0,0]+cm1[1,0])\n",
        "    print('Sensitivity : ', sensitivity )\n",
        "\n",
        "    Specificity = cm1[1,1]/(cm1[1,1]+cm1[0,1])\n",
        "    print('Specificity : ', Specificity )\n",
        "\n",
        "    print('-------------------------------------------------------------------')\n",
        "    print('------------------ Evalute the validation set ------------------------')\n",
        "    print('-------------------------------------------------------------------')\n",
        "\n",
        "    validation_score = model.evaluate(validation_data, validation_labels, verbose=0) # nb_train_samples // batch_size, workers=1)\n",
        "    print('validation loss:', validation_score[0])\n",
        "    print('validation accuracy:', validation_score[1])\n",
        "\n",
        "    y_pred_validation = np.squeeze(model.predict(validation_data))\n",
        "    print(' y_pred_validation shape',y_pred_validation.shape)\n",
        "    #print(' y_pred_validation : \\n ',y_pred_validation)\n",
        "\n",
        "    y_pred_validation = (y_pred_validation > threshold)*1\n",
        "    #y_pred_train.astype(int)   \n",
        "    #print(' y_pred_validation again: \\n ',y_pred_validation)\n",
        "    \n",
        "    accuracy_score_validation = accuracy_score(validation_labels, y_pred_validation, normalize=True)\n",
        "    print('accuracy_score_validation with normalize=True: ', accuracy_score_validation)\n",
        "\n",
        "    accuracy_score_validation = accuracy_score(validation_labels, y_pred_validation, normalize=False)\n",
        "    print('accuracy_score_validation with normalize=False : ', accuracy_score_validation)\n",
        "\n",
        "    print(classification_report(y_pred_validation, validation_labels, target_names=target_names))\n",
        "\n",
        "    cm1 = confusion_matrix(y_pred_validation, validation_labels)\n",
        "    \n",
        "    print('confusion_matrix : \\n', cm1)\n",
        "\n",
        "    total1=sum(sum(cm1))\n",
        "\n",
        "    #####from confusion matrix calculate accuracy\n",
        "    accuracy1=(cm1[0,0]+cm1[1,1])/total1\n",
        "    print ('Accuracy : ', accuracy1)\n",
        "\n",
        "    sensitivity = cm1[0,0]/(cm1[0,0]+cm1[1,0])\n",
        "    print('Sensitivity : ', sensitivity )\n",
        "\n",
        "    Specificity = cm1[1,1]/(cm1[1,1]+cm1[0,1])\n",
        "    print('Specificity : ', Specificity )\n",
        "\n",
        "\n",
        "save_bottleneck_features()\n",
        "train_top_model()\n",
        "\n",
        "print('-------------------------------------------------------------------')\n",
        "print('------------------ Train Done -------------------------------------')\n",
        "print('-------------------------------------------------------------------')\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
            "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/keras/applications/mobilenet.py:224: UserWarning: MobileNet shape is undefined. Weights for input shape (224, 224) will be loaded.\n",
            "  warnings.warn('MobileNet shape is undefined.'\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.6/mobilenet_1_0_224_tf_no_top.h5\n",
            "17227776/17225924 [==============================] - 0s 0us/step\n",
            "Found 3416 images belonging to 2 classes.\n",
            "Found 1464 images belonging to 2 classes.\n",
            " train_labels shape (3416,)\n",
            " validation_labels shape (1464,)\n",
            "Train on 3416 samples, validate on 1464 samples\n",
            "Epoch 1/50\n",
            "3416/3416 [==============================] - 70s 20ms/step - loss: 7.9683 - acc: 0.4991 - val_loss: 7.9712 - val_acc: 0.5000\n",
            "Epoch 2/50\n",
            "3416/3416 [==============================] - 69s 20ms/step - loss: 7.9712 - acc: 0.5000 - val_loss: 7.9712 - val_acc: 0.5000\n",
            "Epoch 3/50\n",
            "3416/3416 [==============================] - 69s 20ms/step - loss: 7.9712 - acc: 0.5000 - val_loss: 7.9712 - val_acc: 0.5000\n",
            "Epoch 4/50\n",
            "3416/3416 [==============================] - 70s 20ms/step - loss: 7.9712 - acc: 0.5000 - val_loss: 7.9712 - val_acc: 0.5000\n",
            "Epoch 5/50\n",
            "3416/3416 [==============================] - 69s 20ms/step - loss: 7.9712 - acc: 0.5000 - val_loss: 7.9712 - val_acc: 0.5000\n",
            "Epoch 6/50\n",
            "3416/3416 [==============================] - 69s 20ms/step - loss: 7.9712 - acc: 0.5000 - val_loss: 7.9712 - val_acc: 0.5000\n",
            "Epoch 7/50\n",
            "3416/3416 [==============================] - 69s 20ms/step - loss: 7.9712 - acc: 0.5000 - val_loss: 7.9712 - val_acc: 0.5000\n",
            "Epoch 8/50\n",
            "3416/3416 [==============================] - 70s 20ms/step - loss: 7.9712 - acc: 0.5000 - val_loss: 7.9712 - val_acc: 0.5000\n",
            "Epoch 9/50\n",
            "3416/3416 [==============================] - 69s 20ms/step - loss: 7.9712 - acc: 0.5000 - val_loss: 7.9712 - val_acc: 0.5000\n",
            "Epoch 10/50\n",
            "3416/3416 [==============================] - 69s 20ms/step - loss: 7.9712 - acc: 0.5000 - val_loss: 7.9712 - val_acc: 0.5000\n",
            "Epoch 11/50\n",
            "3416/3416 [==============================] - 69s 20ms/step - loss: 7.9712 - acc: 0.5000 - val_loss: 7.9712 - val_acc: 0.5000\n",
            "Epoch 12/50\n",
            "3416/3416 [==============================] - 69s 20ms/step - loss: 7.9712 - acc: 0.5000 - val_loss: 7.9712 - val_acc: 0.5000\n",
            "Epoch 13/50\n",
            "3416/3416 [==============================] - 70s 20ms/step - loss: 7.9712 - acc: 0.5000 - val_loss: 7.9712 - val_acc: 0.5000\n",
            "Epoch 14/50\n",
            "3416/3416 [==============================] - 70s 20ms/step - loss: 7.9712 - acc: 0.5000 - val_loss: 7.9712 - val_acc: 0.5000\n",
            "Epoch 15/50\n",
            "3416/3416 [==============================] - 70s 21ms/step - loss: 7.9712 - acc: 0.5000 - val_loss: 7.9712 - val_acc: 0.5000\n",
            "Epoch 16/50\n",
            "3416/3416 [==============================] - 70s 20ms/step - loss: 7.9712 - acc: 0.5000 - val_loss: 7.9712 - val_acc: 0.5000\n",
            "Epoch 17/50\n",
            "3416/3416 [==============================] - 69s 20ms/step - loss: 7.9712 - acc: 0.5000 - val_loss: 7.9712 - val_acc: 0.5000\n",
            "Epoch 18/50\n",
            "3416/3416 [==============================] - 69s 20ms/step - loss: 7.9712 - acc: 0.5000 - val_loss: 7.9712 - val_acc: 0.5000\n",
            "Epoch 19/50\n",
            "3416/3416 [==============================] - 69s 20ms/step - loss: 7.9712 - acc: 0.5000 - val_loss: 7.9712 - val_acc: 0.5000\n",
            "Epoch 20/50\n",
            "3416/3416 [==============================] - 70s 20ms/step - loss: 7.9712 - acc: 0.5000 - val_loss: 7.9712 - val_acc: 0.5000\n",
            "Epoch 21/50\n",
            "3416/3416 [==============================] - 69s 20ms/step - loss: 7.9712 - acc: 0.5000 - val_loss: 7.9712 - val_acc: 0.5000\n",
            "Epoch 22/50\n",
            "3416/3416 [==============================] - 70s 20ms/step - loss: 7.9712 - acc: 0.5000 - val_loss: 7.9712 - val_acc: 0.5000\n",
            "Epoch 23/50\n",
            "3416/3416 [==============================] - 69s 20ms/step - loss: 7.9712 - acc: 0.5000 - val_loss: 7.9712 - val_acc: 0.5000\n",
            "Epoch 24/50\n",
            "3416/3416 [==============================] - 69s 20ms/step - loss: 7.9712 - acc: 0.5000 - val_loss: 7.9712 - val_acc: 0.5000\n",
            "Epoch 25/50\n",
            "3416/3416 [==============================] - 69s 20ms/step - loss: 7.9712 - acc: 0.5000 - val_loss: 7.9712 - val_acc: 0.5000\n",
            "Epoch 26/50\n",
            "3416/3416 [==============================] - 70s 20ms/step - loss: 7.9712 - acc: 0.5000 - val_loss: 7.9712 - val_acc: 0.5000\n",
            "Epoch 27/50\n",
            "3416/3416 [==============================] - 69s 20ms/step - loss: 7.9712 - acc: 0.5000 - val_loss: 7.9712 - val_acc: 0.5000\n",
            "Epoch 28/50\n",
            "3416/3416 [==============================] - 70s 20ms/step - loss: 7.9712 - acc: 0.5000 - val_loss: 7.9712 - val_acc: 0.5000\n",
            "Epoch 29/50\n",
            "3416/3416 [==============================] - 70s 20ms/step - loss: 7.9712 - acc: 0.5000 - val_loss: 7.9712 - val_acc: 0.5000\n",
            "Epoch 30/50\n",
            "3416/3416 [==============================] - 70s 20ms/step - loss: 7.9712 - acc: 0.5000 - val_loss: 7.9712 - val_acc: 0.5000\n",
            "Epoch 31/50\n",
            "3416/3416 [==============================] - 70s 20ms/step - loss: 7.9712 - acc: 0.5000 - val_loss: 7.9712 - val_acc: 0.5000\n",
            "Epoch 32/50\n",
            "3416/3416 [==============================] - 70s 20ms/step - loss: 7.9712 - acc: 0.5000 - val_loss: 7.9712 - val_acc: 0.5000\n",
            "Epoch 33/50\n",
            "3416/3416 [==============================] - 70s 20ms/step - loss: 7.9712 - acc: 0.5000 - val_loss: 7.9712 - val_acc: 0.5000\n",
            "Epoch 34/50\n",
            "3416/3416 [==============================] - 69s 20ms/step - loss: 7.9712 - acc: 0.5000 - val_loss: 7.9712 - val_acc: 0.5000\n",
            "Epoch 35/50\n",
            "3416/3416 [==============================] - 70s 20ms/step - loss: 7.9712 - acc: 0.5000 - val_loss: 7.9712 - val_acc: 0.5000\n",
            "Epoch 36/50\n",
            "3416/3416 [==============================] - 69s 20ms/step - loss: 7.9712 - acc: 0.5000 - val_loss: 7.9712 - val_acc: 0.5000\n",
            "Epoch 37/50\n",
            "3416/3416 [==============================] - 69s 20ms/step - loss: 7.9712 - acc: 0.5000 - val_loss: 7.9712 - val_acc: 0.5000\n",
            "Epoch 38/50\n",
            "3416/3416 [==============================] - 68s 20ms/step - loss: 7.9712 - acc: 0.5000 - val_loss: 7.9712 - val_acc: 0.5000\n",
            "Epoch 39/50\n",
            "3416/3416 [==============================] - 68s 20ms/step - loss: 7.9712 - acc: 0.5000 - val_loss: 7.9712 - val_acc: 0.5000\n",
            "Epoch 40/50\n",
            "3416/3416 [==============================] - 68s 20ms/step - loss: 7.9712 - acc: 0.5000 - val_loss: 7.9712 - val_acc: 0.5000\n",
            "Epoch 41/50\n",
            "3416/3416 [==============================] - 68s 20ms/step - loss: 7.9712 - acc: 0.5000 - val_loss: 7.9712 - val_acc: 0.5000\n",
            "Epoch 42/50\n",
            "3416/3416 [==============================] - 74s 22ms/step - loss: 7.9712 - acc: 0.5000 - val_loss: 7.9712 - val_acc: 0.5000\n",
            "Epoch 43/50\n",
            "3416/3416 [==============================] - 69s 20ms/step - loss: 7.9712 - acc: 0.5000 - val_loss: 7.9712 - val_acc: 0.5000\n",
            "Epoch 44/50\n",
            "3416/3416 [==============================] - 70s 20ms/step - loss: 7.9712 - acc: 0.5000 - val_loss: 7.9712 - val_acc: 0.5000\n",
            "Epoch 45/50\n",
            "3416/3416 [==============================] - 69s 20ms/step - loss: 7.9712 - acc: 0.5000 - val_loss: 7.9712 - val_acc: 0.5000\n",
            "Epoch 46/50\n",
            "3416/3416 [==============================] - 69s 20ms/step - loss: 7.9712 - acc: 0.5000 - val_loss: 7.9712 - val_acc: 0.5000\n",
            "Epoch 47/50\n",
            "3416/3416 [==============================] - 69s 20ms/step - loss: 7.9712 - acc: 0.5000 - val_loss: 7.9712 - val_acc: 0.5000\n",
            "Epoch 48/50\n",
            "3416/3416 [==============================] - 69s 20ms/step - loss: 7.9712 - acc: 0.5000 - val_loss: 7.9712 - val_acc: 0.5000\n",
            "Epoch 49/50\n",
            "3416/3416 [==============================] - 69s 20ms/step - loss: 7.9712 - acc: 0.5000 - val_loss: 7.9712 - val_acc: 0.5000\n",
            "Epoch 50/50\n",
            "3416/3416 [==============================] - 69s 20ms/step - loss: 7.9712 - acc: 0.5000 - val_loss: 7.9712 - val_acc: 0.5000\n",
            "-------------------------------------------------------------------\n",
            "------------------ Evalute the train set ------------------------\n",
            "-------------------------------------------------------------------\n",
            "Train loss: 7.971192421064444\n",
            "Train accuracy: 0.5\n",
            " y_pred_train shape (3416,)\n",
            "accuracy_score_train with normalize=True:  0.5\n",
            "accuracy_score_train with normalize=False :  1708\n",
            "             precision    recall  f1-score   support\n",
            "\n",
            "    class 0       0.00      0.00      0.00         0\n",
            "    class 1       1.00      0.50      0.67      3416\n",
            "\n",
            "avg / total       1.00      0.50      0.67      3416\n",
            "\n",
            "confusion_matrix : \n",
            " [[   0    0]\n",
            " [1708 1708]]\n",
            "Accuracy :  0.5\n",
            "Sensitivity :  0.0\n",
            "Specificity :  1.0\n",
            "-------------------------------------------------------------------\n",
            "------------------ Evalute the validation set ------------------------\n",
            "-------------------------------------------------------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1137: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
            "  'recall', 'true', average, warn_for)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "validation loss: 7.971192424414617\n",
            "validation accuracy: 0.5\n",
            " y_pred_validation shape (1464,)\n",
            "accuracy_score_validation with normalize=True:  0.5\n",
            "accuracy_score_validation with normalize=False :  732\n",
            "             precision    recall  f1-score   support\n",
            "\n",
            "    class 0       0.00      0.00      0.00         0\n",
            "    class 1       1.00      0.50      0.67      1464\n",
            "\n",
            "avg / total       1.00      0.50      0.67      1464\n",
            "\n",
            "confusion_matrix : \n",
            " [[  0   0]\n",
            " [732 732]]\n",
            "Accuracy :  0.5\n",
            "Sensitivity :  0.0\n",
            "Specificity :  1.0\n",
            "-------------------------------------------------------------------\n",
            "------------------ Train Done -------------------------------------\n",
            "-------------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "2DlYEMBLjUDG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "iGm0yDZojUDU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "### Class 0 Vs Class 4"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "orBwU83_jUDl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2809
        },
        "outputId": "2ebea5cb-f972-4eb3-ce01-650d8f1c5fce"
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf \n",
        "import keras\n",
        "import os\n",
        "\n",
        "from subprocess import check_output\n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "from PIL import Image\n",
        "            \n",
        "from keras.applications.mobilenet import preprocess_input               \n",
        "\n",
        "from keras.preprocessing import image\n",
        "\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "from sklearn.cross_validation import train_test_split\n",
        "\n",
        "from keras.utils import np_utils\n",
        "\n",
        "\n",
        "from keras.applications.mobilenet import MobileNet    \n",
        "\n",
        "from keras.models import Model, load_model\n",
        "\n",
        "from keras.layers import Dense, Dropout, Reshape\n",
        "\n",
        "from keras.optimizers import SGD\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.metrics import classification_report,confusion_matrix\n",
        "\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "from keras.layers import Activation, Flatten \n",
        "from keras import backend as K\n",
        "from keras import applications\n",
        "\n",
        "\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0' #use GPU with ID=0\n",
        "config = tf.ConfigProto()\n",
        "config.gpu_options.per_process_gpu_memory_fraction = 0.7 # maximun alloc gpu50% of MEM\n",
        "#config.gpu_options.allow_growth = True #allocate dynamically\n",
        "sess = tf.Session(config = config)\n",
        "keras.backend.set_session(sess)\n",
        "\n",
        "\n",
        "# dimensions of our images.\n",
        "img_width, img_height = 224, 224\n",
        "\n",
        "top_model_weights_path = '/content/gdrive/My Drive/Colab Notebooks/bottleneck_fc_model_MobileNet_PTN_data4.h5'\n",
        "\n",
        "train_data_dir = '/content/gdrive/My Drive/Colab Notebooks/data4_Train496_Test212/train'\n",
        "validation_data_dir = '/content/gdrive/My Drive/Colab Notebooks/data4_Train496_Test212/validation'\n",
        "nb_train_samples = 992\n",
        "nb_validation_samples = 424\n",
        "\n",
        "epochs = 50\n",
        "batch_size = 8  # batch size in flow_images_from_directory needs to correspond to the image number of the test images.\n",
        "\n",
        "\n",
        "def save_bottleneck_features():\n",
        "    datagen = ImageDataGenerator(rescale=1. / 255)\n",
        "\n",
        "    # build the Inception_ResNet_V2 network\n",
        "\n",
        "    model = applications.MobileNet(weights='imagenet', include_top=False)\n",
        "\n",
        "    #model.summary()\n",
        "    \n",
        "    generator = datagen.flow_from_directory(\n",
        "        train_data_dir,\n",
        "        target_size=(img_width, img_height),\n",
        "        batch_size=batch_size,\n",
        "        class_mode=None,\n",
        "        shuffle=False)\n",
        "    \n",
        "    bottleneck_features_train = model.predict_generator(\n",
        "        generator, nb_train_samples // batch_size)\n",
        "    np.save(open('DR1_bottleneck_features_train_MobileNet_PTN_data4.npy', 'wb'),\n",
        "            bottleneck_features_train)\n",
        "\n",
        "    generator = datagen.flow_from_directory(\n",
        "        validation_data_dir,\n",
        "        target_size=(img_width, img_height),\n",
        "        batch_size=batch_size,\n",
        "        class_mode=None,\n",
        "        shuffle=False)\n",
        "    bottleneck_features_validation = model.predict_generator(\n",
        "        generator, nb_validation_samples // batch_size)\n",
        "    np.save(open('DR1_bottleneck_features_validation_MobileNet_PTN_data4.npy', 'wb'),\n",
        "            bottleneck_features_validation)\n",
        "\n",
        "\n",
        "def train_top_model():\n",
        "    train_data = np.load(open('DR1_bottleneck_features_train_MobileNet_PTN_data4.npy','rb'))\n",
        "    train_labels = np.array(\n",
        "        [0] * (nb_train_samples // 2) + [1] * (nb_train_samples // 2))\n",
        "\n",
        "    print(' train_labels shape',train_labels.shape)\n",
        "\n",
        "    #print(' train_labels : \\n ',train_labels)\n",
        "\n",
        "    validation_data = np.load(open('DR1_bottleneck_features_validation_MobileNet_PTN_data4.npy','rb'))\n",
        "    validation_labels = np.array(\n",
        "        [0] * (nb_validation_samples // 2) + [1] * (nb_validation_samples // 2))\n",
        "\n",
        "    print(' validation_labels shape',validation_labels.shape)\n",
        "\n",
        "    #print(' validation_labels : \\n ',validation_labels)\n",
        "\n",
        "    model = Sequential()\n",
        "    model.add(Flatten(input_shape=train_data.shape[1:]))\n",
        "    model.add(Dense(256, activation='relu'))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "    model.compile(optimizer='rmsprop',\n",
        "                  loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    model.fit(train_data, train_labels,\n",
        "              epochs=epochs,\n",
        "              batch_size=batch_size,\n",
        "              validation_data=(validation_data, validation_labels))\n",
        "    model.save_weights(top_model_weights_path)\n",
        "    \n",
        "    print('-------------------------------------------------------------------')\n",
        "    print('------------------ Evalute the train set ------------------------')\n",
        "    print('-------------------------------------------------------------------')\n",
        "\n",
        "    train_score = model.evaluate(train_data, train_labels, verbose=0) # nb_train_samples // batch_size, workers=1)\n",
        "    print('Train loss:', train_score[0])\n",
        "    print('Train accuracy:', train_score[1])\n",
        "\n",
        "    y_pred_train = np.squeeze(model.predict(train_data))\n",
        "    print(' y_pred_train shape',y_pred_train.shape)\n",
        "    #print(' y_pred_train : \\n ',y_pred_train)\n",
        "\n",
        "    threshold = 0.5\n",
        "    y_pred_train = (y_pred_train > threshold)*1\n",
        "    #y_pred_train.astype(int)   \n",
        "    #print(' y_pred_train again: \\n ',y_pred_train)\n",
        "    \n",
        "    accuracy_score_train = accuracy_score(train_labels, y_pred_train, normalize=True)\n",
        "    print('accuracy_score_train with normalize=True: ', accuracy_score_train)\n",
        "\n",
        "    accuracy_score_train = accuracy_score(train_labels, y_pred_train, normalize=False)\n",
        "    print('accuracy_score_train with normalize=False : ', accuracy_score_train)\n",
        "\n",
        "    target_names = ['class 0', 'class 1']\n",
        "\n",
        "    print(classification_report(y_pred_train, train_labels, target_names=target_names))\n",
        "\n",
        "    cm1 = confusion_matrix(y_pred_train, train_labels)\n",
        "    \n",
        "    print('confusion_matrix : \\n', cm1)\n",
        "\n",
        "    total1=sum(sum(cm1))\n",
        "\n",
        "    #####from confusion matrix calculate accuracy\n",
        "    accuracy1=(cm1[0,0]+cm1[1,1])/total1\n",
        "    print ('Accuracy : ', accuracy1)\n",
        "\n",
        "    sensitivity = cm1[0,0]/(cm1[0,0]+cm1[1,0])\n",
        "    print('Sensitivity : ', sensitivity )\n",
        "\n",
        "    Specificity = cm1[1,1]/(cm1[1,1]+cm1[0,1])\n",
        "    print('Specificity : ', Specificity )\n",
        "\n",
        "    print('-------------------------------------------------------------------')\n",
        "    print('------------------ Evalute the validation set ------------------------')\n",
        "    print('-------------------------------------------------------------------')\n",
        "\n",
        "    validation_score = model.evaluate(validation_data, validation_labels, verbose=0) # nb_train_samples // batch_size, workers=1)\n",
        "    print('validation loss:', validation_score[0])\n",
        "    print('validation accuracy:', validation_score[1])\n",
        "\n",
        "    y_pred_validation = np.squeeze(model.predict(validation_data))\n",
        "    print(' y_pred_validation shape',y_pred_validation.shape)\n",
        "    #print(' y_pred_validation : \\n ',y_pred_validation)\n",
        "\n",
        "    y_pred_validation = (y_pred_validation > threshold)*1\n",
        "    #y_pred_train.astype(int)   \n",
        "    #print(' y_pred_validation again: \\n ',y_pred_validation)\n",
        "    \n",
        "    accuracy_score_validation = accuracy_score(validation_labels, y_pred_validation, normalize=True)\n",
        "    print('accuracy_score_validation with normalize=True: ', accuracy_score_validation)\n",
        "\n",
        "    accuracy_score_validation = accuracy_score(validation_labels, y_pred_validation, normalize=False)\n",
        "    print('accuracy_score_validation with normalize=False : ', accuracy_score_validation)\n",
        "\n",
        "    print(classification_report(y_pred_validation, validation_labels, target_names=target_names))\n",
        "\n",
        "    cm1 = confusion_matrix(y_pred_validation, validation_labels)\n",
        "    \n",
        "    print('confusion_matrix : \\n', cm1)\n",
        "\n",
        "    total1=sum(sum(cm1))\n",
        "\n",
        "    #####from confusion matrix calculate accuracy\n",
        "    accuracy1=(cm1[0,0]+cm1[1,1])/total1\n",
        "    print ('Accuracy : ', accuracy1)\n",
        "\n",
        "    sensitivity = cm1[0,0]/(cm1[0,0]+cm1[1,0])\n",
        "    print('Sensitivity : ', sensitivity )\n",
        "\n",
        "    Specificity = cm1[1,1]/(cm1[1,1]+cm1[0,1])\n",
        "    print('Specificity : ', Specificity )\n",
        "\n",
        "\n",
        "save_bottleneck_features()\n",
        "train_top_model()\n",
        "\n",
        "print('-------------------------------------------------------------------')\n",
        "print('------------------ Train Done -------------------------------------')\n",
        "print('-------------------------------------------------------------------')\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras/applications/mobilenet.py:224: UserWarning: MobileNet shape is undefined. Weights for input shape (224, 224) will be loaded.\n",
            "  warnings.warn('MobileNet shape is undefined.'\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Found 996 images belonging to 2 classes.\n",
            "Found 424 images belonging to 2 classes.\n",
            " train_labels shape (992,)\n",
            " validation_labels shape (424,)\n",
            "Train on 992 samples, validate on 424 samples\n",
            "Epoch 1/50\n",
            "992/992 [==============================] - 22s 22ms/step - loss: 7.7955 - acc: 0.5030 - val_loss: 7.9712 - val_acc: 0.5000\n",
            "Epoch 2/50\n",
            "992/992 [==============================] - 20s 21ms/step - loss: 7.9678 - acc: 0.4990 - val_loss: 7.9712 - val_acc: 0.5000\n",
            "Epoch 3/50\n",
            "992/992 [==============================] - 20s 21ms/step - loss: 7.9712 - acc: 0.5000 - val_loss: 7.9712 - val_acc: 0.5000\n",
            "Epoch 4/50\n",
            "992/992 [==============================] - 20s 21ms/step - loss: 7.9712 - acc: 0.5000 - val_loss: 7.9712 - val_acc: 0.5000\n",
            "Epoch 5/50\n",
            "992/992 [==============================] - 21s 21ms/step - loss: 7.9712 - acc: 0.5000 - val_loss: 7.9712 - val_acc: 0.5000\n",
            "Epoch 6/50\n",
            "992/992 [==============================] - 20s 21ms/step - loss: 7.9590 - acc: 0.4990 - val_loss: 7.9712 - val_acc: 0.5000\n",
            "Epoch 7/50\n",
            "992/992 [==============================] - 21s 21ms/step - loss: 5.9636 - acc: 0.6109 - val_loss: 4.6489 - val_acc: 0.6769\n",
            "Epoch 8/50\n",
            "992/992 [==============================] - 20s 21ms/step - loss: 4.2572 - acc: 0.7177 - val_loss: 5.1825 - val_acc: 0.6509\n",
            "Epoch 9/50\n",
            "992/992 [==============================] - 20s 21ms/step - loss: 3.7161 - acc: 0.7560 - val_loss: 3.2282 - val_acc: 0.7854\n",
            "Epoch 10/50\n",
            "992/992 [==============================] - 20s 21ms/step - loss: 4.1550 - acc: 0.7258 - val_loss: 3.2908 - val_acc: 0.7877\n",
            "Epoch 11/50\n",
            "992/992 [==============================] - 21s 21ms/step - loss: 3.8850 - acc: 0.7520 - val_loss: 3.2981 - val_acc: 0.7901\n",
            "Epoch 12/50\n",
            "992/992 [==============================] - 21s 21ms/step - loss: 4.1286 - acc: 0.7359 - val_loss: 3.5080 - val_acc: 0.7712\n",
            "Epoch 13/50\n",
            "992/992 [==============================] - 20s 21ms/step - loss: 3.1739 - acc: 0.7923 - val_loss: 3.3575 - val_acc: 0.7830\n",
            "Epoch 14/50\n",
            "992/992 [==============================] - 20s 21ms/step - loss: 3.1597 - acc: 0.7974 - val_loss: 3.2200 - val_acc: 0.7877\n",
            "Epoch 15/50\n",
            "992/992 [==============================] - 20s 21ms/step - loss: 3.1768 - acc: 0.7913 - val_loss: 3.0613 - val_acc: 0.8042\n",
            "Epoch 16/50\n",
            "992/992 [==============================] - 20s 21ms/step - loss: 3.2673 - acc: 0.7903 - val_loss: 2.8849 - val_acc: 0.8184\n",
            "Epoch 17/50\n",
            "992/992 [==============================] - 20s 21ms/step - loss: 3.1618 - acc: 0.7984 - val_loss: 3.6339 - val_acc: 0.7665\n",
            "Epoch 18/50\n",
            "992/992 [==============================] - 20s 21ms/step - loss: 3.4166 - acc: 0.7802 - val_loss: 3.1498 - val_acc: 0.7995\n",
            "Epoch 19/50\n",
            "992/992 [==============================] - 20s 21ms/step - loss: 3.3986 - acc: 0.7833 - val_loss: 3.1075 - val_acc: 0.7972\n",
            "Epoch 20/50\n",
            "992/992 [==============================] - 20s 21ms/step - loss: 3.1265 - acc: 0.8024 - val_loss: 3.0754 - val_acc: 0.8019\n",
            "Epoch 21/50\n",
            "992/992 [==============================] - 20s 21ms/step - loss: 2.8699 - acc: 0.8165 - val_loss: 3.2140 - val_acc: 0.7925\n",
            "Epoch 22/50\n",
            "992/992 [==============================] - 21s 21ms/step - loss: 3.2062 - acc: 0.7933 - val_loss: 3.3106 - val_acc: 0.7925\n",
            "Epoch 23/50\n",
            "992/992 [==============================] - 20s 21ms/step - loss: 3.8402 - acc: 0.7550 - val_loss: 3.4244 - val_acc: 0.7830\n",
            "Epoch 24/50\n",
            "992/992 [==============================] - 20s 21ms/step - loss: 2.7866 - acc: 0.8216 - val_loss: 3.1439 - val_acc: 0.8042\n",
            "Epoch 25/50\n",
            "992/992 [==============================] - 20s 21ms/step - loss: 3.1054 - acc: 0.7994 - val_loss: 3.4287 - val_acc: 0.7830\n",
            "Epoch 26/50\n",
            "992/992 [==============================] - 20s 21ms/step - loss: 2.8808 - acc: 0.8165 - val_loss: 3.2439 - val_acc: 0.7948\n",
            "Epoch 27/50\n",
            "992/992 [==============================] - 21s 21ms/step - loss: 2.8056 - acc: 0.8206 - val_loss: 4.0698 - val_acc: 0.7429\n",
            "Epoch 28/50\n",
            "992/992 [==============================] - 21s 21ms/step - loss: 2.8555 - acc: 0.8165 - val_loss: 3.6663 - val_acc: 0.7618\n",
            "Epoch 29/50\n",
            "992/992 [==============================] - 20s 21ms/step - loss: 3.4097 - acc: 0.7843 - val_loss: 2.8583 - val_acc: 0.8160\n",
            "Epoch 30/50\n",
            "992/992 [==============================] - 20s 21ms/step - loss: 2.7902 - acc: 0.8216 - val_loss: 3.0612 - val_acc: 0.8042\n",
            "Epoch 31/50\n",
            "992/992 [==============================] - 20s 21ms/step - loss: 3.4594 - acc: 0.7812 - val_loss: 3.9557 - val_acc: 0.7500\n",
            "Epoch 32/50\n",
            "992/992 [==============================] - 20s 21ms/step - loss: 2.7949 - acc: 0.8226 - val_loss: 2.8362 - val_acc: 0.8231\n",
            "Epoch 33/50\n",
            "992/992 [==============================] - 20s 21ms/step - loss: 2.6548 - acc: 0.8317 - val_loss: 2.8584 - val_acc: 0.8184\n",
            "Epoch 34/50\n",
            "992/992 [==============================] - 21s 21ms/step - loss: 2.8324 - acc: 0.8185 - val_loss: 2.9629 - val_acc: 0.8090\n",
            "Epoch 35/50\n",
            "992/992 [==============================] - 20s 21ms/step - loss: 2.7936 - acc: 0.8216 - val_loss: 3.0445 - val_acc: 0.8066\n",
            "Epoch 36/50\n",
            "992/992 [==============================] - 20s 21ms/step - loss: 2.9377 - acc: 0.8125 - val_loss: 3.0228 - val_acc: 0.8113\n",
            "Epoch 37/50\n",
            "992/992 [==============================] - 21s 21ms/step - loss: 2.8144 - acc: 0.8185 - val_loss: 2.8965 - val_acc: 0.8184\n",
            "Epoch 38/50\n",
            "992/992 [==============================] - 21s 21ms/step - loss: 2.4567 - acc: 0.8448 - val_loss: 3.2950 - val_acc: 0.7901\n",
            "Epoch 39/50\n",
            "992/992 [==============================] - 21s 21ms/step - loss: 2.4827 - acc: 0.8427 - val_loss: 3.7627 - val_acc: 0.7618\n",
            "Epoch 40/50\n",
            "992/992 [==============================] - 21s 21ms/step - loss: 2.7779 - acc: 0.8236 - val_loss: 3.4940 - val_acc: 0.7759\n",
            "Epoch 41/50\n",
            "992/992 [==============================] - 21s 21ms/step - loss: 2.6332 - acc: 0.8347 - val_loss: 3.5346 - val_acc: 0.7736\n",
            "Epoch 42/50\n",
            "992/992 [==============================] - 20s 21ms/step - loss: 2.8350 - acc: 0.8206 - val_loss: 2.9662 - val_acc: 0.8137\n",
            "Epoch 43/50\n",
            "992/992 [==============================] - 20s 21ms/step - loss: 2.9474 - acc: 0.8135 - val_loss: 3.1736 - val_acc: 0.7995\n",
            "Epoch 44/50\n",
            "992/992 [==============================] - 20s 21ms/step - loss: 2.7402 - acc: 0.8266 - val_loss: 3.8678 - val_acc: 0.7571\n",
            "Epoch 45/50\n",
            "992/992 [==============================] - 20s 21ms/step - loss: 2.5765 - acc: 0.8337 - val_loss: 2.7165 - val_acc: 0.8278\n",
            "Epoch 46/50\n",
            "992/992 [==============================] - 21s 21ms/step - loss: 2.8818 - acc: 0.8155 - val_loss: 3.1003 - val_acc: 0.7995\n",
            "Epoch 47/50\n",
            "992/992 [==============================] - 21s 21ms/step - loss: 2.5329 - acc: 0.8387 - val_loss: 2.7027 - val_acc: 0.8255\n",
            "Epoch 48/50\n",
            "992/992 [==============================] - 21s 21ms/step - loss: 2.3806 - acc: 0.8458 - val_loss: 3.0110 - val_acc: 0.8113\n",
            "Epoch 49/50\n",
            "992/992 [==============================] - 20s 21ms/step - loss: 2.3278 - acc: 0.8488 - val_loss: 2.7737 - val_acc: 0.8255\n",
            "Epoch 50/50\n",
            "992/992 [==============================] - 20s 21ms/step - loss: 2.5432 - acc: 0.8367 - val_loss: 3.7345 - val_acc: 0.7594\n",
            "-------------------------------------------------------------------\n",
            "------------------ Evalute the train set ------------------------\n",
            "-------------------------------------------------------------------\n",
            "Train loss: 3.050658606713822\n",
            "Train accuracy: 0.8024193548387096\n",
            " y_pred_train shape (992,)\n",
            "accuracy_score_train with normalize=True:  0.8024193548387096\n",
            "accuracy_score_train with normalize=False :  796\n",
            "             precision    recall  f1-score   support\n",
            "\n",
            "    class 0       0.65      0.94      0.77       340\n",
            "    class 1       0.96      0.73      0.83       652\n",
            "\n",
            "avg / total       0.85      0.80      0.81       992\n",
            "\n",
            "confusion_matrix : \n",
            " [[320  20]\n",
            " [176 476]]\n",
            "Accuracy :  0.8024193548387096\n",
            "Sensitivity :  0.6451612903225806\n",
            "Specificity :  0.9596774193548387\n",
            "-------------------------------------------------------------------\n",
            "------------------ Evalute the validation set ------------------------\n",
            "-------------------------------------------------------------------\n",
            "validation loss: 3.734465412373814\n",
            "validation accuracy: 0.7594339622641509\n",
            " y_pred_validation shape (424,)\n",
            "accuracy_score_validation with normalize=True:  0.7594339622641509\n",
            "accuracy_score_validation with normalize=False :  322\n",
            "             precision    recall  f1-score   support\n",
            "\n",
            "    class 0       0.58      0.91      0.71       134\n",
            "    class 1       0.94      0.69      0.80       290\n",
            "\n",
            "avg / total       0.83      0.76      0.77       424\n",
            "\n",
            "confusion_matrix : \n",
            " [[122  12]\n",
            " [ 90 200]]\n",
            "Accuracy :  0.7594339622641509\n",
            "Sensitivity :  0.5754716981132075\n",
            "Specificity :  0.9433962264150944\n",
            "-------------------------------------------------------------------\n",
            "------------------ Train Done -------------------------------------\n",
            "-------------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}